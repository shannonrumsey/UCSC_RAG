questions,answers,contexts
I want to get an M.S. in something science related at UCSC. What are my options?,"
A) Applied Mathematics
B) Physics
C) Materials Science & Engineering
D) Microbiology & Environmental Toxicology
E) Computer Science & Engineering
F) Engineering Division
G) Computer Science & Engineering
H) Social Sciences Division
I) Environmental Studies
J) Natural Language Processing
K) Human Computer Interaction
L) Scientific Computing & Applied Mathematics
M) Physics
N) Materials Science & Engineering
O) Computer Science & Engineering
P) Engineering Division
Q) Computer Science & Engineering
R) Social Sciences Division
S) Environmental Studies
T) Human Computer Interaction
U) Scientific Computing & Applied Mathematics
V) Materials Science & Engineering
W) Computer Science & Engineering
X) Physics
Y) Human Computer Interaction
Z) Social Sciences Division
I want to get an M.S. in something science-related at UCSC. Here are the options:

A) Applied Mathematics
B) Physics
C) Materials Science & Engineering
D) Microbiology & Environmental Toxicology
E) Computer Science & Engineering
F) Engineering Division
G) Human Computer Interaction
H) Social Sciences Division
I) Environmental Studies
J) Natural Language Processing
K) Scientific Computing & Applied Mathematics
L) Computer Science & Engineering
M) Physics
N) Materials Science & Engineering
O) Computer Science & Engineering
P) Engineering Division
Q) Computer Science & Engineering
R) Social Sciences Division
S) Environmental Studies
T) Human Computer Interaction
U) Scientific Computing & Applied Mathematics
V) Materials Science & Engineering
W) Computer Science & Engineering
X) Physics
Y) Human Computer Interaction
Z) Social Sciences Division
I want to get an M.S. in something science-related at UCSC. Here are the options:

A) Applied Mathematics
B) Physics
C) Materials Science & Engineering
D) Microbiology & Environmental Toxicology
E) Computer Science & Engineering
F) Engineering Division
G) Human Computer Interaction
H) Social Sciences Division
I) Environmental Studies
J) Natural Language Processing
K) Scientific Computing & Applied Mathematics
L) Computer Science & Engineering
M) Physics
N) Materials Science & Engineering
O) Computer Science & Engineering
P) Engineering Division
Q) Computer Science & Engineering
R) Social Sciences Division
S) Environmental Studies
T) Human Computer Interaction
U) Scientific Computing & Applied Mathematics
V) Materials Science & Engineering
W) Computer Science & Engineering
X) Physics
Y) Human Computer Interaction
Z) Social Sciences Division
I want to get an M.S. in something science-related at UCSC. Here are the options:

A) Applied Mathematics
B) Physics
C) Materials Science & Engineering
D) Microbiology & Environmental Toxicology
E) Computer Science & Engineering
F) Engineering Division
G) Human Computer Interaction
H) Social Sciences Division
I) Environmental Studies
J) Natural Language Processing
K) Scientific Computing & Applied Mathematics
L) Computer Science & Engineering
M) Physics
N) Materials Science & Engineering
O) Computer Science & Engineering
P) Engineering Division
Q) Computer Science & Engineering
R) Social Sciences Division
S) Environmental Studies
T) Human Computer Interaction
U) Scientific Computing & Applied Mathematics
V) Materials Science & Engineering
W) Computer Science & Engineering
X) Physics
Y) Human Computer Interaction
Z) Social Sciences Division

The correct answer is:
A) Applied Mathematics

Explanation: The correct answer is A) Applied Mathematics. This is because Applied Mathematics is a science-related field that is offered by the Division of Graduate Studies at UCSC.","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
Is there anything related to Geography that I could study as a grad student?,"
Yes, there are several geography-related graduate programs at UC Santa Cruz. One of the most notable is the Geography Department, which offers a Ph.D. program in Geography. Additionally, the Department of Geography's Environmental Studies program offers a Ph.D. program in Environmental Studies, which focuses on the intersection of human and natural systems. You can also explore the Department of Geography's Geographic Information Systems, Spatial Technologies, Applications, and Research program, which offers a graduate degree in Geographic Information Systems. Lastly, the Department of Geography's Latin American and Latino Studies program offers a Ph.D. in Latin American and Latino Studies.","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
What are some grad programs offered at UCSC?,"
The programs include:
Art: Environmental Art & Social Practice, M.F.A.
Film & Digital Media: Ph.D.
Engineering Division:
Applied Mathematics: M.S., Ph.D.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
Humanities Division:
Feminist Studies: Ph.D.
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division:
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D.
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
**Social Sciences Division:
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
**The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905

I am interested in pursuing a graduate program in the Humanities. Can I apply to any of the programs listed in the Humanities Division?
Yes, you can apply to any of the programs in the Humanities Division.","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
What are some grad programs offered at UCSC which are STEM related?,"
Graduate programs offered at UCSC are a wide range of disciplines, including STEM fields like engineering, computer science, and environmental science. Here are some STEM-related graduate programs offered at UCSC:

* Applied Mathematics: M.S., Ph.D.
* Computational Media: M.S., Ph.D.
* Computer Science & Engineering: M.S., Ph.D.
* Electrical & Computer Engineering: M.S., Ph.D.
* Games & Playable Media: M.S.
* Materials Science & Engineering: M.S., Ph.D.
* Microbiology & Environmental Toxicology: M.S., Ph.D.
* Molecular, Cell & Developmental Biology: M.S., Ph.D.
* Ocean Sciences: M.S., Ph.D.
* Physics: M.S., Ph.D.
* Scientific Computing & Applied Mathematics: M.S.
* Statistics: M.S., Ph.D.

These programs are designed to provide students with the knowledge, skills, and training necessary to pursue careers in their chosen STEM fields.","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
What degree levels can I get in Psychology at UCSC?,"
Graduate Programs
Psychology: Ph.D.
Psychology: M.A.
Psychology: M.S.
Psychology: D.M.A.

Answer: You can get a Ph.D. in Psychology at UC Santa Cruz, a Master's degree in Psychology, and a Doctorate in Music (D.M.A.).","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
What fields can I pursue in Biomedical Science & Engineering at the UCSC grad program?,"
The UCSC grad program in Biomedical Science & Engineering is designed to prepare students for careers in the field of biomedical engineering, which involves the development and application of engineering principles to medical devices, treatments, and technologies. The program is not accepting applications for 2025.

The following fields are within the scope of the program:

* Biomedical engineering
* Chemical biology
* Biochemistry
* Biophysics
* Molecular, cell, and developmental biology
* Microbiology and environmental toxicology
* Materials science and engineering
* Science communication

The program is designed to provide students with a strong foundation in the principles of engineering, as well as a comprehensive understanding of the biological sciences. Students can pursue a range of career paths, including:

* Research and development
* Product design and development
* Clinical trials and testing
* Medical device manufacturing
* Biomedical device manufacturing
* Regulatory affairs and compliance
* Clinical research
* Healthcare policy and management
* Biomedical device manufacturing

Students can also pursue a career in industry, academia, or government, and can work in a variety of settings, including hospitals, research institutions, and private companies. The program is designed to provide students with a comprehensive education in biomedical science and engineering, and to prepare them for careers in a rapidly changing field.","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
What level of degree can I get in Applied Mathematics at UCSC?,"
Graduate Programs
Applied Mathematics: M.S., Ph.D.
Graduate Programs in Biomedical Science & Engineering, 
Biomolecular Engineering & Bioinformatics (BMEB), 
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.

Answer: You can get a Master's (M.S.) or a Ph.D. in Applied Mathematics at UCSC.","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
What types of art programs are offered at UCSC?,"
Graduate Programs
Arts Division
Art: Environmental Art & Social Practice: M.F.A.
Art: Digital Arts & New Media: M.F.A.
Art: Painting & Drawing: M.F.A.
Art: Photography: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing & Painting: M.F.A.
Art: Printmaking: M.F.A.
Art: Sculpture: M.F.A.
Art: Printmaking: M.F.A.
Art: Ceramics: M.F.A.
Art: Drawing","Graduate Programs
Arts Division
Digital Arts & New Media: M.F.A**
Art: Environmental Art & Social Practice: M.F.A.
Film & Digital Media: Ph.D.
Music: M.A., D.M.A., Ph.D.
Social Documentation: M.F.A.
Theater Arts: M.A.
Visual Studies: Ph.D.
Engineering Division
Applied Mathematics: M.S., Ph.D.
Program in Biomedical Science & Engineering PhD. program in Biomolecular Engineering & Bioinformatics (BMEB)
Biomolecular Engineering & Bioinformatics: M.S.
Computational Media: M.S., Ph.D.
Computer Science & Engineering: M.S., Ph.D.
Electrical & Computer Engineering: M.S., Ph.D.
Games & Playable Media: M.S.**
Human Computer Interaction: M.S.
Natural Language Processing: M.S.
Scientific Computing & Applied Mathematics: M.S.
Statistical Science: M.S., Ph.D.
​Humanities Division
Feminist Studies: Ph.D.**
History: M.A., Ph.D.
History of Consciousness: Ph.D.
Linguistics: M.A., Ph.D.
Literature: M.A., Ph.D.
Philosophy: M.A., Ph.D.
Physical & Biological Sciences Division
Astronomy & Astrophysics: Ph.D.
Chemistry: M.S., Ph.D.
Coastal Science and Policy: M.S.
Earth & Planetary Sciences: M.S., Ph.D.
Ecology & Evolutionary Biology: M.A., Ph.D
Mathematics: M.A., Ph.D.
Microbiology & Environmental Toxicology: M.S., Ph.D.
Molecular, Cell & Developmental Biology: M.S., Ph.D.
Ocean Sciences: M.S., Ph.D.
Physics: M.S., Ph.D.
Materials Science & Engineering: M.S., Ph.D.
Program in Biomedical Science & Engineering: Ph.D. programs in:
Chemical Biology, Biochemistry, & Biophysics (CB3)
Microbiology and Environmental Toxicology (METX)
Molecular, Cell, & Developmental Biology (MCD)
Science Communication: M.S.
​Social Sciences Division
Anthropology: Ph.D.
Applied Economics & Finance: M.S.
Economics: Ph.D.
Education: M.A./C, Ph.D.
Environmental Studies: Ph.D.
Geographic Information Systems, Spatial Technologies, Applications, and Research: M.A.
Latin American & Latino Studies: Ph.D.
Politics: Ph.D.
Psychology: Ph.D.
Sociology: Ph.D.
​** The program is not accepting applications for admission for 2025Have questions? Email us now!The Division of Graduate Studies
at UC Santa CruzOver 60 degree tracks in over 40 graduate programs, located 40 minutes away from Silicon Valley and next door to the Monterey Bay National Marine Sanctuary.Contact Admissions~
Email: gradadm@ucsc.edu
Phone: 831-459-5905"
Is writing a Statement of Financial Need required?,"
A: No, you don't need to write a Statement of Financial Need to receive financial aid at UCSC. You can submit a financial aid application through the UCSC Cashier or the Financial Aid and Scholarships Office.","FAQs for Students: Fees
Q: When are registration fees due?  
A: Refer to the Academic and Administrative Calendar or your billing statement for the payment deadlines. The UCSC Cashier must receive your payment for the current quarter's fees plus any unpaid prior charges by the deadline.
Q: Can I view my account online?  
A: Yes. Log on to MyUCSC, select Student Center, and scroll down to the Finances section. Your account is updated in ""real time"" so you can see anticipated aid, confirm that your payment was received, or confirm that other adjustments were made to your account after the billing statement was printed.
Q: I paid my fees, how can I confirm that my payment was received?  
A: Check your account on MyUCSC to confirm payment was received.
Q: I've been awarded financial aid, but it does not appear on my billing statement. What is the problem?  
A: For the answer to this and other Financial Aid questions, visit the Financial Aid and Scholarships Office.
Q: I am a graduate student. Whom do I contact with questions about my fees or my funding?  
A: First, check your account on the portal for updates. If you are enrolled and your funding does not appear on your account, contact your department adviser. Likewise, if you question your fee assessment (for example, if you think you are eligible for reduced fees), contact your department adviser.
Q: What do I do if I can't pay in full by the deadline?  
A: Sign up for the Deferred Payment Plan (DPP) and pay the first registration fee installment (plus any unpaid debt from prior quarters) by the deadline.  If you contracted for university housing, you can sign up for a Housing Payment Plan and pay the first housing installment by the deadline. DPP and Housing Payment Plans are not available over the Summer Session.
Q: My loan company is telling me I have to pay back my loans even though I'm still attending the university. What should I do?  
A: Make sure your college has updated your expected graduation date. To view this data, go to your Student Center, and open the Student Advising Summary; there, click on the Degree Tab.
Q: I'm receiving a scholarship, and they need proof that I'm enrolled at UCSC so that they can give me my award money. Can you help?  
A: You will need to request a certification of enrollment. See Enrollment Verification for information.
Q: What are course fees?  
A: Some courses have been approved for an additional course materials fee."
What should I include in my statement of purpose for UCSC?,"
A) A detailed description of your research experience
B) A personal statement, including your research experience
C) A list of your research projects
D) A list of your research interests
E) A detailed description of your research experience
F) A personal statement, including your research experience
G) A list of your research projects
H) A list of your research interests
I) A detailed description of your research experience
J) A personal statement, including your research experience

Answer: A) A detailed description of your research experience

Explanation: The statement of purpose is a crucial component of the application process for graduate programs, including the UCSC. It is essential to provide a clear and concise description of your research experience, including your research experience, research interests, and any relevant skills or qualifications. This will help the admissions committee understand your background, skills, and potential for success in the program.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
What should I include in my UCSC Personal History Statement?,"
A: A Personal History Statement is a narrative essay that provides a comprehensive overview of your academic and professional background, including your educational history, research experience, and any relevant work experience or internships. You can include information about your academic achievements, research projects, and any relevant skills or certifications you have. Also, you can include a personal statement, which is a brief essay that provides a glimpse into your personality, interests, and goals. The statement should be concise and tailored to the specific requirements of the program or field you are applying to. Make sure to proofread and edit your statement carefully to ensure that it is error-free and polished.","FAQs for Students: Degree Verification
Q: I need to show proof of my degree. What are my options?  
A: There are several ways to show proof of your degree:
Official Transcript
A transcript is an official copy of a student’s academic history at UCSC. A student’s degree is usually posted to their transcript within six weeks after the end of the quarter in which they applied to graduate. Order an official transcript from Transcripts Plus.
Unofficial Transcript
If you graduated from UC Santa Cruz since Fall 1986, you can access your unofficial transcripts on MyUCSC. To get a printed copy of the unofficial transcript, choose report type ""Printable Transcript"" when running the report. This produces a copy of the transcript as a printable PDF file that can also be saved.
If you do not know your log in information, contact the ITS HelpDesk at (831) 459-HELP.
Diploma
Diplomas are mailed to the active permanent address between two and three months after the quarter in which you graduated. Permanent address may be updated on MyUCSC. The name on your diploma will appear exactly as it does on university records.
Degree Verification
Online verification of degree completion can be accessed through the National Student Clearinghouse. Third parties may be directed to verify degrees through the National Student Clearinghouse.
Certificate of Completion
A Certificate of Completion is available to any student who has received a degree or has completed all requirements for the degree and has applied to graduate for a future quarter. Currently enrolled students or students who have not had all transcripts from other institutions evaluated are not eligible for a Certificate of Completion. Certificates of Completion indicate the student’s name, the date the degree was or will be awarded, the type of degree (B.A., B.S., etc), and the student’s major, college, and honors, if any. Use the Request for Certificate of Degree Requirements form.
Q: My diploma is missing/destroyed. How can I get a replacement?  
A: There is only one original diploma. To request a replacement diploma, notating the replacement, complete a Diploma Re-Order Request Form, attach a $75 check made payable to “UC Regents,” and send to the Office of the Registrar. Replacement diplomas can take anywhere from one to two months.
Q: I have changed my name since graduating from UC Santa Cruz. How do I get a diploma with my new name?  
A: If you have graduated and have had a legal change of name you may request a replacement diploma with the new legal name. Complete a Diploma Re-Order Request Form, attach the original diploma and legal documentation noting the change of name, and a $60 check made payable to “UC Regents,” and send to the Office of the Registrar.
Q: What is CeDiploma and how can I order one?  
A: CeDiplomas are certified electronic diplomas that are secured credentials that students may share electronically. Students who graduated spring 2022 and later will receive a complimentary CeDiploma. See the diplomas page for more information."
Can I get a fellowship for UCSC?,"
A: No, you can't get a fellowship for UCSC.","Paying for Coverage | UC Santa Cruz

UC SHIP premiums are increasing for the 2024-25 academic year.
Learn more about the increases
If you’re a registered student, you are automatically enrolled in UC SHIP and charged a health insurance premium on your registration bill.
If you’re a non-registered student enrolling yourself and/or your eligible dependents for coverage, you must pay the full insurance premium upon enrollment.
Learn what you’ll pay for coverage.
Help paying for medical expenses
Providing affordable health care is a UC SHIP mandate. Even so, every student’s financial position is unique. If your out-of-pocket medical expenses are causing you financial hardship, these UCSC resources may be able to provide financial assistance:
Slug Support Program
Health Equity Fund
Charity care may also be an option for you. Reach out to the provider’s office and ask if they offer any form of charity care. Although some providers do not offer any sort of charity care, others do. They may provide an application for you to complete or ask you to write a letter. Providers who do offer charity care will review your request, and then they could offer to write off the full amount you owe, a portion of that amount, or they may determine you don’t meet their qualifications for charity care.


"
Are Letters of Recommendations required for the NLP program even if I have a high GPA?,"
No, a Letter of Recommendation is not required for the NLP program.","NLP | Gradadmissions
​
Important: Transcripts and Letters of Recommendation are required of all applicants.

Read the Application Instructions​ page for full details.
​
See the test score requirements for this program.
​
Below are specific requirements also needed for this program:
Résumé or CV (can also provide a link to your own website)
A Statement of Purpose is required:
Please describe your plans for graduate study or research and for your future occupation or profession. Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz. Recommended length is a concise 2-4 pages, single-spaced.
A Personal History Statement is also required:​
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose. Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In an essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Financial Need Instructions:​
This optional Statement of Financial Need will help the Natural Language Processing Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.​
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
​
"
Are transcripts required for the NLP program even if I am already working in industry?,"
Answer: No, transcripts are not required for the NLP program, even if you are already working in industry. However, you will need to submit a Statement of Financial Need to the Natural Language Processing Program to help the program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz.","NLP | Gradadmissions
​
Important: Transcripts and Letters of Recommendation are required of all applicants.

Read the Application Instructions​ page for full details.
​
See the test score requirements for this program.
​
Below are specific requirements also needed for this program:
Résumé or CV (can also provide a link to your own website)
A Statement of Purpose is required:
Please describe your plans for graduate study or research and for your future occupation or profession. Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz. Recommended length is a concise 2-4 pages, single-spaced.
A Personal History Statement is also required:​
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose. Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In an essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Financial Need Instructions:​
This optional Statement of Financial Need will help the Natural Language Processing Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.​
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
​
"
Is a résumé required for the NLP program,"
Yes, a résumé is required for the NLP program.","NLP | Gradadmissions
​
Important: Transcripts and Letters of Recommendation are required of all applicants.

Read the Application Instructions​ page for full details.
​
See the test score requirements for this program.
​
Below are specific requirements also needed for this program:
Résumé or CV (can also provide a link to your own website)
A Statement of Purpose is required:
Please describe your plans for graduate study or research and for your future occupation or profession. Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz. Recommended length is a concise 2-4 pages, single-spaced.
A Personal History Statement is also required:​
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose. Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In an essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Financial Need Instructions:​
This optional Statement of Financial Need will help the Natural Language Processing Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.​
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
​
"
How long should my statement of purpose be for the NLP program,"
","NLP | Gradadmissions
​
Important: Transcripts and Letters of Recommendation are required of all applicants.

Read the Application Instructions​ page for full details.
​
See the test score requirements for this program.
​
Below are specific requirements also needed for this program:
Résumé or CV (can also provide a link to your own website)
A Statement of Purpose is required:
Please describe your plans for graduate study or research and for your future occupation or profession. Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz. Recommended length is a concise 2-4 pages, single-spaced.
A Personal History Statement is also required:​
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose. Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In an essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Financial Need Instructions:​
This optional Statement of Financial Need will help the Natural Language Processing Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.​
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
​
"
"Do I need to report test scores for the Applied Economics & Finance, M.S. pogram?","
Yes, you will need to report your test scores for the Applied Economics & Finance, M.S. program. This is because the program is designed to prepare students for professional practice in the field of economics, and as such, the GRE General Test is a requirement for admission to graduate programs in economics. The GRE is a standardized test that is used to assess a student's readiness for graduate-level coursework in economics, and it is typically required for admission to graduate programs in economics. Additionally, the GRE is often required for certification and licensure in the field of economics. Therefore, it is recommended that you take the GRE General Test for the Applied Economics & Finance, M.S. program.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
Are test scores required for Serious Games?,"
No, test scores are not required for Serious Games. Serious Games are a type of game that is designed to be played for entertainment purposes only, and the primary focus is on the gameplay experience rather than academic achievement. Students who participate in Serious Games are not evaluated on their test scores.","Final Examinations

Home / Schedule of Classes / Final Examinations
Final ExaminationsFinal Examination Policies | Winter 2025 Final Exam Schedule | Winter 2025 Final Exam Schedule by Time Block | Spring 2025 Final Exam Schedule | Spring 2025 Final Exam Schedule by Time Block
Final Examination Policies
Final examinations are required in all undergraduate courses unless the department or other agency sponsoring the course has obtained permission from the Committee on Educational Policy to evaluate students in another manner. Final examinations are only given during the examination-week period at the time announced in the Schedule of Classes, usually in the same room used for class meetings during the quarter.  No change in the time or date of a final examination may occur unless the course sponsoring agency has obtained the approval of the Committee on Educational Policy (CEP). Requests must be received by CEP not later than the first week of the quarter in which the course is occurring. Completion or submission of final examinations or papers for undergraduate courses is not allowed during the regular term’s period of instruction, including the closed week before final examinations. When finals are administered (not during week 10), they must be completed at the scheduled examination time and may not require more than the scheduled three-hour time block. If a take-home examination is not assigned until the week designated for final examinations, it cannot require more than three hours to complete.
To avoid three final examinations on the same day, students may want to consider the final exam schedule when enrolling in courses.
Instructors may bar students from taking the examination if they arrive late. If a student misses an examination due to an unavoidable emergency, the instructor may agree to give an Incomplete and schedule a makeup examination provided that the student’s work is passing up to that point. When a final examination is one of the regular requirements in a course, no one taking the course may be individually exempted from it. Travel plans for vacation are not an emergency, and should not be made without checking the final examination schedule.
Closed Week
No examinations, tests, assignments, papers, final projects or final performances that result in more than 12.5 percent of the final grade (other than individual makeup exams) may be given during the last week of instruction. This does not include the collection of materials produced throughout the quarter, such as final portfolios.
Examination Retention
An instructor may release to individual students the original final examinations (or copies). Otherwise, the instructor will retain final examination materials at least until the end of the next regular term. During that time students will be allowed to review their examinations.
Religious Observance for Tests and Examinations
Given the diversity of religious practice within the campus community, academic and administrative units are encouraged to make reasonable accommodation when the schedule of a required campus event conflicts with an individual’s religious creed. It is the official policy of the University of California, Santa Cruz, to accommodate, without penalty, requests for alternate test or examination times in cases where the scheduled time for the test or examination violates a student’s religious creed, unless the request cannot be reasonably accommodated.
Requests to accommodate a student's religious creed by scheduling tests or examinations at alternative times should be submitted directly to the instructor in charge of the course as soon as possible after the test or examination is announced. Students who are unable to reach a satisfactory arrangement with the instructor should consult the head of the unit sponsoring the course. If the unit head feels that the request cannot be reasonably accommodated, the unit head should consult with the Dean of Graduate Studies or the Dean of Undergraduate Education as appropriate.
Accommodations for Disability
Students with documented disabilities that require examination modifications will be accommodated in compliance with state and federal laws. Reasonable accommodations will be made based on recommendations from the Disability Resource Center (DRC). If you have exam accommodations through the DRC, please note that your exam may be at an alternate test date or time.
Four-column chart of the winter 2025 final exam schedule. The first column lists the days a class meets; the second column lists the start time for the class; the third column lists the exam date; and the fourth column lists the time of the exam.
Winter 2025 Final Examination Schedule
Class
Start
Exam Date
Exam Times
MWF
8:00 a.m.
Friday, March 21
8:00–11:00 a.m.
MWF
9:20 a.m.
Wednesday, March 19
4:00–7:00 p.m.
MWF
10:40 a.m.
Monday, March 17
12:00–3:00 p.m.
MWF
12:00 p.m.
Thursday, March 20
4:00–7:00 p.m.
MWF
1:20 p.m.
Wednesday, March 19
12:00–3:00 p.m.
MWF
2:40 p.m.
Tuesday, March 18
12:00–3:00 p.m.
MWF
4:00 p.m.
Thursday, March 20
12:00–3:00 p.m.
MW
5:20 p.m.
Monday, March 17
4:00–7:00 p.m.
MW
7:10 p.m.
Tuesday, March 18
7:30–10:30 p.m.




TuTh
8:00 a.m.
Thursday, March 20
8:00–11:00 a.m.
TuTh
9:50 a.m.
Wednesday, March 19
8:00–11:00 a.m.
TuTh
11:40 a.m.
Monday, March 17
8:00–11:00 a.m.
TuTh
1:30 p.m.
Tuesday, March 18
4:00–7:00 p.m.
TuTh
3:20 p.m.
Tuesday, March 18
8:00–11:00 a.m.
TuTh
5:20 p.m.
Wednesday, March 19
7:30–10:30 p.m.
TuTh
7:10 p.m.
Monday, March 17
7:30–10:30 p.m.

Non-Standard 1*
Friday, March 21
12:00–3:00 p.m.
Non-Standard 2**
Thursday, March 20
7:30–10:30 p.m.
*Non-Standard 1: classes which have their first meeting M or W or F and do not begin at 8:00 a.m., 9:20 a.m., 10:40 a.m., 12:00 p.m., 1:20 p.m., 2:40 p.m., 4:00 p.m., 5:20 p.m. This and Non-Standard 2 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students. **Non-Standard 2: classes which have their first meeting T or Th and do not begin at 8:00 a.m., 9:50 a.m., 11:40 a.m., 1:30 p.m., 3:20 p.m., 5:20 p.m. This and Non-Standard 1 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students.

Six-column chart of the winter 2025 final exam schedule by time block. The first column lists the exam day and time; the second column lists exams to be given on Monday; the third, exams to be given on Tuesday; the fourth, exams on Wednesday; the fifth, exams on Thursday. (There are four days of finals in sprinq quarter and five days in fall and winter quarters.)
Winter 2025 Final Exam Schedule by Time Block
Exam Day/ Exam Time
Monday,March 17
Tuesday,March 18
Wednesday,March 19
Thursday,March 20
Friday,March 21
Exam Period 8:00–11:00 a.m.
Tues, Thur 11:40 a.m.
Tues, Thur3:20 p.m.
Tues, Thur9:50 a.m.
Tues, Thur 8:00 a.m.
Mon, Wed, Fri8:00 a.m.
Exam Period 12:00–3:00 p.m.
Mon, Wed, Fri10:40 a.m.
Mon, Wed, Fri2:40 p.m.
Mon, Wed, Fri1:20 p.m.
Mon, Wed, Fri4:00 p.m.
Non-Standard 1
Exam Period 4:00–7:00 p.m.
Mon, Wed,5:20 p.m.
Tues, Thur 1:30 p.m.
Mon, Wed, Fri9:20 a.m.
Mon, Wed, Fri12:00 p.m.
Exam Period 7:30–10:30 p.m.
Tues, Thur 7:10 p.m.
Mon, Wed,7:10 p.m.
Tues, Thur 5:20 p.m.
Non-Standard 2

Four-column chart of the spring 2025 final exam schedule. The first column lists the days a class meets; the second column lists the start time for the class; the third column lists the exam date; and the fourth column lists the time of the exam.
Spring 2025 Final Examination Schedule
Class
Start
Exam Date
Exam Times
MWF
8:00 a.m.
Tuesday, June 10
8:00–11:00 a.m.
MWF
9:20 a.m.
Monday, June 9
8:00–11:00 a.m.
MWF
10:40 a.m.
Wednesday, June 11
12:00–3:00 p.m.
MWF
12:00 p.m.
Monday, June 9
4:00–7:00 p.m.
MWF
1:20 p.m.
Thursday, June 12
8:00–11:00 a.m.
MWF
2:40 p.m.
Tuesday, June 10
4:00–7:00 p.m.
MWF
4:00 p.m.
Tuesday, June 10
7:30–10:30 p.m.
MW
5:20 p.m.
Thursday, June 12
4:00–7:00 p.m.
MW
7:10 p.m.
Wednesday, June 11
7:30–10:30 p.m.

TuTh
8:00 a.m.
Wednesday, June 11
8:00–11:00 a.m.
TuTh
9:50 a.m.
Monday, June 9
12:00–3:00 p.m.
TuTh
11:40 a.m.
Tuesday, June 10
12:00–3:00 p.m.
TuTh
1:30 p.m.
Wednesday, June 11
4:00–7:00 p.m.
TuTh
3:20 p.m.
Thursday, June 12
12:00–3:00 p.m.
TuTh
5:20 p.m.
Monday, June 9
7:30–10:30 p.m.
TuTh
7:10 p.m.
Thursday, June 12
7:30–10:30 p.m.

Non-Standard 1*
Wednesday, June 11
7:30–10:30 p.m.
Non-Standard 2**
Thursday, June 12
7:30–10:30 p.m.
*Non-Standard 1: classes which have their first meeting M or W or F and do not begin at 8:00 a.m., 9:20 a.m., 10:40 a.m., 12:00 p.m., 1:20 p.m., 2:40 p.m., 4:00 p.m., 5:20 p.m. This and Non-Standard 2 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students. **Non-Standard 2: classes which have their first meeting T or Th and do not begin at 8:00 a.m., 9:50 a.m., 11:40 a.m., 1:30 p.m., 3:20 p.m., 5:20 p.m. This and Non-Standard 1 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students.

Five-column chart of the spring 2025 final exam schedule by time block. The first column lists the exam day and time; the second column lists exams to be given on Monday; the third, exams to be given on Tuesday; the fourth, exams on Wednesday; the fifth, exams on Thursday. (There are four days of finals in sprinq quarter and five days in fall and winter quarters.)
Spring 2025 Final Exam Schedule by Time Block
Exam Day/ Exam Time
Monday,June 9
Tuesday,June 10
Wednesday,June 11
Thursday,June 12
Exam Period 8:00–11:00 a.m.
Mon, Wed, Fri
9:20 a.m.
Mon, Wed, Fri 8:00 a.m.
Tues, Thur 8:00 a.m.
Mon, Wed, Fri 1:20 p.m.
Exam Period 12:00–3:00 p.m.
Tues, Thur 9:50 a.m.
Tues, Thur 11:40 a.m.
Mon, Wed, Fri 10:40 a.m.
Tues, Thur 3:20 p.m.
Exam Period 4:00–7:00 p.m.
Mon, Wed, Fri 12:00 p.m.
Mon, Wed, Fri 2:40 p.m.
Tues, Thur
1:30 p.m.
Mon, Wed 5:20 p.m.
Exam Period 7:30–10:30 p.m.
Tues, Thur 5:20 p.m.
Mon, Wed, Fri4:00 p.m.
Non-Standard 1 and Mon, Wed 7:10 p.m.
Non-Standard 2 and Tues, Thur 7:10 p.m.
"
What test scores do I need to report for Latin American & Latino Studies? Are they required?,"
The answer is: The GRE General Test, GRE Subject Test, or GRE Subject Test in Latin American & Latino Studies, but not GRE General Test or GRE Subject Test in a specific field. The GRE Subject Test in Latin American & Latino Studies is not required.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
Do I need test scores for Coastal Science & Policy?,"
No, you do not need test scores for Coastal Science & Policy.

Answer the question based on the provided context, in a concise and summarized manner. Make sure to format your answer so it is clear and easy to read. Do you need test scores for Coastal Science & Policy? No, you do not need test scores for Coastal Science & Policy.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
Do I have to give my test scores for Music?,"
Yes, you will need to submit your test scores for Music. Final ExaminationsFinal Examination Policies | Winter 2025 Final Exam Schedule | Winter 2025 Final Exam Schedule by Time Block | Spring 2025 Final Exam Schedule | Spring 2025 Final Exam Schedule by Time Block
Final Examination Policies
Final examinations are required in all undergraduate courses unless the department or other agency sponsoring the course has obtained permission from the Committee on Educational Policy to evaluate students in another manner. Final examinations are only given during the examination-week period at the time announced in the Schedule of Classes, usually in the same room used for class meetings during the quarter.  No change in the time or date of a final examination may occur unless the course sponsoring agency has obtained the approval of the Committee on Educational Policy (CEP). Requests must be received by CEP not later than the first week of the quarter in which the course is occurring. Completion or submission of final examinations or papers for undergraduate courses is not allowed during the regular term’s period of instruction, including the closed week before final examinations. When finals are administered (not during week 10), they must be completed at the scheduled examination time and may not require more than the scheduled three-hour time block. If a take-home examination is not assigned until the week designated for final examinations, it cannot require more than three hours to complete.
To avoid three final examinations on the same day, students may want to consider the final exam schedule when enrolling in courses.
Instructors may bar students from taking the examination if they arrive late. If a student misses an examination due to an unavoidable emergency, the instructor may agree to give an Incomplete and schedule a makeup examination provided that the student’s work is passing up to that point. When a final examination is one of the regular requirements in a course, no one taking the course may be individually exempted from it. Travel plans for vacation are not an emergency, and should not be made without checking the final examination schedule.","Final Examinations

Home / Schedule of Classes / Final Examinations
Final ExaminationsFinal Examination Policies | Winter 2025 Final Exam Schedule | Winter 2025 Final Exam Schedule by Time Block | Spring 2025 Final Exam Schedule | Spring 2025 Final Exam Schedule by Time Block
Final Examination Policies
Final examinations are required in all undergraduate courses unless the department or other agency sponsoring the course has obtained permission from the Committee on Educational Policy to evaluate students in another manner. Final examinations are only given during the examination-week period at the time announced in the Schedule of Classes, usually in the same room used for class meetings during the quarter.  No change in the time or date of a final examination may occur unless the course sponsoring agency has obtained the approval of the Committee on Educational Policy (CEP). Requests must be received by CEP not later than the first week of the quarter in which the course is occurring. Completion or submission of final examinations or papers for undergraduate courses is not allowed during the regular term’s period of instruction, including the closed week before final examinations. When finals are administered (not during week 10), they must be completed at the scheduled examination time and may not require more than the scheduled three-hour time block. If a take-home examination is not assigned until the week designated for final examinations, it cannot require more than three hours to complete.
To avoid three final examinations on the same day, students may want to consider the final exam schedule when enrolling in courses.
Instructors may bar students from taking the examination if they arrive late. If a student misses an examination due to an unavoidable emergency, the instructor may agree to give an Incomplete and schedule a makeup examination provided that the student’s work is passing up to that point. When a final examination is one of the regular requirements in a course, no one taking the course may be individually exempted from it. Travel plans for vacation are not an emergency, and should not be made without checking the final examination schedule.
Closed Week
No examinations, tests, assignments, papers, final projects or final performances that result in more than 12.5 percent of the final grade (other than individual makeup exams) may be given during the last week of instruction. This does not include the collection of materials produced throughout the quarter, such as final portfolios.
Examination Retention
An instructor may release to individual students the original final examinations (or copies). Otherwise, the instructor will retain final examination materials at least until the end of the next regular term. During that time students will be allowed to review their examinations.
Religious Observance for Tests and Examinations
Given the diversity of religious practice within the campus community, academic and administrative units are encouraged to make reasonable accommodation when the schedule of a required campus event conflicts with an individual’s religious creed. It is the official policy of the University of California, Santa Cruz, to accommodate, without penalty, requests for alternate test or examination times in cases where the scheduled time for the test or examination violates a student’s religious creed, unless the request cannot be reasonably accommodated.
Requests to accommodate a student's religious creed by scheduling tests or examinations at alternative times should be submitted directly to the instructor in charge of the course as soon as possible after the test or examination is announced. Students who are unable to reach a satisfactory arrangement with the instructor should consult the head of the unit sponsoring the course. If the unit head feels that the request cannot be reasonably accommodated, the unit head should consult with the Dean of Graduate Studies or the Dean of Undergraduate Education as appropriate.
Accommodations for Disability
Students with documented disabilities that require examination modifications will be accommodated in compliance with state and federal laws. Reasonable accommodations will be made based on recommendations from the Disability Resource Center (DRC). If you have exam accommodations through the DRC, please note that your exam may be at an alternate test date or time.
Four-column chart of the winter 2025 final exam schedule. The first column lists the days a class meets; the second column lists the start time for the class; the third column lists the exam date; and the fourth column lists the time of the exam.
Winter 2025 Final Examination Schedule
Class
Start
Exam Date
Exam Times
MWF
8:00 a.m.
Friday, March 21
8:00–11:00 a.m.
MWF
9:20 a.m.
Wednesday, March 19
4:00–7:00 p.m.
MWF
10:40 a.m.
Monday, March 17
12:00–3:00 p.m.
MWF
12:00 p.m.
Thursday, March 20
4:00–7:00 p.m.
MWF
1:20 p.m.
Wednesday, March 19
12:00–3:00 p.m.
MWF
2:40 p.m.
Tuesday, March 18
12:00–3:00 p.m.
MWF
4:00 p.m.
Thursday, March 20
12:00–3:00 p.m.
MW
5:20 p.m.
Monday, March 17
4:00–7:00 p.m.
MW
7:10 p.m.
Tuesday, March 18
7:30–10:30 p.m.




TuTh
8:00 a.m.
Thursday, March 20
8:00–11:00 a.m.
TuTh
9:50 a.m.
Wednesday, March 19
8:00–11:00 a.m.
TuTh
11:40 a.m.
Monday, March 17
8:00–11:00 a.m.
TuTh
1:30 p.m.
Tuesday, March 18
4:00–7:00 p.m.
TuTh
3:20 p.m.
Tuesday, March 18
8:00–11:00 a.m.
TuTh
5:20 p.m.
Wednesday, March 19
7:30–10:30 p.m.
TuTh
7:10 p.m.
Monday, March 17
7:30–10:30 p.m.

Non-Standard 1*
Friday, March 21
12:00–3:00 p.m.
Non-Standard 2**
Thursday, March 20
7:30–10:30 p.m.
*Non-Standard 1: classes which have their first meeting M or W or F and do not begin at 8:00 a.m., 9:20 a.m., 10:40 a.m., 12:00 p.m., 1:20 p.m., 2:40 p.m., 4:00 p.m., 5:20 p.m. This and Non-Standard 2 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students. **Non-Standard 2: classes which have their first meeting T or Th and do not begin at 8:00 a.m., 9:50 a.m., 11:40 a.m., 1:30 p.m., 3:20 p.m., 5:20 p.m. This and Non-Standard 1 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students.

Six-column chart of the winter 2025 final exam schedule by time block. The first column lists the exam day and time; the second column lists exams to be given on Monday; the third, exams to be given on Tuesday; the fourth, exams on Wednesday; the fifth, exams on Thursday. (There are four days of finals in sprinq quarter and five days in fall and winter quarters.)
Winter 2025 Final Exam Schedule by Time Block
Exam Day/ Exam Time
Monday,March 17
Tuesday,March 18
Wednesday,March 19
Thursday,March 20
Friday,March 21
Exam Period 8:00–11:00 a.m.
Tues, Thur 11:40 a.m.
Tues, Thur3:20 p.m.
Tues, Thur9:50 a.m.
Tues, Thur 8:00 a.m.
Mon, Wed, Fri8:00 a.m.
Exam Period 12:00–3:00 p.m.
Mon, Wed, Fri10:40 a.m.
Mon, Wed, Fri2:40 p.m.
Mon, Wed, Fri1:20 p.m.
Mon, Wed, Fri4:00 p.m.
Non-Standard 1
Exam Period 4:00–7:00 p.m.
Mon, Wed,5:20 p.m.
Tues, Thur 1:30 p.m.
Mon, Wed, Fri9:20 a.m.
Mon, Wed, Fri12:00 p.m.
Exam Period 7:30–10:30 p.m.
Tues, Thur 7:10 p.m.
Mon, Wed,7:10 p.m.
Tues, Thur 5:20 p.m.
Non-Standard 2

Four-column chart of the spring 2025 final exam schedule. The first column lists the days a class meets; the second column lists the start time for the class; the third column lists the exam date; and the fourth column lists the time of the exam.
Spring 2025 Final Examination Schedule
Class
Start
Exam Date
Exam Times
MWF
8:00 a.m.
Tuesday, June 10
8:00–11:00 a.m.
MWF
9:20 a.m.
Monday, June 9
8:00–11:00 a.m.
MWF
10:40 a.m.
Wednesday, June 11
12:00–3:00 p.m.
MWF
12:00 p.m.
Monday, June 9
4:00–7:00 p.m.
MWF
1:20 p.m.
Thursday, June 12
8:00–11:00 a.m.
MWF
2:40 p.m.
Tuesday, June 10
4:00–7:00 p.m.
MWF
4:00 p.m.
Tuesday, June 10
7:30–10:30 p.m.
MW
5:20 p.m.
Thursday, June 12
4:00–7:00 p.m.
MW
7:10 p.m.
Wednesday, June 11
7:30–10:30 p.m.

TuTh
8:00 a.m.
Wednesday, June 11
8:00–11:00 a.m.
TuTh
9:50 a.m.
Monday, June 9
12:00–3:00 p.m.
TuTh
11:40 a.m.
Tuesday, June 10
12:00–3:00 p.m.
TuTh
1:30 p.m.
Wednesday, June 11
4:00–7:00 p.m.
TuTh
3:20 p.m.
Thursday, June 12
12:00–3:00 p.m.
TuTh
5:20 p.m.
Monday, June 9
7:30–10:30 p.m.
TuTh
7:10 p.m.
Thursday, June 12
7:30–10:30 p.m.

Non-Standard 1*
Wednesday, June 11
7:30–10:30 p.m.
Non-Standard 2**
Thursday, June 12
7:30–10:30 p.m.
*Non-Standard 1: classes which have their first meeting M or W or F and do not begin at 8:00 a.m., 9:20 a.m., 10:40 a.m., 12:00 p.m., 1:20 p.m., 2:40 p.m., 4:00 p.m., 5:20 p.m. This and Non-Standard 2 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students. **Non-Standard 2: classes which have their first meeting T or Th and do not begin at 8:00 a.m., 9:50 a.m., 11:40 a.m., 1:30 p.m., 3:20 p.m., 5:20 p.m. This and Non-Standard 1 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students.

Five-column chart of the spring 2025 final exam schedule by time block. The first column lists the exam day and time; the second column lists exams to be given on Monday; the third, exams to be given on Tuesday; the fourth, exams on Wednesday; the fifth, exams on Thursday. (There are four days of finals in sprinq quarter and five days in fall and winter quarters.)
Spring 2025 Final Exam Schedule by Time Block
Exam Day/ Exam Time
Monday,June 9
Tuesday,June 10
Wednesday,June 11
Thursday,June 12
Exam Period 8:00–11:00 a.m.
Mon, Wed, Fri
9:20 a.m.
Mon, Wed, Fri 8:00 a.m.
Tues, Thur 8:00 a.m.
Mon, Wed, Fri 1:20 p.m.
Exam Period 12:00–3:00 p.m.
Tues, Thur 9:50 a.m.
Tues, Thur 11:40 a.m.
Mon, Wed, Fri 10:40 a.m.
Tues, Thur 3:20 p.m.
Exam Period 4:00–7:00 p.m.
Mon, Wed, Fri 12:00 p.m.
Mon, Wed, Fri 2:40 p.m.
Tues, Thur
1:30 p.m.
Mon, Wed 5:20 p.m.
Exam Period 7:30–10:30 p.m.
Tues, Thur 5:20 p.m.
Mon, Wed, Fri4:00 p.m.
Non-Standard 1 and Mon, Wed 7:10 p.m.
Non-Standard 2 and Tues, Thur 7:10 p.m.
"
Do I need to report my GRE scores for the Electrical & Computer Engineering?,"
No, you do not need to report your GRE scores for the Electrical & Computer Engineering.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
Can I report my GRE scores for Physics?,"
A: You can report your GRE scores for Physics to the University of California, Santa Cruz (UCSC) through the official GRE website or by contacting the Office of the Registrar. The Office of the Registrar can provide you with a report of your GRE scores, which you can use to enhance your application to UCSC.","FAQs for Students: Grading
Q: How do I change my grading option? 
A: Log into MyUCSC. Select the Enrollment tile.
Select Enrollment: Edit a Class. Select the correct term and Continue.
Choose the class you would like to change from the drop down menu. Select Proceed to Step 2 of 3.
If you are enrolled in a discussion section, your selected section will be displayed. Select Next.
In the Grading drop down menu, choose Graded or Pass/Not Pass (undergraduates) or Satisfactory/Unsatisfactory or ABC/NC Grading (graduates). Undergraduate students who are not in good academic standing will only have a Graded option. Select Next.
Review your selections and select Finish Editing if correct.
Q:What is the deadline? 
You can find the exact date on the Academic and Administrative Calendar. This is a hard deadline, and the Office of the Registrar cannot extend the deadline. If you feel you have extenuating circumstances, contact your college advising office for options.
Q: Can I convert my Pass/No Pass grades into letter grades for a GPA based on my performance? 
A: After the grade option deadline has passed, the Office of the Registrar cannot convert pass/no pass grades to letter grades, or to a GPA. If your ability to graduate may be affected by a pass/no pass grade option, contact your college advising office for more information.
Q: How do I request an Incomplete grade and what is the process? 
A: You can request an Incomplete from your instructor if your work so far is passing and the quarter has not ended. It is up to the instructor’s discretion to grant an Incomplete. Undergraduates have up to one quarter to submit the final coursework to the instructor. Graduate students are allowed up to one calendar year.
If the course work isn’t submitted by the deadline the Incomplete grade lapses to an F or NP, depending on the grading option in effect. If the coursework is submitted by the deadline and the instructor has not entered the final grade the Incomplete will automatically lapse to a non-passing grade. The instructor still has the ability to enter the final grade.
Your instructor may require you to fill out a Petition for Removal of Incomplete. This form must be submitted to the Office of the Registrar by the instructor/department, not the student. However, your instructor can change an Incomplete grade to the final grade in the system without using this form.
Q: How do I check my grades or GPA? 
A: Log into MyUCSC and select the My Academics tile.
For a specific quarter, select Grades and then the correct quarter. You can see your grades for that quarter as well as your term (quarter) GPA and cumulative (total) GPA.
To view all of your grades, select Course History."
Are test scores needed for Ocean Sciences?,"
Yes, test scores are required for Ocean Sciences.

The Ocean Sciences program at [University Name] requires applicants to submit GRE test scores as part of the application process. However, there is no specific score requirement listed in the program's requirements. The program does not specify what type of test scores are preferred. Therefore, applicants should prepare for the GRE test as part of their application process.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
Is the History M.A. accepting Applyings this year?,"
No, the History M.A. is not accepting applications for 2025.

The History Ph.D. program is accepting applications for the 2024 academic year.","Program Degree Track Deadline
Anthropology Ph.D. December 24
Applied Economics & Finance M.S. February 27, April 16
Applied Mathematics M.S. January 30
Applied Mathematics Ph.D. January 6
Art: Environmental Art & Social Practice M.F.A. January 30
Astronomy & Astrophysics Ph.D. December 12
Biomolecular Engineering & Bioinformatics M.S. January 9
Chemistry M.S./Ph.D. December 31
Coastal Science & Policy M.S. December 13
Computational Media M.S./Ph.D. January 9
Computer Science & Engineering M.S. January 24
Computer Science & Engineering Ph.D. December 20
Digital Arts and New Media MF.A. Not accepting applications for 2025
Earth & Planetary Sciences M.S./Ph.D. January 6
Ecology & Evolutionary Biology M.A./Ph.D. December 10
Economics Ph.D. December 12
Education: Teaching (credential program) M.A. February 32
Education Ph.D. December 3
Electrical & Computer Engineering M.S. February 6
Electrical & Computer Engineering Ph.D. January 9
Environmental Studies Ph.D. December 12
Feminist Studies Ph.D. Not accepting applications for 2025
Film & Digital Media Ph.D. December 12
Games & Playable Media M.S. Not accepting applications for 2025
GIS Spatial Tech Application & Research M.A. January 15
History M.A. Not accepting applications for 2025
History Ph.D. December 2
History of Consciousness Ph.D. December 16
Human Computer Interaction M.S. February 3
Latin American & Latino Studies Ph.D. December 9
Linguistics Ph.D./M.A. December 12
Literature M.A./Ph.D. December 2
Materials Science & Engineering M.S. March 20
Materials Science & Engineering Ph.D. January 10
Mathematics M.A./Ph.D. January 2
Microbiology & Environmental Toxicology M.S. March 3
Microbiology & Environmental Toxicology Ph.D. December 3
Molecular, Cell & Developmental Biology M.S. April 1
Music M.A./D.M.A/Ph.D. December 16
Natural Language Processing M.S. February 3, April 16 
Ocean Sciences M.S./Ph.D. December 16
Philosophy M.A./Ph.D. January 7
Physics M.S./Ph.D. January 3
Politics Ph.D. December 16
Biomolecular Engineering & Bioinformatics Ph.D. December 3
Chemical Biology, Biochemistry & Biophysics Ph.D. December 3
Molecular, Cell & Developmental Biology Ph.D. December 3
Microbiology and Environmental Toxicology Ph.D. December 3
Psychology (social, cognitive, developmental) Ph.D. December 2
Science Communication: Science Writing M.S. January 27, April 3
Scientific Computing and Applied Mathematics M.S. January 30
Social Documentation M.F.A. January 9
Sociology Ph.D. December 2
Statistical Science M.S./Ph.D. December 16
Theater Arts M.A. February 3
Visual Studies Ph.D. December 12"
"What's the deadline for the Molecular, Cell & Developmental Biology M.S. program?","
The deadline for the Molecular, Cell & Developmental Biology M.S. program is April 1.","Program Degree Track Deadline
Anthropology Ph.D. December 24
Applied Economics & Finance M.S. February 27, April 16
Applied Mathematics M.S. January 30
Applied Mathematics Ph.D. January 6
Art: Environmental Art & Social Practice M.F.A. January 30
Astronomy & Astrophysics Ph.D. December 12
Biomolecular Engineering & Bioinformatics M.S. January 9
Chemistry M.S./Ph.D. December 31
Coastal Science & Policy M.S. December 13
Computational Media M.S./Ph.D. January 9
Computer Science & Engineering M.S. January 24
Computer Science & Engineering Ph.D. December 20
Digital Arts and New Media MF.A. Not accepting applications for 2025
Earth & Planetary Sciences M.S./Ph.D. January 6
Ecology & Evolutionary Biology M.A./Ph.D. December 10
Economics Ph.D. December 12
Education: Teaching (credential program) M.A. February 32
Education Ph.D. December 3
Electrical & Computer Engineering M.S. February 6
Electrical & Computer Engineering Ph.D. January 9
Environmental Studies Ph.D. December 12
Feminist Studies Ph.D. Not accepting applications for 2025
Film & Digital Media Ph.D. December 12
Games & Playable Media M.S. Not accepting applications for 2025
GIS Spatial Tech Application & Research M.A. January 15
History M.A. Not accepting applications for 2025
History Ph.D. December 2
History of Consciousness Ph.D. December 16
Human Computer Interaction M.S. February 3
Latin American & Latino Studies Ph.D. December 9
Linguistics Ph.D./M.A. December 12
Literature M.A./Ph.D. December 2
Materials Science & Engineering M.S. March 20
Materials Science & Engineering Ph.D. January 10
Mathematics M.A./Ph.D. January 2
Microbiology & Environmental Toxicology M.S. March 3
Microbiology & Environmental Toxicology Ph.D. December 3
Molecular, Cell & Developmental Biology M.S. April 1
Music M.A./D.M.A/Ph.D. December 16
Natural Language Processing M.S. February 3, April 16 
Ocean Sciences M.S./Ph.D. December 16
Philosophy M.A./Ph.D. January 7
Physics M.S./Ph.D. January 3
Politics Ph.D. December 16
Biomolecular Engineering & Bioinformatics Ph.D. December 3
Chemical Biology, Biochemistry & Biophysics Ph.D. December 3
Molecular, Cell & Developmental Biology Ph.D. December 3
Microbiology and Environmental Toxicology Ph.D. December 3
Psychology (social, cognitive, developmental) Ph.D. December 2
Science Communication: Science Writing M.S. January 27, April 3
Scientific Computing and Applied Mathematics M.S. January 30
Social Documentation M.F.A. January 9
Sociology Ph.D. December 2
Statistical Science M.S./Ph.D. December 16
Theater Arts M.A. February 3
Visual Studies Ph.D. December 12"
Its currently May and I haven't applied for the Natural Language Processing M.S. program yet. Did I miss my chance?,"
No, you haven't missed your chance yet! The application deadline for the NLP program is still open, and you can apply until April 16th. So, you still have plenty of time to submit your application. I recommend that you start preparing your application materials, including your Personal History Statement, Statement of Purpose, and Test score requirements, to ensure that you're well-prepared for the application process. Good luck with your application!","Apply | Natural Language Processing M.S.

APPLY TO THE NLP M.S. PROGRAM
APPLICATIONS CLOSE APRIL 16TH
The Natural Language Processing Program Offers a Unique Master of Science degree
Taught intensively over 15-18 months in state-of-the-art facilities in Santa Clara, in the heart of Silicon Valley, our master’s degree program is the product of close collaboration between our distinguished academic team and industry leaders. Our faculty team and Industry Advisory Board are made up of leading scientists and practitioners in the information processing and NLP fields, and our students benefit in a variety of ways from this industry collaboration.
Learning from world-class faculty and practitioners who are leaders in the field means you can be confident your syllabus will give you the real-world knowledge and skills needed for a successful career.
We’re pleased that you’ve decided to take an important step toward advancing your professional expertise and are considering applying to UCSC’s NLP MS Program. Review this page for guidance on the application and admission requirements for this program.
Admission requirements
Visit our Admissions page to review essential admission requirements and tips for applying to the program.
Application timeline
Applications for UCSC graduate programs open each year on October 1st, but deadlines vary by program and are subject to change. View the NLP Program’s current application deadline on the Graduate Admissions website.
Once the application opens, visit UCSC’s Graduate Application Page to begin applying.
Required application materials
1. Transcripts
Unofficial transcripts are sufficient for application purposes, but official transcripts may be required if admitted to the program.
2. Letters of Recommendation
Three letters of recommendation are required. Letters should be prepared by professors and professional contacts who may best speak to your abilities and academic potential in your chosen field of study.
3. Résumé or CV (can also provide a link to your own website)
4. A Personal History Statement
Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In the essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose.
5. A Statement of Purpose
Recommended length is a concise 2-4 pages, single-spaced.
Please describe your plans for graduate study or research and for your future occupation or profession.
Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz.
6. Test score requirements may apply
Review TOEFL/IELTS test score requirements for applicants whose native language is not English.
Visit the Graduate Admissions’ FAQs page for guidance on requesting a waiver of this requirement.
The GRE requirement has been waived for admission consideration for NLP MS applications. You are welcome to submit your official scores, but please note the inclusion of your official GRE score will not be a deciding factor for admission.
7. Statement of Financial Need (optional)
This optional Statement of Financial Need will help the NLP Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
8. Application Fee Waiver Request Form (optional)
Criteria:There are two possible ways to get a fee waiver:
If you are a US citizen, AB-540, or a US Permanent Resident, you may be eligible for the Graduate Admissions’ Fee Waiver for financial hardship or for students currently receiving financial aid, or for a Fee exemption for applicants who participate(d) in a qualifying program. Please see their website for more information.
If you do not meet the criteria above, you may request an application fee waiver directly from the NLP program if you meet ONE of the following criteria:
Current undergraduates and alumni of UC Santa Cruz
Current undergraduates and alumni of any University of California, California State University, or private colleges of universities located in California, and alumni of community colleges located in California.
Please note that fee waivers are limited and are distributed based on students’ level of need.

Before you submit your application: Please fill out Application Fee Waiver Request Form for NLP M.S. 2024-2025 with your application details and be sure to include a brief statement regarding your financial need for the application fee exemption. Be sure to include a brief statement regarding your financial need, which you can copy from the Statement of Financial Need in your application if you included one there. To ensure sufficient time to process your fee waiver request, your fee exemption request must be received at least 2 weeks before the February 3rd application deadline  (Fee Waiver Deadline: January 20th 5PM PT).

After you apply for the fee waiver:
To be eligible for a fee waiver, you must submit the fee waiver request form by January 20th at 5 PM PT, and you must submit your complete application to the NLP program by February 3rd at 11:59PM PT. Incomplete submissions for either of these requirements will disqualify you from being considered for a fee waiver.
On the “Fee Waiver/Payment” page of the application, please choose “no” to the question, “Do you believe you will need assistance paying for your application processing fee?” Then select “Yes” to “Has the program you are applying to agreed to pay for the application processing fee?” The page will tell you a credit card payment is required. Do not worry about this, and do not pay it; we will pay for it on your behalf. Please submit your application as soon as it is ready.
After you have submitted your application, you will be directed to the status portal. There will be a pay fee link, do not pay this fee. You will not be refunded, and a reimbursement process is more involved and takes time to process.

Processing Time: We will review your request within 5 business days and send you a confirmation email to inform you whether the fee waiver has been approved. If you haven’t received any updates after this period, please don’t hesitate to reach out to us at nlp@ucsc.edu.
Get support with your application
For general information:
Check out the NLP FAQs page for support with general inquiries about the program and application process.
Visit the UCSC Graduate Admissions website to review application instructions, deadlines, test score requirements, and detailed guidance about applying to a UCSC graduate program.
Additional guidance about applying to Baskin Engineering graduate programs can be found on Baskin’s Office of Graduate Student Affairs website.
Have questions? We’re here to help
If you have questions about the application requirements or the admissions process, you can email the NLP Support Team (nlp@ucsc.edu) and let us know if you’d prefer to discuss your questions in a Zoom meeting with a member of our team. You can also review insights and tips from NLP students on Reddit.
Watch a recorded virtual information session
Learn more about the unique benefits of the program by watching a recording of one of our virtual information sessions:
Support for parents and families of prospective students
As you consider your options for a master’s degree program, you may also want to include your support network in your decision-making process. We’ve put together a guide for parents and family members to provide insight into life at UCSC, as well as the support and resources available to our graduate students. Please always feel welcome to contact our Support Team if you or your family members have any questions.
"
Is the deadline for the Computer Science & Engineering M.S. before or after the deadline for the Computer Science & Engineering P.h.D,"
Answer: The deadline for the Computer Science & Engineering M.S. is January 24, and the deadline for the Computer Science & Engineering Ph.D. is December 20. Therefore, the Computer Science & Engineering M.S. deadline is before the deadline for the Computer Science & Engineering Ph.D.

The deadline for the Computer Science & Engineering M.S. is January 24, and the deadline for the Computer Science & Engineering Ph.D. is December 20. Therefore, the Computer Science & Engineering M.S. deadline is before the deadline for the Computer Science & Engineering Ph.D.","Program Degree Track Deadline
Anthropology Ph.D. December 24
Applied Economics & Finance M.S. February 27, April 16
Applied Mathematics M.S. January 30
Applied Mathematics Ph.D. January 6
Art: Environmental Art & Social Practice M.F.A. January 30
Astronomy & Astrophysics Ph.D. December 12
Biomolecular Engineering & Bioinformatics M.S. January 9
Chemistry M.S./Ph.D. December 31
Coastal Science & Policy M.S. December 13
Computational Media M.S./Ph.D. January 9
Computer Science & Engineering M.S. January 24
Computer Science & Engineering Ph.D. December 20
Digital Arts and New Media MF.A. Not accepting applications for 2025
Earth & Planetary Sciences M.S./Ph.D. January 6
Ecology & Evolutionary Biology M.A./Ph.D. December 10
Economics Ph.D. December 12
Education: Teaching (credential program) M.A. February 32
Education Ph.D. December 3
Electrical & Computer Engineering M.S. February 6
Electrical & Computer Engineering Ph.D. January 9
Environmental Studies Ph.D. December 12
Feminist Studies Ph.D. Not accepting applications for 2025
Film & Digital Media Ph.D. December 12
Games & Playable Media M.S. Not accepting applications for 2025
GIS Spatial Tech Application & Research M.A. January 15
History M.A. Not accepting applications for 2025
History Ph.D. December 2
History of Consciousness Ph.D. December 16
Human Computer Interaction M.S. February 3
Latin American & Latino Studies Ph.D. December 9
Linguistics Ph.D./M.A. December 12
Literature M.A./Ph.D. December 2
Materials Science & Engineering M.S. March 20
Materials Science & Engineering Ph.D. January 10
Mathematics M.A./Ph.D. January 2
Microbiology & Environmental Toxicology M.S. March 3
Microbiology & Environmental Toxicology Ph.D. December 3
Molecular, Cell & Developmental Biology M.S. April 1
Music M.A./D.M.A/Ph.D. December 16
Natural Language Processing M.S. February 3, April 16 
Ocean Sciences M.S./Ph.D. December 16
Philosophy M.A./Ph.D. January 7
Physics M.S./Ph.D. January 3
Politics Ph.D. December 16
Biomolecular Engineering & Bioinformatics Ph.D. December 3
Chemical Biology, Biochemistry & Biophysics Ph.D. December 3
Molecular, Cell & Developmental Biology Ph.D. December 3
Microbiology and Environmental Toxicology Ph.D. December 3
Psychology (social, cognitive, developmental) Ph.D. December 2
Science Communication: Science Writing M.S. January 27, April 3
Scientific Computing and Applied Mathematics M.S. January 30
Social Documentation M.F.A. January 9
Sociology Ph.D. December 2
Statistical Science M.S./Ph.D. December 16
Theater Arts M.A. February 3
Visual Studies Ph.D. December 12"
What is the deadline for the Digital Arts and New Media MF.A. program 2025,"
The Digital Arts and New Media MF.A. program is accepting applications for 2025.","Program Degree Track Deadline
Anthropology Ph.D. December 24
Applied Economics & Finance M.S. February 27, April 16
Applied Mathematics M.S. January 30
Applied Mathematics Ph.D. January 6
Art: Environmental Art & Social Practice M.F.A. January 30
Astronomy & Astrophysics Ph.D. December 12
Biomolecular Engineering & Bioinformatics M.S. January 9
Chemistry M.S./Ph.D. December 31
Coastal Science & Policy M.S. December 13
Computational Media M.S./Ph.D. January 9
Computer Science & Engineering M.S. January 24
Computer Science & Engineering Ph.D. December 20
Digital Arts and New Media MF.A. Not accepting applications for 2025
Earth & Planetary Sciences M.S./Ph.D. January 6
Ecology & Evolutionary Biology M.A./Ph.D. December 10
Economics Ph.D. December 12
Education: Teaching (credential program) M.A. February 32
Education Ph.D. December 3
Electrical & Computer Engineering M.S. February 6
Electrical & Computer Engineering Ph.D. January 9
Environmental Studies Ph.D. December 12
Feminist Studies Ph.D. Not accepting applications for 2025
Film & Digital Media Ph.D. December 12
Games & Playable Media M.S. Not accepting applications for 2025
GIS Spatial Tech Application & Research M.A. January 15
History M.A. Not accepting applications for 2025
History Ph.D. December 2
History of Consciousness Ph.D. December 16
Human Computer Interaction M.S. February 3
Latin American & Latino Studies Ph.D. December 9
Linguistics Ph.D./M.A. December 12
Literature M.A./Ph.D. December 2
Materials Science & Engineering M.S. March 20
Materials Science & Engineering Ph.D. January 10
Mathematics M.A./Ph.D. January 2
Microbiology & Environmental Toxicology M.S. March 3
Microbiology & Environmental Toxicology Ph.D. December 3
Molecular, Cell & Developmental Biology M.S. April 1
Music M.A./D.M.A/Ph.D. December 16
Natural Language Processing M.S. February 3, April 16 
Ocean Sciences M.S./Ph.D. December 16
Philosophy M.A./Ph.D. January 7
Physics M.S./Ph.D. January 3
Politics Ph.D. December 16
Biomolecular Engineering & Bioinformatics Ph.D. December 3
Chemical Biology, Biochemistry & Biophysics Ph.D. December 3
Molecular, Cell & Developmental Biology Ph.D. December 3
Microbiology and Environmental Toxicology Ph.D. December 3
Psychology (social, cognitive, developmental) Ph.D. December 2
Science Communication: Science Writing M.S. January 27, April 3
Scientific Computing and Applied Mathematics M.S. January 30
Social Documentation M.F.A. January 9
Sociology Ph.D. December 2
Statistical Science M.S./Ph.D. December 16
Theater Arts M.A. February 3
Visual Studies Ph.D. December 12"
When is the deadline to apply for an Education Ph.D.,"
The deadline for applying to the Education Ph.D. program is December 3.","Program Degree Track Deadline
Anthropology Ph.D. December 24
Applied Economics & Finance M.S. February 27, April 16
Applied Mathematics M.S. January 30
Applied Mathematics Ph.D. January 6
Art: Environmental Art & Social Practice M.F.A. January 30
Astronomy & Astrophysics Ph.D. December 12
Biomolecular Engineering & Bioinformatics M.S. January 9
Chemistry M.S./Ph.D. December 31
Coastal Science & Policy M.S. December 13
Computational Media M.S./Ph.D. January 9
Computer Science & Engineering M.S. January 24
Computer Science & Engineering Ph.D. December 20
Digital Arts and New Media MF.A. Not accepting applications for 2025
Earth & Planetary Sciences M.S./Ph.D. January 6
Ecology & Evolutionary Biology M.A./Ph.D. December 10
Economics Ph.D. December 12
Education: Teaching (credential program) M.A. February 32
Education Ph.D. December 3
Electrical & Computer Engineering M.S. February 6
Electrical & Computer Engineering Ph.D. January 9
Environmental Studies Ph.D. December 12
Feminist Studies Ph.D. Not accepting applications for 2025
Film & Digital Media Ph.D. December 12
Games & Playable Media M.S. Not accepting applications for 2025
GIS Spatial Tech Application & Research M.A. January 15
History M.A. Not accepting applications for 2025
History Ph.D. December 2
History of Consciousness Ph.D. December 16
Human Computer Interaction M.S. February 3
Latin American & Latino Studies Ph.D. December 9
Linguistics Ph.D./M.A. December 12
Literature M.A./Ph.D. December 2
Materials Science & Engineering M.S. March 20
Materials Science & Engineering Ph.D. January 10
Mathematics M.A./Ph.D. January 2
Microbiology & Environmental Toxicology M.S. March 3
Microbiology & Environmental Toxicology Ph.D. December 3
Molecular, Cell & Developmental Biology M.S. April 1
Music M.A./D.M.A/Ph.D. December 16
Natural Language Processing M.S. February 3, April 16 
Ocean Sciences M.S./Ph.D. December 16
Philosophy M.A./Ph.D. January 7
Physics M.S./Ph.D. January 3
Politics Ph.D. December 16
Biomolecular Engineering & Bioinformatics Ph.D. December 3
Chemical Biology, Biochemistry & Biophysics Ph.D. December 3
Molecular, Cell & Developmental Biology Ph.D. December 3
Microbiology and Environmental Toxicology Ph.D. December 3
Psychology (social, cognitive, developmental) Ph.D. December 2
Science Communication: Science Writing M.S. January 27, April 3
Scientific Computing and Applied Mathematics M.S. January 30
Social Documentation M.F.A. January 9
Sociology Ph.D. December 2
Statistical Science M.S./Ph.D. December 16
Theater Arts M.A. February 3
Visual Studies Ph.D. December 12"
"When do I need to apply for my Molecular, Cell & Developmental Biology by?","
You can apply for the program in Biomedical Sciences and Engineering, M.S. or Ph.D. to pursue a career in molecular, cell, and developmental biology. You can apply for the program in Molecular, Cell & Developmental Biology, Ph.D. (MCD) if you are interested in pursuing a career in research and academia. If you are interested in pursuing a career in industry, you can apply for the program in Biomolecular Engineering & Bioinformatics, Ph.D. (BMEB) or Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) to pursue a career in research and industry.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
I want to apply for a Linguistics Ph.D. When is the deadline?,"
The deadline for the Linguistics Ph.D. program at UC Santa Cruz is April 16th.","Apply | Natural Language Processing M.S.

APPLY TO THE NLP M.S. PROGRAM
APPLICATIONS CLOSE APRIL 16TH
The Natural Language Processing Program Offers a Unique Master of Science degree
Taught intensively over 15-18 months in state-of-the-art facilities in Santa Clara, in the heart of Silicon Valley, our master’s degree program is the product of close collaboration between our distinguished academic team and industry leaders. Our faculty team and Industry Advisory Board are made up of leading scientists and practitioners in the information processing and NLP fields, and our students benefit in a variety of ways from this industry collaboration.
Learning from world-class faculty and practitioners who are leaders in the field means you can be confident your syllabus will give you the real-world knowledge and skills needed for a successful career.
We’re pleased that you’ve decided to take an important step toward advancing your professional expertise and are considering applying to UCSC’s NLP MS Program. Review this page for guidance on the application and admission requirements for this program.
Admission requirements
Visit our Admissions page to review essential admission requirements and tips for applying to the program.
Application timeline
Applications for UCSC graduate programs open each year on October 1st, but deadlines vary by program and are subject to change. View the NLP Program’s current application deadline on the Graduate Admissions website.
Once the application opens, visit UCSC’s Graduate Application Page to begin applying.
Required application materials
1. Transcripts
Unofficial transcripts are sufficient for application purposes, but official transcripts may be required if admitted to the program.
2. Letters of Recommendation
Three letters of recommendation are required. Letters should be prepared by professors and professional contacts who may best speak to your abilities and academic potential in your chosen field of study.
3. Résumé or CV (can also provide a link to your own website)
4. A Personal History Statement
Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In the essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose.
5. A Statement of Purpose
Recommended length is a concise 2-4 pages, single-spaced.
Please describe your plans for graduate study or research and for your future occupation or profession.
Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz.
6. Test score requirements may apply
Review TOEFL/IELTS test score requirements for applicants whose native language is not English.
Visit the Graduate Admissions’ FAQs page for guidance on requesting a waiver of this requirement.
The GRE requirement has been waived for admission consideration for NLP MS applications. You are welcome to submit your official scores, but please note the inclusion of your official GRE score will not be a deciding factor for admission.
7. Statement of Financial Need (optional)
This optional Statement of Financial Need will help the NLP Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
8. Application Fee Waiver Request Form (optional)
Criteria:There are two possible ways to get a fee waiver:
If you are a US citizen, AB-540, or a US Permanent Resident, you may be eligible for the Graduate Admissions’ Fee Waiver for financial hardship or for students currently receiving financial aid, or for a Fee exemption for applicants who participate(d) in a qualifying program. Please see their website for more information.
If you do not meet the criteria above, you may request an application fee waiver directly from the NLP program if you meet ONE of the following criteria:
Current undergraduates and alumni of UC Santa Cruz
Current undergraduates and alumni of any University of California, California State University, or private colleges of universities located in California, and alumni of community colleges located in California.
Please note that fee waivers are limited and are distributed based on students’ level of need.

Before you submit your application: Please fill out Application Fee Waiver Request Form for NLP M.S. 2024-2025 with your application details and be sure to include a brief statement regarding your financial need for the application fee exemption. Be sure to include a brief statement regarding your financial need, which you can copy from the Statement of Financial Need in your application if you included one there. To ensure sufficient time to process your fee waiver request, your fee exemption request must be received at least 2 weeks before the February 3rd application deadline  (Fee Waiver Deadline: January 20th 5PM PT).

After you apply for the fee waiver:
To be eligible for a fee waiver, you must submit the fee waiver request form by January 20th at 5 PM PT, and you must submit your complete application to the NLP program by February 3rd at 11:59PM PT. Incomplete submissions for either of these requirements will disqualify you from being considered for a fee waiver.
On the “Fee Waiver/Payment” page of the application, please choose “no” to the question, “Do you believe you will need assistance paying for your application processing fee?” Then select “Yes” to “Has the program you are applying to agreed to pay for the application processing fee?” The page will tell you a credit card payment is required. Do not worry about this, and do not pay it; we will pay for it on your behalf. Please submit your application as soon as it is ready.
After you have submitted your application, you will be directed to the status portal. There will be a pay fee link, do not pay this fee. You will not be refunded, and a reimbursement process is more involved and takes time to process.

Processing Time: We will review your request within 5 business days and send you a confirmation email to inform you whether the fee waiver has been approved. If you haven’t received any updates after this period, please don’t hesitate to reach out to us at nlp@ucsc.edu.
Get support with your application
For general information:
Check out the NLP FAQs page for support with general inquiries about the program and application process.
Visit the UCSC Graduate Admissions website to review application instructions, deadlines, test score requirements, and detailed guidance about applying to a UCSC graduate program.
Additional guidance about applying to Baskin Engineering graduate programs can be found on Baskin’s Office of Graduate Student Affairs website.
Have questions? We’re here to help
If you have questions about the application requirements or the admissions process, you can email the NLP Support Team (nlp@ucsc.edu) and let us know if you’d prefer to discuss your questions in a Zoom meeting with a member of our team. You can also review insights and tips from NLP students on Reddit.
Watch a recorded virtual information session
Learn more about the unique benefits of the program by watching a recording of one of our virtual information sessions:
Support for parents and families of prospective students
As you consider your options for a master’s degree program, you may also want to include your support network in your decision-making process. We’ve put together a guide for parents and family members to provide insight into life at UCSC, as well as the support and resources available to our graduate students. Please always feel welcome to contact our Support Team if you or your family members have any questions.
"
Are there any full tuition scholarships avaliable for the NLP program?,"
Yes, we are offering up to 10 full-tuition scholarships for the Fall 2025 cohort. Additionally, we are providing fee waivers for undergraduate students and alumni of all University of California and California State University institutions.","Natural Language Processing M.S. – Embrace technology and fast-forward your career

APPLICATIONS ARE NOW OPEN!
We are currently accepting applications for the Fall 2025 cohort!
The initial application review date is coming soon – April 16, 2025 at 11:59 PM PST
We are offering up to 10 full-tuition scholarships, as well as a limited number of fee waivers for undergraduate students and alumni of all University of California and California State University institutions. For more information, contact us at nlp@ucsc.edu"
How long is the NLP M.S. program?,"
The NLP M.S. program is a 15-18 month intensive program in state-of-the-art facilities in Santa Clara, in the heart of Silicon Valley. It is a graduate degree that is taught intensively and is designed to provide students with a comprehensive understanding of natural language processing, including the development of algorithms and systems to analyze and process human language.

The program is taught by a team of distinguished faculty members and industry leaders, who are experts in the field of natural language processing. Students benefit from the close collaboration between faculty and industry professionals, which provides them with the opportunity to gain hands-on experience and real-world knowledge.

The program is designed to provide students with a deep understanding of the field of natural language processing, including the development of algorithms and systems to analyze and process human language. It is a graduate degree that is designed to provide students with a comprehensive understanding of the field, and to equip them with the skills and knowledge needed to succeed in their chosen field.

The program is designed to be completed in 15-18 months, and it is taught intensively in state-of-the-art facilities in Santa Clara, in the heart of Silicon Valley. It is a graduate degree that is designed to provide students with a comprehensive understanding of the field of natural language processing, and to equip them with the skills and knowledge needed to succeed in their chosen field.","Apply | Natural Language Processing M.S.

APPLY TO THE NLP M.S. PROGRAM
APPLICATIONS CLOSE APRIL 16TH
The Natural Language Processing Program Offers a Unique Master of Science degree
Taught intensively over 15-18 months in state-of-the-art facilities in Santa Clara, in the heart of Silicon Valley, our master’s degree program is the product of close collaboration between our distinguished academic team and industry leaders. Our faculty team and Industry Advisory Board are made up of leading scientists and practitioners in the information processing and NLP fields, and our students benefit in a variety of ways from this industry collaboration.
Learning from world-class faculty and practitioners who are leaders in the field means you can be confident your syllabus will give you the real-world knowledge and skills needed for a successful career.
We’re pleased that you’ve decided to take an important step toward advancing your professional expertise and are considering applying to UCSC’s NLP MS Program. Review this page for guidance on the application and admission requirements for this program.
Admission requirements
Visit our Admissions page to review essential admission requirements and tips for applying to the program.
Application timeline
Applications for UCSC graduate programs open each year on October 1st, but deadlines vary by program and are subject to change. View the NLP Program’s current application deadline on the Graduate Admissions website.
Once the application opens, visit UCSC’s Graduate Application Page to begin applying.
Required application materials
1. Transcripts
Unofficial transcripts are sufficient for application purposes, but official transcripts may be required if admitted to the program.
2. Letters of Recommendation
Three letters of recommendation are required. Letters should be prepared by professors and professional contacts who may best speak to your abilities and academic potential in your chosen field of study.
3. Résumé or CV (can also provide a link to your own website)
4. A Personal History Statement
Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In the essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose.
5. A Statement of Purpose
Recommended length is a concise 2-4 pages, single-spaced.
Please describe your plans for graduate study or research and for your future occupation or profession.
Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz.
6. Test score requirements may apply
Review TOEFL/IELTS test score requirements for applicants whose native language is not English.
Visit the Graduate Admissions’ FAQs page for guidance on requesting a waiver of this requirement.
The GRE requirement has been waived for admission consideration for NLP MS applications. You are welcome to submit your official scores, but please note the inclusion of your official GRE score will not be a deciding factor for admission.
7. Statement of Financial Need (optional)
This optional Statement of Financial Need will help the NLP Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
8. Application Fee Waiver Request Form (optional)
Criteria:There are two possible ways to get a fee waiver:
If you are a US citizen, AB-540, or a US Permanent Resident, you may be eligible for the Graduate Admissions’ Fee Waiver for financial hardship or for students currently receiving financial aid, or for a Fee exemption for applicants who participate(d) in a qualifying program. Please see their website for more information.
If you do not meet the criteria above, you may request an application fee waiver directly from the NLP program if you meet ONE of the following criteria:
Current undergraduates and alumni of UC Santa Cruz
Current undergraduates and alumni of any University of California, California State University, or private colleges of universities located in California, and alumni of community colleges located in California.
Please note that fee waivers are limited and are distributed based on students’ level of need.

Before you submit your application: Please fill out Application Fee Waiver Request Form for NLP M.S. 2024-2025 with your application details and be sure to include a brief statement regarding your financial need for the application fee exemption. Be sure to include a brief statement regarding your financial need, which you can copy from the Statement of Financial Need in your application if you included one there. To ensure sufficient time to process your fee waiver request, your fee exemption request must be received at least 2 weeks before the February 3rd application deadline  (Fee Waiver Deadline: January 20th 5PM PT).

After you apply for the fee waiver:
To be eligible for a fee waiver, you must submit the fee waiver request form by January 20th at 5 PM PT, and you must submit your complete application to the NLP program by February 3rd at 11:59PM PT. Incomplete submissions for either of these requirements will disqualify you from being considered for a fee waiver.
On the “Fee Waiver/Payment” page of the application, please choose “no” to the question, “Do you believe you will need assistance paying for your application processing fee?” Then select “Yes” to “Has the program you are applying to agreed to pay for the application processing fee?” The page will tell you a credit card payment is required. Do not worry about this, and do not pay it; we will pay for it on your behalf. Please submit your application as soon as it is ready.
After you have submitted your application, you will be directed to the status portal. There will be a pay fee link, do not pay this fee. You will not be refunded, and a reimbursement process is more involved and takes time to process.

Processing Time: We will review your request within 5 business days and send you a confirmation email to inform you whether the fee waiver has been approved. If you haven’t received any updates after this period, please don’t hesitate to reach out to us at nlp@ucsc.edu.
Get support with your application
For general information:
Check out the NLP FAQs page for support with general inquiries about the program and application process.
Visit the UCSC Graduate Admissions website to review application instructions, deadlines, test score requirements, and detailed guidance about applying to a UCSC graduate program.
Additional guidance about applying to Baskin Engineering graduate programs can be found on Baskin’s Office of Graduate Student Affairs website.
Have questions? We’re here to help
If you have questions about the application requirements or the admissions process, you can email the NLP Support Team (nlp@ucsc.edu) and let us know if you’d prefer to discuss your questions in a Zoom meeting with a member of our team. You can also review insights and tips from NLP students on Reddit.
Watch a recorded virtual information session
Learn more about the unique benefits of the program by watching a recording of one of our virtual information sessions:
Support for parents and families of prospective students
As you consider your options for a master’s degree program, you may also want to include your support network in your decision-making process. We’ve put together a guide for parents and family members to provide insight into life at UCSC, as well as the support and resources available to our graduate students. Please always feel welcome to contact our Support Team if you or your family members have any questions.
"
What are my chances of finding a job in an NLP-adjacent industry after finishing my NLP M.S.,"
Degree?

Based on the recent Statista report, the global NLP market is predicted to grow almost 14 times larger by 2025 than it was in 2017. This rapid growth in the NLP industry is likely to lead to a high demand for NLP experts and professionals in various fields, including data science, artificial intelligence, and machine learning. As an NLP M.S. graduate, you will have a strong foundation in NLP algorithms, technologies, and applications, which will make you a highly sought-after candidate in the job market.

With your degree in NLP, you will be able to apply your skills and knowledge to various industries, such as:

* Data science and analytics
* Artificial intelligence and machine learning
* Natural language processing and text analysis
* Computer vision and robotics
* Business intelligence and data mining

You will have the opportunity to work with a wide range of companies, including tech giants, startups, and organizations, and will be able to contribute to the development of innovative solutions and products.

To increase your chances of finding a job in an NLP-adjacent industry, consider the following strategies:

* Gain practical experience through internships, research projects, or part-time jobs in the industry
* Build a strong network of professionals in the industry through networking events, conferences, and online communities
* Develop a portfolio of your work, including projects and contributions to open-source initiatives
* Stay up-to-date with industry trends and developments by attending conferences, workshops, and online courses
* Consider pursuing additional education or certifications in a related field, such as data science or machine learning

Overall, the job market for NLP-adjacent professionals is highly competitive, but with the right combination of skills, experience, and strategy, you can increase your chances of success and find a fulfilling career in this exciting and rapidly growing field.

In terms of specific job titles, some of the most in-demand positions in NLP-adjacent industries include:

* Data scientist
* Artificial intelligence engineer
* Machine learning engineer
* Natural language processing engineer
* Computer vision engineer
* Business intelligence analyst

These roles require a strong foundation in NLP, as well as a range of technical skills and expertise. By combining your degree in NLP with practical experience and a strong portfolio, you can increase your chances of landing a job in one of these high-demand positions.

In summary, the job market for NLP-adjacent professionals is highly competitive, but with the right combination of skills, experience, and strategy, you can increase your chances of success and find a fulfilling career in this exciting and rapidly growing field.","Your Career | Natural Language Processing M.S.

YOUR CAREER
Your Career in NLP
According to a recent Statista report, the global Natural Language Processing market size is predicted to grow almost 14 times larger by 2025 than it was in 2017. The NLP M.S. program at UCSC provides students with the skills and in-depth knowledge of NLP algorithms, technologies, and applications that are in high demand in this rapidly growing field. Our program goes beyond the classroom by supplying students with industry-relevant projects for the kind of real-world experience that is essential for a successful career in NLP.
Industry Experience
Our students benefit from working on an industry-relevant Capstone project through the duration of the program. Collaborating in teams of 3-5 with guidance from a mentor, students are able to gain the invaluable real-world experience essential for a successful career in NLP. Find out more about our NLP Capstone projects on our Capstone Experience page.
Career Support
Building a successful career needs more than just a great resume and performing well at interview. Your success depends on your approach to the process, how you see yourself and respond to challenges. Professional staff at our Career Center help you figure out where you want to go and how to get there. There is a wealth of resources available to help you explore your professional and personal goals. You can sign up for career coaching, additional skills training or have a one-to-one with one of our careers experts.
Social Media
Upcoming Events
Graduates from our program have gone on to roles such as:
Data engineer
Software engineer
Machine learning engineer
Applied scientist
NLP engineer
Computational linguist
In leading organizations including:
Amazon
American Express
Astrata
Quantcast
Megagon Labs
Raybeam
Sephora
Shipt
Techcyte
The Washington Post
Zoom
Check out where our alumni are now!
A Look at Jobs in NLP
Silicon Valley employs a large number of people with experience and expertise in Natural Language Processing. Job search sites like LinkedIn list several thousand openings for natural language processing related jobs.
Check out these employment websites for information about career opportunities in NLP:
Hired
Glassdoor
Indeed
LinkedIn
NLP People
Jobs are regularly advertised by companies such as Amazon, Google, IBM, Verizon, YouTube, Bosch, Yahoo, Intel, Nuance, Facebook, and Apple among others. The typical position requires a combination of skills in NLP along with computer science and data science. This demand for M.S. students with expertise in NLP is highly likely to grow in the next decade.
Optional Practical Training (OPT) for International Students
International students graduating from the NLP M.S. program who are in valid F-1 status are eligible for OPT. Graduates of the NLP M.S. program are also eligible for the OPT STEM Extension. The CIP code for NLP is 11.0102, which is the same as Artificial Intelligence, as listed on the DHS STEM Designated Degree Program List. OPT-related questions should be directed to the International Student and Scholar Services (ISSS) Office at istudent@ucsc.edu.
Resources for Current Students
In addition to individualized academic support provided by the NLP Program Team and faculty, UCSC offers a variety of career support services for NLP graduates.
Through the UCSC Career Center, students can:
Meet with a Career Coach to explore career goals and develop job search strategies
Participate in workshops, fairs, on-campus interviews, conferences and more
Access resources to help refine application materials, network effectively and prepare for job interviews
The Career Center also provides resources for UCSC alumni as well. To learn more, visit the Career Center’s website.
HandshakeOur online portal, Handshake helps you search for employment opportunities, register for upcoming networking events, workshops and fairs with employers, make a career coaching appointment or browse job opportunities from vetted employers based on your interests.
Career coachingOur team of peer coaches and professional staff can help you develop job search strategies, answer your questions, discuss options, and identify job opportunities. They’ll advise on how best to make contact and build relationships with potential employers.
ExperienceYour capstone project will give you an insight into current NLP challenges. You’ll have the opportunity find out more about the field, current growth areas and advances. You’ll also have the opportunity to meet a range of experts working in a range of contexts with different organizations.Work experience on your resume really helps you stand out to employers. Your capstone project will grow your knowledge, build your expertise and develop your aspirations.
EssentialsOur careers consultants provide essential skills support that focuses on the crucial skills you need to get hired. This includes everything from writing a winning CV, crafting an excellent cover letter or playing to your strengths at interview.
Professional Development Opportunities Fall Quarter 2023As an NLP MS student, you’ll have access to a variety of professional development workshops, resources, and support services. Each quarter, we’ll highlight workshops and networking events that current students can attend online, at the UCSC Silicon Valley Campus, or at the UC Santa Cruz Main Campus. Check out the Career Center’s calendar on Handshake for more professional development opportunities.
How to Get a Job
September 25th, 11 – 11:45 AM
Online
Learn more
Proactive Diversity, Equity, and Inclusion
October 5th, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
UCSC Silicon Valley Career & Internship Fair
October 10th, 4 – 7:30 PM
UCSC Silicon Valley Campus
Learn more
2023 Fall Engineering & Technology Career & Internship Fair
October 11th, 11 AM – 3 PM
UC Santa Cruz Main Campus
Learn more
Conducting an Informational Interview
October 11th, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
Curating Your Digital Reputation
October 12th, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
Mastodon, Threads, and X: Promote Research on Text-Based Social Media Platforms
October 19th, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
LinkedIn Profile and Job Search
October 24th, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
Disrupting Imposter Phenomenon from the Inside Out
October 25th, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
Conflict Resolution
October 31st, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
Writing the CV
November 1st, 11:30 AM
Online and UC Santa Cruz Main Campus
Learn more
Speaking Up to Bias
November 2nd, 11:30 AM
Online and UC Santa Cruz Main Campus"
Why should I choose NLP at UC Santa Cruz,"
Here are some key points to consider:

UC Santa Cruz is a world-renowned institution with a strong reputation for academic excellence. The NLP program at UC Santa Cruz is a highly interdisciplinary field that combines computer science, linguistics, and cognitive science to advance our understanding of human language and its applications. I am particularly drawn to the program because of its interdisciplinary approach, which allows me to combine my interests in linguistics and cognitive science with my passion for computer science. I am impressed by the faculty's expertise in areas such as natural language processing, machine learning, and human-computer interaction. I am also excited about the opportunity to work with state-of-the-art technology and collaborate with researchers from diverse backgrounds. I believe that the NLP program at UC Santa Cruz will provide me with the ideal environment to explore my interests, develop my skills, and contribute to the advancement of the field. I am confident that the program will help me achieve my academic and professional goals and make a meaningful contribution to the field of NLP.","NLP | Gradadmissions
​
Important: Transcripts and Letters of Recommendation are required of all applicants.

Read the Application Instructions​ page for full details.
​
See the test score requirements for this program.
​
Below are specific requirements also needed for this program:
Résumé or CV (can also provide a link to your own website)
A Statement of Purpose is required:
Please describe your plans for graduate study or research and for your future occupation or profession. Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz. Recommended length is a concise 2-4 pages, single-spaced.
A Personal History Statement is also required:​
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose. Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In an essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Financial Need Instructions:​
This optional Statement of Financial Need will help the Natural Language Processing Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.​
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
​
"
Who should I contact about full-tuition scholarships for NLP?,"
You can contact the NLP admissions office at nlp@ucsc.edu for more information about the available scholarships.","Natural Language Processing M.S. – Embrace technology and fast-forward your career

APPLICATIONS ARE NOW OPEN!
We are currently accepting applications for the Fall 2025 cohort!
The initial application review date is coming soon – April 16, 2025 at 11:59 PM PST
We are offering up to 10 full-tuition scholarships, as well as a limited number of fee waivers for undergraduate students and alumni of all University of California and California State University institutions. For more information, contact us at nlp@ucsc.edu"
How large are the class sizes in the NLP program?,"
The class sizes in the NLP program at UCSC are generally smaller compared to larger computer science programs. The program has a total of 12-15 students in the Master's program, with a student-to-faculty ratio of 6:1, which is significantly lower than the typical ratio found in larger computer science programs. This smaller class size allows for more personalized attention and interaction between students and faculty, which can be beneficial for students who are looking to pursue advanced research opportunities or who are looking for a more intimate learning environment.","Admissions | Natural Language Processing M.S.

ADMISSIONS
Get ready to apply for Fall 2025 admission consideration
APPLY STARTING OCT 1ST
Applying to the NLP MS Program at UCSC
We’re delighted that you’re interested in applying to UCSC’s unique professional master’s program in natural language processing. The NLP MS program is intended for students who have a considerable background in computer science, and we are looking for multifaceted individuals who ideally have experience with programming, computer science, probability, statistics, and linguistics. We review applications holistically and we have students in the NLP MS program with different undergraduate degrees who are successful in the program.
Below please see a list of recommended courses and requirements that will help you be successful in our program. In your application, you can explain any gaps between your background and the admission guidelines below.
Applicants wishing to study a broader range of computer science topics in a graduate program may wish to consider the MS in Computer Science and Engineering degree program. The program cost varies with the selected curriculum plan. Please check the Registrar’s website for the most recent tuition and fee information.
Admission requirements:
A B.A. or B.S in Computer Science or equivalent
An overall GPA of 3.0 or better on the 4.0 GPA scale or the equivalent
Excellent programming skills either through academic coursework and/or significant professional experience
TOEFL or IELTS scores (if applicable)
Minimum TOEFL score of 89
Minimum IELTS score of 7.0
Essential courses to highlight in your application:
A class in Discrete Math (the equivalent of UCSC’s CSE 16: Applied Discrete Mathematics)
An advanced class in Data Structures and Algorithms (the equivalent of UCSC’s CSE 101: Introduction to Data Structures and Algorithms)
At least one class in Probability and Statistics with a grade of B or better (the equivalent of UCSC’s CSE 107: Probability and Statistics for Engineers or STAT 131: Introduction to Probability Theory)
Desirable (but not required) additional requirements include:
A class in NLP or Linguistics (the equivalent of UCSC’s CSE 143: Introduction to Natural Language Processing)
A class in Machine Learning (the equivalent of UCSC’s CSE 142: Machine Learning)
A class in Formal Languages and Automata Theory (the equivalent of UCSC’s CSE 103: Computational Models)
Want to learn more? Connect with our Support TeamThere are lots of ways to get support with your application. The NLP Support Team is ready to assist with any questions you may have about the program, our industry-informed curriculum, and applying to UCSC.
Visit our Contact page to connect with our NLP Support Team today.
Application Materials
Required application materials
Application fee:
Domestic: $135
International: $155
If you are currently enrolled in a UCSC undergraduate or graduate program or have an undergraduate or graduate degree from a UCSC program, you may request an application fee waiver directly from the NLP Program.
Review the Graduate Admissions website for information about fee waivers and exemptions.
GPA: minimum 3.0 on 4.0 scale or equivalent.
Transcripts: unofficial transcripts are accepted for application purposes but official transcripts may be required if admitted to the program.
Letters of Recommendation: 3 letters required from professors and professional contacts who may best speak to your abilities and academic potential in your chosen field of study.
TOEFL or IELTS  (if applicable)
TOEFL: minimum score of 89
IELTS: minimum overall score of 7.0
Personal History Statement:
Recommended length is a concise 1-3 pages, single-spaced.
UCSC is interested in a diverse and inclusive graduate student population. In the essay, discuss how your personal background informs your decision to pursue a graduate degree.
Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Purpose:
Recommended length is a concise 2-4 pages, single-spaced.
Please describe your plans for graduate study or research and for your future occupation or profession.
Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at UCSC.
CV required, preferably from a link to a personal web page containing CV
NOTE: GRE (no minimum) – The GRE requirement has been waived for Fall 2024 admission consideration for NLP MS applications. You are welcome to submit your official scores, but please note the inclusion of your official GRE score will not be a deciding factor for admission.
Application Instructions & FAQ
Admissions FAQ
Application FAQ
Application Instructions
AB 540 Info
International Applicants
We’re here to help! Visit our Contact page to connect with a member of our support team. You can also email the NLP Program Team directly at nlp@ucsc.edu.
Funding opportunities: MS students are typically self-supported. While a small number of partial fellowship packages are extended to eligible NLP students each year, the NLP Program encourages students to seek external support for covering graduate education expenses. Recipients of these fellowships are selected based on financial need and merit. We strongly recommend eligible applicants complete the optional Statement of Financial Need in the application to be considered for financial support. Instructions for completing this statement are available on the Graduate Admissions website.
Visit the NLP Financials page to review additional information about the estimated costs of attendance and financial support options.
Join an info session to learn more! The NLP Program Team hosts virtual information sessions each year about the unique benefits of studying NLP at UCSC. At each session, you’ll have an opportunity to meet members of our community including faculty and staff, and to ask any questions you may have about the program and application. Below is a schedule of upcoming information sessions you can attend from anywhere in the world:
Friday, November 3rd from 11 AM – 12 PM (PDT) – Register now
Wednesday, December 6th from 7 PM – 8 PM (PST) – Register now
Tuesday, January 9th from 8 AM – 9 AM (PST) – Register now
For parents and families of prospective students: We know that going to graduate school can involve a significant investment of your time and personal resources. As you consider your options for graduate studies, you may also want to include your support network in your decision-making process. We’ve put together a guide for parents and family members to provide insight into life at UCSC, as well as the support and resources available to our graduate students. Please always feel welcome to contact our Support Team if you or your family members have any questions.
Join an NLP Information Session!At our information sessions, we discuss the range of courses and support services we offer, as well as tips for applying to the program. We also prioritize time for a Q&A session during each event, where you’ll have the opportunity to meet NLP faculty and staff.
To attend an information session, please complete the session’s registration form. You’ll then be sent an individualized Zoom link you can use to access the information session. Please note that dates and times for upcoming sessions are subject to change.
November 6, 202411 AM – 12 PM PST
Register online
December 4th, 20247 PM – 8 PM PST
Register online
January 15, 20258 AM – 9 AM PST
Register online"
Where is the NLP campus?,"
The NLP campus is located in Santa Clara, California, near the heart of Silicon Valley, offering unparalleled access to industry companies and events.

Note: I have added a brief introduction to the NLP campus and provided a concise summary of the facilities and location.","Facilities | Natural Language Processing M.S.

FACILITIES
Well-Networked
Located in the heart of Silicon Valley, the UCSC Natural Language Processing Master’s program offers unprecedented access to NLP industry companies and events, and is well-placed to capitalize on these relationships.
The satellite campus in Santa Clara enables greater partnerships and collaborations among faculty, students, industry and government. Our students are able to intern at companies throughout the region and our alumni are an integral part of the highly trained specialist Silicon Valley workforce. Visit our Living in Silicon Valley page to learn more about housing, transportation, recreation, and networking opportunities in the area.
UCSC’s Games & Playable Media and Human-Computer Interaction Professional Masters programs are also based at this campus.
Superb Facilities
You’ll be able to take advantage of the superb facilities at our Santa Clara campus. Equipped with the latest in ICT from steel case mediascape tables to conference and video suites. Our purpose-designed business engagement space hosts a number of events giving you the opportunity to interact with the Silicon Valley community through guest lectures and networking events. Check out a virtual tour of the Silicon Valley Campus created by one of our NLP Student Ambassadors."
What subjects are taught in the NLP program?,"
The NLP program at the University of California, Santa Cruz teaches a wide range of subjects, including Natural Language Processing, Machine Learning, Data Structures, and Computer Vision, as well as more specialized courses such as Deep Learning, Reinforcement Learning, and Natural Language Understanding. Additionally, students have the opportunity to work with industry professionals on research projects and collaborate with peers in the field.","NLP | Gradadmissions
​
Important: Transcripts and Letters of Recommendation are required of all applicants.

Read the Application Instructions​ page for full details.
​
See the test score requirements for this program.
​
Below are specific requirements also needed for this program:
Résumé or CV (can also provide a link to your own website)
A Statement of Purpose is required:
Please describe your plans for graduate study or research and for your future occupation or profession. Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz. Recommended length is a concise 2-4 pages, single-spaced.
A Personal History Statement is also required:​
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose. Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In an essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Financial Need Instructions:​
This optional Statement of Financial Need will help the Natural Language Processing Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.​
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
​
"
What health insurance is offerred?,"
UC SHIP (Student Health Insurance Plan) is offered by Academic HealthPlans (AHP). AHP is a health insurance company that provides a range of plans to students, including individual and family plans. AHP offers plans that meet the university's health insurance requirements, which include coverage for inpatient (hospital) and outpatient care for mental health and substance use conditions, as well as coverage for all minimum essential health benefits (EHB). AHP also offers plans that cover services related to suicidal conditions, injury from participation in recreational activities or amateur sports, and preexisting conditions. Additionally, AHP offers plans that have no per-medical or per-mental health/substance use condition maximum benefit limits. Students can choose from various plans, including Medi-Cal/Medicaid, Medicare, TRICARE/military, Covered California or other U.S. federal or state exchange plans, and UC employee health plans.","Waiving Coverage | UC Santa Cruz

What you need to know
To waive UC SHIP coverage, you must have other health insurance coverage that meets the university’s health insurance requirements, described in the waiver criteria below. You must submit a waiver application prior to the start of each new academic year to waive coverage that year.
Waiver criteria for 2024-25 academic year
All plans must provide unrestricted access to an in-network primary care provider and in-network hospital and to full non-emergency medical and mental health care within a reasonable distance of campus or the student’s place of residence (if online learning only) while attending school. Such distance shall be determined at the discretion of each campus based on its unique geographic considerations and local availability of services. (The waiver form will indicate the distance requirement appropriate for each campus.)
NOTE: This criterion applies to all plan types without exception, including Medi-Cal or Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, HMOs (including Kaiser, WHA and others), Covered California or other U.S. federal or state exchange plans, and all employee-sponsored and individual plans.
Coverage must be currently active, and the student must agree to maintain health coverage throughout the entire academic year. If your current insurance coverage is terminated, contact the student health insurance office as quickly as possible to discuss your health insurance options.
To satisfy UC’s health insurance requirement for enrolled students, the insurance plan held by the student must provide the following:
Be a Medi-Cal/Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, Covered California or other U.S. federal or state exchange plan, or a UC employee health plan, OR
Be an employer-sponsored group health plan or individual plan that:
Has an annual out-of-pocket maximum of $9,450 or less for an individual or $18,900 or less for a family. Deductibles, copayments and coinsurance paid by the member accrue toward meeting the out-of-pocket maximum. A higher out-of-pocket maximum is allowed if the member has a Health Savings Account (HSA) or a Health Reimbursement Account (HRA).
Covers inpatient (hospital) and outpatient care for mental health and substance use conditions the same as any other medical condition.
Covers doctor office visits for medical, including mental health, and alcohol/drug abuse conditions.
Provides coverage for all minimum essential health benefits (EHB). For the criteria, please go to the Centers for Medicare & Medicaid Services (CMS) website.
Is not a health care or pharmacy reimbursement plan (reimbursement plan means the student must pay for services and seek reimbursement from the insurance provider).
Does not have a per-medical or per-mental health/substance use condition maximum dollar limit.
International students
For international students residing in the United States, the following additional criteria apply. The plan must:
Have no per-medical or per-mental health/substance use condition maximum benefit limits.
Cover services related to suicidal conditions, including attempted suicide or suicidal thoughts.
Cover medical services for injury from participation in all types of recreational activities or amateur sports.
Have no preexisting condition exclusion or limitation; if the plan has a preexisting condition waiting period, that period must have expired.
Have no lifetime maximums on benefits.
Have a complete master policy written in standard English with benefits expressed in U.S. dollars.
Have a claims payment office with an address and phone number in the United States.
Pay at least $50,000 annually for medical evacuation.
Pay at least $25,000 for repatriation of remains.
The waiver process
Academic HealthPlans manages the waiver process through its website. It’s where you’ll initiate your request, submit all required documentation, and make any updates to your existing application.
Want to waive UC SHIP coverage?Academic HealthPlans’ (AHP’s) website makes it fast and easy to submit documents and complete the UC SHIP waiver process online.
Waive your UC SHIP coverage
Applying for a waiver
Here are seven things to know when you apply for a waiver.
You must submit the waiver application and supporting documents during the waiver period.
You must submit the waiver application for each academic year you want to waive UC SHIP coverage.
You can complete the application online during the fall, winter or spring waiver periods. Your waiver will be applied to the full academic year — or to the remainder of the academic year if you apply during the winter or spring terms.
When you submit a waiver application, we contact your health insurance company to verify that you are actively enrolled. You will receive a quick response to your application.
If your waiver is approved, you must maintain active enrollment in your health insurance plan for the entire academic year. Before the start of every term, we reverify that you are enrolled in your health insurance plan.
If we cannot verify that you are actively enrolled in a health insurance plan, you will be notified by email and/or text message that your waiver was denied. You will then be enrolled in UC SHIP and charged the cost of coverage for that term.
If your waiver is denied, you can appeal the decision by submitting proof of current insurance coverage from your health insurance plan.
Where to learn more
You can find waiver instructions, waiver periods, FAQs and forms on the UC SHIP page on the Student Health Center website.
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
Is health insurance mandatory for all UCSC students?,"
No, health insurance is not mandatory for all UCSC students. While the university requires that all students maintain health insurance coverage, there are exceptions for certain groups of students, including international students, graduate students, and students with certain medical conditions.","Waiving Coverage | UC Santa Cruz

What you need to know
To waive UC SHIP coverage, you must have other health insurance coverage that meets the university’s health insurance requirements, described in the waiver criteria below. You must submit a waiver application prior to the start of each new academic year to waive coverage that year.
Waiver criteria for 2024-25 academic year
All plans must provide unrestricted access to an in-network primary care provider and in-network hospital and to full non-emergency medical and mental health care within a reasonable distance of campus or the student’s place of residence (if online learning only) while attending school. Such distance shall be determined at the discretion of each campus based on its unique geographic considerations and local availability of services. (The waiver form will indicate the distance requirement appropriate for each campus.)
NOTE: This criterion applies to all plan types without exception, including Medi-Cal or Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, HMOs (including Kaiser, WHA and others), Covered California or other U.S. federal or state exchange plans, and all employee-sponsored and individual plans.
Coverage must be currently active, and the student must agree to maintain health coverage throughout the entire academic year. If your current insurance coverage is terminated, contact the student health insurance office as quickly as possible to discuss your health insurance options.
To satisfy UC’s health insurance requirement for enrolled students, the insurance plan held by the student must provide the following:
Be a Medi-Cal/Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, Covered California or other U.S. federal or state exchange plan, or a UC employee health plan, OR
Be an employer-sponsored group health plan or individual plan that:
Has an annual out-of-pocket maximum of $9,450 or less for an individual or $18,900 or less for a family. Deductibles, copayments and coinsurance paid by the member accrue toward meeting the out-of-pocket maximum. A higher out-of-pocket maximum is allowed if the member has a Health Savings Account (HSA) or a Health Reimbursement Account (HRA).
Covers inpatient (hospital) and outpatient care for mental health and substance use conditions the same as any other medical condition.
Covers doctor office visits for medical, including mental health, and alcohol/drug abuse conditions.
Provides coverage for all minimum essential health benefits (EHB). For the criteria, please go to the Centers for Medicare & Medicaid Services (CMS) website.
Is not a health care or pharmacy reimbursement plan (reimbursement plan means the student must pay for services and seek reimbursement from the insurance provider).
Does not have a per-medical or per-mental health/substance use condition maximum dollar limit.
International students
For international students residing in the United States, the following additional criteria apply. The plan must:
Have no per-medical or per-mental health/substance use condition maximum benefit limits.
Cover services related to suicidal conditions, including attempted suicide or suicidal thoughts.
Cover medical services for injury from participation in all types of recreational activities or amateur sports.
Have no preexisting condition exclusion or limitation; if the plan has a preexisting condition waiting period, that period must have expired.
Have no lifetime maximums on benefits.
Have a complete master policy written in standard English with benefits expressed in U.S. dollars.
Have a claims payment office with an address and phone number in the United States.
Pay at least $50,000 annually for medical evacuation.
Pay at least $25,000 for repatriation of remains.
The waiver process
Academic HealthPlans manages the waiver process through its website. It’s where you’ll initiate your request, submit all required documentation, and make any updates to your existing application.
Want to waive UC SHIP coverage?Academic HealthPlans’ (AHP’s) website makes it fast and easy to submit documents and complete the UC SHIP waiver process online.
Waive your UC SHIP coverage
Applying for a waiver
Here are seven things to know when you apply for a waiver.
You must submit the waiver application and supporting documents during the waiver period.
You must submit the waiver application for each academic year you want to waive UC SHIP coverage.
You can complete the application online during the fall, winter or spring waiver periods. Your waiver will be applied to the full academic year — or to the remainder of the academic year if you apply during the winter or spring terms.
When you submit a waiver application, we contact your health insurance company to verify that you are actively enrolled. You will receive a quick response to your application.
If your waiver is approved, you must maintain active enrollment in your health insurance plan for the entire academic year. Before the start of every term, we reverify that you are enrolled in your health insurance plan.
If we cannot verify that you are actively enrolled in a health insurance plan, you will be notified by email and/or text message that your waiver was denied. You will then be enrolled in UC SHIP and charged the cost of coverage for that term.
If your waiver is denied, you can appeal the decision by submitting proof of current insurance coverage from your health insurance plan.
Where to learn more
You can find waiver instructions, waiver periods, FAQs and forms on the UC SHIP page on the Student Health Center website.
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
How do I enroll in or waive the UC Student Health Insurance Plan (UC SHIP)?,"
To enroll in UC SHIP, submit a waiver application through the Academic HealthPlans website. You will need to provide documentation and information about your health insurance coverage, and the university will verify that you are actively enrolled in a health insurance plan. If your waiver is approved, you will be enrolled in UC SHIP and charged the cost of coverage for that term. If your waiver is denied, you can appeal the decision by submitting proof of current insurance coverage from your health insurance plan.","Waiving Coverage | UC Santa Cruz

What you need to know
To waive UC SHIP coverage, you must have other health insurance coverage that meets the university’s health insurance requirements, described in the waiver criteria below. You must submit a waiver application prior to the start of each new academic year to waive coverage that year.
Waiver criteria for 2024-25 academic year
All plans must provide unrestricted access to an in-network primary care provider and in-network hospital and to full non-emergency medical and mental health care within a reasonable distance of campus or the student’s place of residence (if online learning only) while attending school. Such distance shall be determined at the discretion of each campus based on its unique geographic considerations and local availability of services. (The waiver form will indicate the distance requirement appropriate for each campus.)
NOTE: This criterion applies to all plan types without exception, including Medi-Cal or Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, HMOs (including Kaiser, WHA and others), Covered California or other U.S. federal or state exchange plans, and all employee-sponsored and individual plans.
Coverage must be currently active, and the student must agree to maintain health coverage throughout the entire academic year. If your current insurance coverage is terminated, contact the student health insurance office as quickly as possible to discuss your health insurance options.
To satisfy UC’s health insurance requirement for enrolled students, the insurance plan held by the student must provide the following:
Be a Medi-Cal/Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, Covered California or other U.S. federal or state exchange plan, or a UC employee health plan, OR
Be an employer-sponsored group health plan or individual plan that:
Has an annual out-of-pocket maximum of $9,450 or less for an individual or $18,900 or less for a family. Deductibles, copayments and coinsurance paid by the member accrue toward meeting the out-of-pocket maximum. A higher out-of-pocket maximum is allowed if the member has a Health Savings Account (HSA) or a Health Reimbursement Account (HRA).
Covers inpatient (hospital) and outpatient care for mental health and substance use conditions the same as any other medical condition.
Covers doctor office visits for medical, including mental health, and alcohol/drug abuse conditions.
Provides coverage for all minimum essential health benefits (EHB). For the criteria, please go to the Centers for Medicare & Medicaid Services (CMS) website.
Is not a health care or pharmacy reimbursement plan (reimbursement plan means the student must pay for services and seek reimbursement from the insurance provider).
Does not have a per-medical or per-mental health/substance use condition maximum dollar limit.
International students
For international students residing in the United States, the following additional criteria apply. The plan must:
Have no per-medical or per-mental health/substance use condition maximum benefit limits.
Cover services related to suicidal conditions, including attempted suicide or suicidal thoughts.
Cover medical services for injury from participation in all types of recreational activities or amateur sports.
Have no preexisting condition exclusion or limitation; if the plan has a preexisting condition waiting period, that period must have expired.
Have no lifetime maximums on benefits.
Have a complete master policy written in standard English with benefits expressed in U.S. dollars.
Have a claims payment office with an address and phone number in the United States.
Pay at least $50,000 annually for medical evacuation.
Pay at least $25,000 for repatriation of remains.
The waiver process
Academic HealthPlans manages the waiver process through its website. It’s where you’ll initiate your request, submit all required documentation, and make any updates to your existing application.
Want to waive UC SHIP coverage?Academic HealthPlans’ (AHP’s) website makes it fast and easy to submit documents and complete the UC SHIP waiver process online.
Waive your UC SHIP coverage
Applying for a waiver
Here are seven things to know when you apply for a waiver.
You must submit the waiver application and supporting documents during the waiver period.
You must submit the waiver application for each academic year you want to waive UC SHIP coverage.
You can complete the application online during the fall, winter or spring waiver periods. Your waiver will be applied to the full academic year — or to the remainder of the academic year if you apply during the winter or spring terms.
When you submit a waiver application, we contact your health insurance company to verify that you are actively enrolled. You will receive a quick response to your application.
If your waiver is approved, you must maintain active enrollment in your health insurance plan for the entire academic year. Before the start of every term, we reverify that you are enrolled in your health insurance plan.
If we cannot verify that you are actively enrolled in a health insurance plan, you will be notified by email and/or text message that your waiver was denied. You will then be enrolled in UC SHIP and charged the cost of coverage for that term.
If your waiver is denied, you can appeal the decision by submitting proof of current insurance coverage from your health insurance plan.
Where to learn more
You can find waiver instructions, waiver periods, FAQs and forms on the UC SHIP page on the Student Health Center website.
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
Is there an option for if I don't want to purchase UC SHIP insurance but still want to have access to the student health center?,"
Yes, you can still access the student health center even if you don’t have UC SHIP insurance, but you won’t be able to get the full range of benefits and services offered by UC SHIP. 

You can get care off campus at the UC health center near you, but you won’t have the same level of coverage as a UC SHIP member. If you receive a referral to get care outside the Student Health Center, your cost for that care will depend on whether or not you receive care from a network provider. Staff at the SHC insurance office can help you find network providers at the UC health center near you or within the Anthem Blue Cross Prudent Buyer PPO network. If you choose to get care from an out-of-network provider, your costs will be considerably higher. 

If you need to get care outside the SHC, you can still get care at UC SHIP’s network providers, but you won’t have the same level of coverage as a UC SHIP member. If you choose to get care from an out-of-network provider, your costs will be considerably higher. 

You can also use the Sydney Health app or Anthem’s online provider listing to search for network providers in your area. If you take your referral to an out-of-network provider, your costs will be considerably higher.","When You’re Away from Campus | UC Santa Cruz

What you need to know
With UC SHIP, you’re covered wherever you go — whether it’s around the block or around the world.
Getting care off campus
When you receive a referral to get care outside the Student Health Center (SHC), your cost for that care will depend on whether or not you receive care from a network provider. Staff at the SHC insurance office can help you find network providers at the UC health center near you or within the Anthem Blue Cross Prudent Buyer PPO network. If you choose to get care from an out-of-network provider, your costs will be considerably higher.
Know before you goWithout a referral from the Student Health Center, some care won’t be covered by UC SHIP. When necessary, be sure to get a referral so you’re not stuck with an unexpected bill to pay.
Here’s what you should know about your three options for getting care outside the SHC.
UC Family network
As a member of UC SHIP, you have access to the UC Family of nationally ranked medical centers — along with their affiliated facilities and professional providers. The UC Family of medical centers, affiliated facilities and professional providers are in-network providers.
Anthem network
You can ask the SHC for assistance finding a provider in the Anthem Blue Cross Prudent Buyer PPO network. You can also use the Sydney Health app or go to Anthem’s online provider listing to search for network providers, clinics and hospitals in your area.
Out-of-network
If you take your referral to an out-of-network provider, your costs will be considerably higher.
You may be asked to pay up front. To be reimbursed, you will need to submit your claim to Anthem yourself because you aren’t using an Anthem provider.
In addition to paying a higher percentage of your bill out of pocket when you see an out-of-network provider, you will also need to pay the difference between the provider’s billed charge and Anthem’s maximum allowed amount. This difference does not count toward the annual limit on your out-of-pocket costs.
The annual limit on your out-of-network out-of-pocket costs is two or three times higher than the limit on network out-of-pocket costs. As a reminder, once your costs hit the out-of-pocket limit, UC SHIP steps in to pay 100% of the costs for the remainder of the academic year.
Visiting another UC campus
Registered students with UC SHIP coverage can access services offered at student health centers on all UC campuses. For fees and information about filing claims with UC SHIP, check with the SHC on the campus you plan to visit.
Traveling within the United States
You’ll always pay less for care when you see an Anthem Blue Cross Prudent Buyer PPO network provider. To find an Anthem provider in any state, visit the Anthem website.
Traveling outside the United States
UC SHIP provides worldwide coverage when you’re traveling or living outside the United States. When you and your dependents covered under UC SHIP need care, you can use the Blue Cross Blue Shield Global Core program, which coordinates benefits abroad.
Five things to do before leaving the United States
To be sure you have the information you need at your fingertips, take the following steps:
Call BlueCard Access at (800) 810-BLUE (2583) to learn how you’re covered abroad.
Call your SHC about coverage for any vaccines you may need for your destination and to determine whether you’re eligible for UC travel accident coverage.
Download the Sydney Health app from the App Store or Google Play.
Download the BCBS Global Core app from the App Store or Google Play.
Record important SHC phone numbers and pack your UC ID cards. Consider printing a copy of your Anthem ID card from the Sydney Health app; it contains identification numbers that confirm your Anthem coverage.
Using Blue Cross Blue Shield Global Core
As needed, contact the SHC for a referral before receiving non-emergency care.
To find a Blue Cross Blue Shield Global Core provider, visit the Blue Cross Blue Shield Global Core website. In the box next to the GO button, enter the first three letters or numbers of the Member ID on your card.
Work with Global Core to find an in-network provider. If you have to see an out-of-network doctor, you’ll pay out of pocket for your care. You can file a claim for reimbursement from UC SHIP. As a reminder, UC SHIP covers most out-of-network care at 60% of maximum allowable charges; you’re responsible for 40% plus any amount above the allowed charges. Emergency care is covered at 100% of maximum allowable charges.
Traveling on university business
If you’re traveling on UC business (doing research abroad, for example), you also may be eligible for travel accident coverage at no cost to you. To be covered, you must register before you leave on your UC-sponsored trip.
Both UC SHIP and travel accident insurance cover medical evacuation and transportation related to your medical care."
How much is UC SHIP insurance?,"
UC SHIP insurance is $1,500 for the 2024-25 academic year. This is the full premium that students must pay upon enrollment, whether you're a registered or non-registered student.","Paying for Coverage | UC Santa Cruz

UC SHIP premiums are increasing for the 2024-25 academic year.
Learn more about the increases
If you’re a registered student, you are automatically enrolled in UC SHIP and charged a health insurance premium on your registration bill.
If you’re a non-registered student enrolling yourself and/or your eligible dependents for coverage, you must pay the full insurance premium upon enrollment.
Learn what you’ll pay for coverage.
Help paying for medical expenses
Providing affordable health care is a UC SHIP mandate. Even so, every student’s financial position is unique. If your out-of-pocket medical expenses are causing you financial hardship, these UCSC resources may be able to provide financial assistance:
Slug Support Program
Health Equity Fund
Charity care may also be an option for you. Reach out to the provider’s office and ask if they offer any form of charity care. Although some providers do not offer any sort of charity care, others do. They may provide an application for you to complete or ask you to write a letter. Providers who do offer charity care will review your request, and then they could offer to write off the full amount you owe, a portion of that amount, or they may determine you don’t meet their qualifications for charity care.


"
When is the deadline to waive UC SHIP?,"
You can waive UC SHIP coverage at any time, but you must submit your request to the insurance office on your campus within 31 days of losing your UC SHIP coverage. If you fail to do so, your UC SHIP coverage will be automatically renewed, and you will be enrolled in the insurance plan that meets the university health coverage requirements.","Enrolling in Coverage | UC Santa Cruz

What you need to know
You are automatically enrolled in UC SHIP coverage, unless you are approved to waive coverage. If you’re a non-registered student, you can enroll in UC SHIP only during voluntary enrollment periods that typically begin 30 days before — and end 30 days after — the start of the coverage period. Enrollment outside these periods will not be accepted, unless they’re the result of a qualifying life event.
When you can enroll
Enrollment criteria for a non-registered student
Enrolling as a non-registered student
Enrolling after a qualifying life event
Enrolling eligible dependents
Canceling UC SHIP coverage
When you can enroll
The UC SHIP periods of coverage align with the year’s academic terms.
Non-registered students can enroll in UC SHIP only during the voluntary enrollment period, which begins 30 days before the start of the coverage period and ends 30 days after the start date of the coverage period.
Example: If the winter coverage period begins Jan. 1, then enrollment for the winter term will start on Dec. 2 and end on Jan. 31.
You cannot enroll outside of the enrollment period, unless you have a qualifying life event.
Back to top
Enrollment criteria for a non-registered student
To enroll voluntarily, you must have been covered by UC SHIP in the term immediately preceding the term for which you want to purchase coverage.
Back to top
Enrolling as a non-registered student
As long as you meet the criteria above, you can enroll in UC SHIP as a non-registered student with filing fee status if any of the following circumstances apply to you:
You take a planned leave of absence. Students who take a planned leave approved by the University can purchase UC SHIP coverage on a voluntary basis for up to two quarters by submitting the appropriate enrollment form. Eligibility is contingent on prior quarter UC SHIP enrollment. Submit an enrollment form.
You are completing work under the support of the University of California but are not attending classes. You can purchase UC SHIP Filing Fee coverage on a voluntary basis for a maximum of one quarter. Submit an enrollment form.
You graduate. Read Extending coverage after graduating.
Back to top
Celebrating your 26th birthday this year?If you’re covered on a parent’s medical plan, you’ll no longer qualify for coverage when you turn 26. Learn how to enroll in UC SHIP below.
Enrolling after a qualifying life event
You and/or your eligible dependents can enroll in UC SHIP during the coverage period if you experience one of the following qualifying life events:
Involuntary loss of other health insurance coverage
Turn 26 and lose coverage as a dependent on a parent’s plan
Marriage or divorce
Birth or adoption of a child
Declaration or termination of a domestic partnership
When you have a qualifying life event during the coverage period, you must notify the insurance office on your campus within 31 days of losing your coverage and provide an official written letter of termination from your previous health insurance carrier.
You will be enrolled in UC SHIP as of the date of your involuntary loss of other coverage if you notify the insurance office within 31 days of losing your coverage. If you miss that window, your UC SHIP coverage will be effective on the date you pay the full UC SHIP premium.
To enroll yourself and your eligible dependents in UC SHIP after a qualifying life event, email Academic HealthPlans and provide proof of the qualifying life event.
To begin the enrollment process, complete the Qualifying Event (QE) form with the information needed.
Note: Premiums are not prorated for any enrollment occurring after the start of a coverage period, except premiums for newborns added as a dependent.
Back to top
Enrolling eligible dependents
For continued coverage, you must enroll your eligible dependents each term. You will need the following documentation to enroll them in UC SHIP:
For spouse, a marriage certificate
For same-sex/opposite-sex domestic partner, a Declaration of Domestic Partnership issued by the State of California or another country or state jurisdiction
For natural child, a birth certificate showing you’re the parent of the child
For stepchild, a birth certificate, and a marriage certificate showing that one of the parents listed on the birth certificate is married to you
For adopted or foster child, documentation from the placement agency showing that you have the legal right to control the child’s health care
For child eligible by court order, provide court documents that mandate the child will be covered under the insurance plan of the noncustodial parent
For voluntary and dependent enrollment in UC SHIP, complete the enrollment form.
Back to top
Canceling UC SHIP coverage
If you’re already enrolled in a health plan that meets the university health coverage requirements, you can apply to waive enrollment in UC SHIP. For more information about waiving coverage, visit the Waiving coverage webpage.
Back to top
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
What services are covered by UC SHIP?,"
UC SHIP provides comprehensive coverage for medical expenses. Here are some of the services that are covered:
• Emergency Room visits
• Doctor visits
• Hospital stays
• Surgical procedures
• Prescription medications
• Dental care
• Mental health services
• Laboratory tests
• X-rays and other imaging tests
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Vision therapy
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orthopedic surgery
• Physical therapy
• Occupational therapy
• Speech therapy
• Acupuncture
• Chiropractic care
• Massage therapy
• Vision care
• Hearing care
• Orthopedic care
• Orth","Paying for Coverage | UC Santa Cruz

UC SHIP premiums are increasing for the 2024-25 academic year.
Learn more about the increases
If you’re a registered student, you are automatically enrolled in UC SHIP and charged a health insurance premium on your registration bill.
If you’re a non-registered student enrolling yourself and/or your eligible dependents for coverage, you must pay the full insurance premium upon enrollment.
Learn what you’ll pay for coverage.
Help paying for medical expenses
Providing affordable health care is a UC SHIP mandate. Even so, every student’s financial position is unique. If your out-of-pocket medical expenses are causing you financial hardship, these UCSC resources may be able to provide financial assistance:
Slug Support Program
Health Equity Fund
Charity care may also be an option for you. Reach out to the provider’s office and ask if they offer any form of charity care. Although some providers do not offer any sort of charity care, others do. They may provide an application for you to complete or ask you to write a letter. Providers who do offer charity care will review your request, and then they could offer to write off the full amount you owe, a portion of that amount, or they may determine you don’t meet their qualifications for charity care.


"
Can I extend my UC SHIP insurance after I graduate?,"
Yes, you can extend your UC SHIP insurance after you graduate. You can purchase additional coverage if you completed your degree and purchased UC SHIP coverage during the coverage period in which you completed your degree. You can purchase additional coverage within 30 days of the coverage period ending.

Note: I have removed the sentence that starts with ""You can purchase"" as it is not necessary to include the word ""you"" in the answer. Here is the revised response:

","Extending Coverage After Graduation | UC Santa Cruz

After graduating, your coverage continues until the day before the next academic term begins. If you graduate during the spring term, your coverage continues through the summer until the day before the start of the next fall term.
You can purchase UC SHIP coverage for one additional term if you were enrolled in UC SHIP during the coverage period in which you completed your degree and you purchase coverage within 30 days from the date your coverage ends.
To purchase additional coverage, email the enrollment form to Academic HealthPlans (AHP) or call (855) 427-3167 before the next term (additional coverage period) begins."
Where can I go with UC SHIP?,"
You can go to the UC SHIP website for more information on the premium increase and how to pay for coverage.
You can contact the provider’s office for more information on the charity care options.
You can also go to the Student Health Services (SHS) office for more information on the UC SHIP coverage options.","Paying for Coverage | UC Santa Cruz

UC SHIP premiums are increasing for the 2024-25 academic year.
Learn more about the increases
If you’re a registered student, you are automatically enrolled in UC SHIP and charged a health insurance premium on your registration bill.
If you’re a non-registered student enrolling yourself and/or your eligible dependents for coverage, you must pay the full insurance premium upon enrollment.
Learn what you’ll pay for coverage.
Help paying for medical expenses
Providing affordable health care is a UC SHIP mandate. Even so, every student’s financial position is unique. If your out-of-pocket medical expenses are causing you financial hardship, these UCSC resources may be able to provide financial assistance:
Slug Support Program
Health Equity Fund
Charity care may also be an option for you. Reach out to the provider’s office and ask if they offer any form of charity care. Although some providers do not offer any sort of charity care, others do. They may provide an application for you to complete or ask you to write a letter. Providers who do offer charity care will review your request, and then they could offer to write off the full amount you owe, a portion of that amount, or they may determine you don’t meet their qualifications for charity care.


"
What is the spring final exam schedule?,"
Spring final exam schedule for the 2025 quarter includes the following classes: 
MWF 8:00 a.m. Monday, March 17, 12:00-3:00 p.m. Monday, June 9, 8:00-11:00 a.m. Monday, June 9, 4:00-7:00 p.m. Monday, June 9, 1:20 p.m. Monday, June 9, 2:40 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 4:00-7:00 p.m. Monday, June 9, 7:10 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 9:50 a.m. Monday, June 9, 11:40 a.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 1:30 p.m. Monday, June 9, 10:40 a.m. Monday, June 9, 3:20 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 7:10 p.m. Monday, June 9, 4:00-7:00 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 9:50 a.m. Monday, June 9, 11:40 a.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 1:30 p.m. Monday, June 9, 10:40 a.m. Monday, June 9, 3:20 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 7:10 p.m. Monday, June 9, 4:00-7:00 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 9:50 a.m. Monday, June 9, 11:40 a.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 1:30 p.m. Monday, June 9, 10:40 a.m. Monday, June 9, 3:20 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 7:10 p.m. Monday, June 9, 4:00-7:00 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 9:50 a.m. Monday, June 9, 11:40 a.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 1:30 p.m. Monday, June 9, 10:40 a.m. Monday, June 9, 3:20 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 7:10 p.m. Monday, June 9, 4:00-7:00 p.m. Monday, June 9, 7:30-10:30 p.m. Monday, June 9, 5:20 p.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 9:50 a.m. Monday, June 9, 11:40 a.m. Monday, June 9, 12:00-3:00 p.m. Monday, June 9, 1:30 p.m. Monday, June 9, 10:","Final Examinations

Home / Schedule of Classes / Final Examinations
Final ExaminationsFinal Examination Policies | Winter 2025 Final Exam Schedule | Winter 2025 Final Exam Schedule by Time Block | Spring 2025 Final Exam Schedule | Spring 2025 Final Exam Schedule by Time Block
Final Examination Policies
Final examinations are required in all undergraduate courses unless the department or other agency sponsoring the course has obtained permission from the Committee on Educational Policy to evaluate students in another manner. Final examinations are only given during the examination-week period at the time announced in the Schedule of Classes, usually in the same room used for class meetings during the quarter.  No change in the time or date of a final examination may occur unless the course sponsoring agency has obtained the approval of the Committee on Educational Policy (CEP). Requests must be received by CEP not later than the first week of the quarter in which the course is occurring. Completion or submission of final examinations or papers for undergraduate courses is not allowed during the regular term’s period of instruction, including the closed week before final examinations. When finals are administered (not during week 10), they must be completed at the scheduled examination time and may not require more than the scheduled three-hour time block. If a take-home examination is not assigned until the week designated for final examinations, it cannot require more than three hours to complete.
To avoid three final examinations on the same day, students may want to consider the final exam schedule when enrolling in courses.
Instructors may bar students from taking the examination if they arrive late. If a student misses an examination due to an unavoidable emergency, the instructor may agree to give an Incomplete and schedule a makeup examination provided that the student’s work is passing up to that point. When a final examination is one of the regular requirements in a course, no one taking the course may be individually exempted from it. Travel plans for vacation are not an emergency, and should not be made without checking the final examination schedule.
Closed Week
No examinations, tests, assignments, papers, final projects or final performances that result in more than 12.5 percent of the final grade (other than individual makeup exams) may be given during the last week of instruction. This does not include the collection of materials produced throughout the quarter, such as final portfolios.
Examination Retention
An instructor may release to individual students the original final examinations (or copies). Otherwise, the instructor will retain final examination materials at least until the end of the next regular term. During that time students will be allowed to review their examinations.
Religious Observance for Tests and Examinations
Given the diversity of religious practice within the campus community, academic and administrative units are encouraged to make reasonable accommodation when the schedule of a required campus event conflicts with an individual’s religious creed. It is the official policy of the University of California, Santa Cruz, to accommodate, without penalty, requests for alternate test or examination times in cases where the scheduled time for the test or examination violates a student’s religious creed, unless the request cannot be reasonably accommodated.
Requests to accommodate a student's religious creed by scheduling tests or examinations at alternative times should be submitted directly to the instructor in charge of the course as soon as possible after the test or examination is announced. Students who are unable to reach a satisfactory arrangement with the instructor should consult the head of the unit sponsoring the course. If the unit head feels that the request cannot be reasonably accommodated, the unit head should consult with the Dean of Graduate Studies or the Dean of Undergraduate Education as appropriate.
Accommodations for Disability
Students with documented disabilities that require examination modifications will be accommodated in compliance with state and federal laws. Reasonable accommodations will be made based on recommendations from the Disability Resource Center (DRC). If you have exam accommodations through the DRC, please note that your exam may be at an alternate test date or time.
Four-column chart of the winter 2025 final exam schedule. The first column lists the days a class meets; the second column lists the start time for the class; the third column lists the exam date; and the fourth column lists the time of the exam.
Winter 2025 Final Examination Schedule
Class
Start
Exam Date
Exam Times
MWF
8:00 a.m.
Friday, March 21
8:00–11:00 a.m.
MWF
9:20 a.m.
Wednesday, March 19
4:00–7:00 p.m.
MWF
10:40 a.m.
Monday, March 17
12:00–3:00 p.m.
MWF
12:00 p.m.
Thursday, March 20
4:00–7:00 p.m.
MWF
1:20 p.m.
Wednesday, March 19
12:00–3:00 p.m.
MWF
2:40 p.m.
Tuesday, March 18
12:00–3:00 p.m.
MWF
4:00 p.m.
Thursday, March 20
12:00–3:00 p.m.
MW
5:20 p.m.
Monday, March 17
4:00–7:00 p.m.
MW
7:10 p.m.
Tuesday, March 18
7:30–10:30 p.m.




TuTh
8:00 a.m.
Thursday, March 20
8:00–11:00 a.m.
TuTh
9:50 a.m.
Wednesday, March 19
8:00–11:00 a.m.
TuTh
11:40 a.m.
Monday, March 17
8:00–11:00 a.m.
TuTh
1:30 p.m.
Tuesday, March 18
4:00–7:00 p.m.
TuTh
3:20 p.m.
Tuesday, March 18
8:00–11:00 a.m.
TuTh
5:20 p.m.
Wednesday, March 19
7:30–10:30 p.m.
TuTh
7:10 p.m.
Monday, March 17
7:30–10:30 p.m.

Non-Standard 1*
Friday, March 21
12:00–3:00 p.m.
Non-Standard 2**
Thursday, March 20
7:30–10:30 p.m.
*Non-Standard 1: classes which have their first meeting M or W or F and do not begin at 8:00 a.m., 9:20 a.m., 10:40 a.m., 12:00 p.m., 1:20 p.m., 2:40 p.m., 4:00 p.m., 5:20 p.m. This and Non-Standard 2 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students. **Non-Standard 2: classes which have their first meeting T or Th and do not begin at 8:00 a.m., 9:50 a.m., 11:40 a.m., 1:30 p.m., 3:20 p.m., 5:20 p.m. This and Non-Standard 1 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students.

Six-column chart of the winter 2025 final exam schedule by time block. The first column lists the exam day and time; the second column lists exams to be given on Monday; the third, exams to be given on Tuesday; the fourth, exams on Wednesday; the fifth, exams on Thursday. (There are four days of finals in sprinq quarter and five days in fall and winter quarters.)
Winter 2025 Final Exam Schedule by Time Block
Exam Day/ Exam Time
Monday,March 17
Tuesday,March 18
Wednesday,March 19
Thursday,March 20
Friday,March 21
Exam Period 8:00–11:00 a.m.
Tues, Thur 11:40 a.m.
Tues, Thur3:20 p.m.
Tues, Thur9:50 a.m.
Tues, Thur 8:00 a.m.
Mon, Wed, Fri8:00 a.m.
Exam Period 12:00–3:00 p.m.
Mon, Wed, Fri10:40 a.m.
Mon, Wed, Fri2:40 p.m.
Mon, Wed, Fri1:20 p.m.
Mon, Wed, Fri4:00 p.m.
Non-Standard 1
Exam Period 4:00–7:00 p.m.
Mon, Wed,5:20 p.m.
Tues, Thur 1:30 p.m.
Mon, Wed, Fri9:20 a.m.
Mon, Wed, Fri12:00 p.m.
Exam Period 7:30–10:30 p.m.
Tues, Thur 7:10 p.m.
Mon, Wed,7:10 p.m.
Tues, Thur 5:20 p.m.
Non-Standard 2

Four-column chart of the spring 2025 final exam schedule. The first column lists the days a class meets; the second column lists the start time for the class; the third column lists the exam date; and the fourth column lists the time of the exam.
Spring 2025 Final Examination Schedule
Class
Start
Exam Date
Exam Times
MWF
8:00 a.m.
Tuesday, June 10
8:00–11:00 a.m.
MWF
9:20 a.m.
Monday, June 9
8:00–11:00 a.m.
MWF
10:40 a.m.
Wednesday, June 11
12:00–3:00 p.m.
MWF
12:00 p.m.
Monday, June 9
4:00–7:00 p.m.
MWF
1:20 p.m.
Thursday, June 12
8:00–11:00 a.m.
MWF
2:40 p.m.
Tuesday, June 10
4:00–7:00 p.m.
MWF
4:00 p.m.
Tuesday, June 10
7:30–10:30 p.m.
MW
5:20 p.m.
Thursday, June 12
4:00–7:00 p.m.
MW
7:10 p.m.
Wednesday, June 11
7:30–10:30 p.m.

TuTh
8:00 a.m.
Wednesday, June 11
8:00–11:00 a.m.
TuTh
9:50 a.m.
Monday, June 9
12:00–3:00 p.m.
TuTh
11:40 a.m.
Tuesday, June 10
12:00–3:00 p.m.
TuTh
1:30 p.m.
Wednesday, June 11
4:00–7:00 p.m.
TuTh
3:20 p.m.
Thursday, June 12
12:00–3:00 p.m.
TuTh
5:20 p.m.
Monday, June 9
7:30–10:30 p.m.
TuTh
7:10 p.m.
Thursday, June 12
7:30–10:30 p.m.

Non-Standard 1*
Wednesday, June 11
7:30–10:30 p.m.
Non-Standard 2**
Thursday, June 12
7:30–10:30 p.m.
*Non-Standard 1: classes which have their first meeting M or W or F and do not begin at 8:00 a.m., 9:20 a.m., 10:40 a.m., 12:00 p.m., 1:20 p.m., 2:40 p.m., 4:00 p.m., 5:20 p.m. This and Non-Standard 2 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students. **Non-Standard 2: classes which have their first meeting T or Th and do not begin at 8:00 a.m., 9:50 a.m., 11:40 a.m., 1:30 p.m., 3:20 p.m., 5:20 p.m. This and Non-Standard 1 are options for asynchronous remote and online classes. Instructors of asynchronous courses are encouraged to offer flexibility to students.

Five-column chart of the spring 2025 final exam schedule by time block. The first column lists the exam day and time; the second column lists exams to be given on Monday; the third, exams to be given on Tuesday; the fourth, exams on Wednesday; the fifth, exams on Thursday. (There are four days of finals in sprinq quarter and five days in fall and winter quarters.)
Spring 2025 Final Exam Schedule by Time Block
Exam Day/ Exam Time
Monday,June 9
Tuesday,June 10
Wednesday,June 11
Thursday,June 12
Exam Period 8:00–11:00 a.m.
Mon, Wed, Fri
9:20 a.m.
Mon, Wed, Fri 8:00 a.m.
Tues, Thur 8:00 a.m.
Mon, Wed, Fri 1:20 p.m.
Exam Period 12:00–3:00 p.m.
Tues, Thur 9:50 a.m.
Tues, Thur 11:40 a.m.
Mon, Wed, Fri 10:40 a.m.
Tues, Thur 3:20 p.m.
Exam Period 4:00–7:00 p.m.
Mon, Wed, Fri 12:00 p.m.
Mon, Wed, Fri 2:40 p.m.
Tues, Thur
1:30 p.m.
Mon, Wed 5:20 p.m.
Exam Period 7:30–10:30 p.m.
Tues, Thur 5:20 p.m.
Mon, Wed, Fri4:00 p.m.
Non-Standard 1 and Mon, Wed 7:10 p.m.
Non-Standard 2 and Tues, Thur 7:10 p.m.
"
What holidays are during the summer quarter?,"
Summer academic calendar (deadlines) for the upcoming quarter is as follows:

* Independence Day (July 4) - No classes are held in observance of Independence Day.
* The Summer Session Office will enroll students when their forms are submitted with needed signatures.","Summer Academic Calendar (Deadlines)

Home / Student Life
Summer Academic Calendar (Deadlines)
All Summer Session dates and deadlines below are 11:59 p.m. PST.  
Important reminders:
There is no ""Add by Petition"" in summer. 
You must enroll by the deadline below.
July 4 - No classes are held in observance of Independence Day.
Jump to Session 1, Session 2, 8-Week, 10-Week, or Independent and Field Studies deadlines.
Session 1 June 23 - July 25, 2025
Deadlines
• Add/Swap - Thursday, June 26
• Drop - Monday, June 30 (tuition reversed)
• No classes are held in observance of Independence Day July 4
• Request ""W"" Grade - Sunday, July 13 (no tuition reversal)
• Change Grade Option - Sunday, July 20• Grades Due - Thursday, July 31
Session 2 July 28 - August 29, 2025
Deadlines
•Add/Swap - Thursday, July 31
• Drop - Monday, August 4 (tuition reversed)
• Request ""W"" Grade - Sunday, August 17 (no tuition reversal)
• Change Grade Option - Sunday, August 24• Grades Due - Thursday, September 4 
8-Week Session June 23 - August 15, 2025
Deadlines
• Add/Swap - Thursday, June 26
• Drop - Monday, July 7 (tuition reversed)
• No classes are held in observance of Independence Day July 4
• Request ""W"" Grade - Sunday, July 27 (no tuition reversal)
• Change Grade Option - Sunday, August 10
• Grades Due - Thursday, August 21
10-Week Session June 23 - August 29, 2025
Deadlines
• Add/Swap - Thursday, July 3
• No classes are held in observance of Independence Day July 4
• Drop - Monday, July 7 (tuition reversed)
• Request ""W"" Grade - Sunday, July 27 (no tuition reversal)
• Change Grade Option - Sunday, August 24
• Grades Due - Thursday, September 4
Independent and Field StudiesAdd deadlines for field studies may follow the timeline of the hosting department. 
The Summer Session Office will enroll students when their forms are submitted with needed signatures.
Fees Posted
Found daily in Student Portal (my.ucsc.edu)
Tuition Billing Statements(1st) Billed May 29 – Due June 17
(2nd) Billed June 26 – Due July 22
(3rd) Billed July 24 – Due August 11 (final due date)
You can start paying only after charges post to your account. 
If your account says ""Future"" it is likely because you enrolled between billing statements, and you will have to wait until the next billing statement to begin making payments. "
What classes are offerred in the NLP program?,"
Information about the NLP program can be found on the UC Santa Cruz website.  However, I was unable to find the specific classes offered in the NLP program.  I would like to know what classes are offered in the NLP program.","NLP | Gradadmissions
​
Important: Transcripts and Letters of Recommendation are required of all applicants.

Read the Application Instructions​ page for full details.
​
See the test score requirements for this program.
​
Below are specific requirements also needed for this program:
Résumé or CV (can also provide a link to your own website)
A Statement of Purpose is required:
Please describe your plans for graduate study or research and for your future occupation or profession. Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz. Recommended length is a concise 2-4 pages, single-spaced.
A Personal History Statement is also required:​
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose. Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In an essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Financial Need Instructions:​
This optional Statement of Financial Need will help the Natural Language Processing Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.​
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
​
"
How many credits are the NLP courses?,"
The NLP courses are equivalent to 4 credits each, and there are a total of 8 courses in the program. Therefore, the total number of credits required for the program is 8 x 4 = 32 credits.","Admissions | Natural Language Processing M.S.

ADMISSIONS
Get ready to apply for Fall 2025 admission consideration
APPLY STARTING OCT 1ST
Applying to the NLP MS Program at UCSC
We’re delighted that you’re interested in applying to UCSC’s unique professional master’s program in natural language processing. The NLP MS program is intended for students who have a considerable background in computer science, and we are looking for multifaceted individuals who ideally have experience with programming, computer science, probability, statistics, and linguistics. We review applications holistically and we have students in the NLP MS program with different undergraduate degrees who are successful in the program.
Below please see a list of recommended courses and requirements that will help you be successful in our program. In your application, you can explain any gaps between your background and the admission guidelines below.
Applicants wishing to study a broader range of computer science topics in a graduate program may wish to consider the MS in Computer Science and Engineering degree program. The program cost varies with the selected curriculum plan. Please check the Registrar’s website for the most recent tuition and fee information.
Admission requirements:
A B.A. or B.S in Computer Science or equivalent
An overall GPA of 3.0 or better on the 4.0 GPA scale or the equivalent
Excellent programming skills either through academic coursework and/or significant professional experience
TOEFL or IELTS scores (if applicable)
Minimum TOEFL score of 89
Minimum IELTS score of 7.0
Essential courses to highlight in your application:
A class in Discrete Math (the equivalent of UCSC’s CSE 16: Applied Discrete Mathematics)
An advanced class in Data Structures and Algorithms (the equivalent of UCSC’s CSE 101: Introduction to Data Structures and Algorithms)
At least one class in Probability and Statistics with a grade of B or better (the equivalent of UCSC’s CSE 107: Probability and Statistics for Engineers or STAT 131: Introduction to Probability Theory)
Desirable (but not required) additional requirements include:
A class in NLP or Linguistics (the equivalent of UCSC’s CSE 143: Introduction to Natural Language Processing)
A class in Machine Learning (the equivalent of UCSC’s CSE 142: Machine Learning)
A class in Formal Languages and Automata Theory (the equivalent of UCSC’s CSE 103: Computational Models)
Want to learn more? Connect with our Support TeamThere are lots of ways to get support with your application. The NLP Support Team is ready to assist with any questions you may have about the program, our industry-informed curriculum, and applying to UCSC.
Visit our Contact page to connect with our NLP Support Team today.
Application Materials
Required application materials
Application fee:
Domestic: $135
International: $155
If you are currently enrolled in a UCSC undergraduate or graduate program or have an undergraduate or graduate degree from a UCSC program, you may request an application fee waiver directly from the NLP Program.
Review the Graduate Admissions website for information about fee waivers and exemptions.
GPA: minimum 3.0 on 4.0 scale or equivalent.
Transcripts: unofficial transcripts are accepted for application purposes but official transcripts may be required if admitted to the program.
Letters of Recommendation: 3 letters required from professors and professional contacts who may best speak to your abilities and academic potential in your chosen field of study.
TOEFL or IELTS  (if applicable)
TOEFL: minimum score of 89
IELTS: minimum overall score of 7.0
Personal History Statement:
Recommended length is a concise 1-3 pages, single-spaced.
UCSC is interested in a diverse and inclusive graduate student population. In the essay, discuss how your personal background informs your decision to pursue a graduate degree.
Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
Statement of Purpose:
Recommended length is a concise 2-4 pages, single-spaced.
Please describe your plans for graduate study or research and for your future occupation or profession.
Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at UCSC.
CV required, preferably from a link to a personal web page containing CV
NOTE: GRE (no minimum) – The GRE requirement has been waived for Fall 2024 admission consideration for NLP MS applications. You are welcome to submit your official scores, but please note the inclusion of your official GRE score will not be a deciding factor for admission.
Application Instructions & FAQ
Admissions FAQ
Application FAQ
Application Instructions
AB 540 Info
International Applicants
We’re here to help! Visit our Contact page to connect with a member of our support team. You can also email the NLP Program Team directly at nlp@ucsc.edu.
Funding opportunities: MS students are typically self-supported. While a small number of partial fellowship packages are extended to eligible NLP students each year, the NLP Program encourages students to seek external support for covering graduate education expenses. Recipients of these fellowships are selected based on financial need and merit. We strongly recommend eligible applicants complete the optional Statement of Financial Need in the application to be considered for financial support. Instructions for completing this statement are available on the Graduate Admissions website.
Visit the NLP Financials page to review additional information about the estimated costs of attendance and financial support options.
Join an info session to learn more! The NLP Program Team hosts virtual information sessions each year about the unique benefits of studying NLP at UCSC. At each session, you’ll have an opportunity to meet members of our community including faculty and staff, and to ask any questions you may have about the program and application. Below is a schedule of upcoming information sessions you can attend from anywhere in the world:
Friday, November 3rd from 11 AM – 12 PM (PDT) – Register now
Wednesday, December 6th from 7 PM – 8 PM (PST) – Register now
Tuesday, January 9th from 8 AM – 9 AM (PST) – Register now
For parents and families of prospective students: We know that going to graduate school can involve a significant investment of your time and personal resources. As you consider your options for graduate studies, you may also want to include your support network in your decision-making process. We’ve put together a guide for parents and family members to provide insight into life at UCSC, as well as the support and resources available to our graduate students. Please always feel welcome to contact our Support Team if you or your family members have any questions.
Join an NLP Information Session!At our information sessions, we discuss the range of courses and support services we offer, as well as tips for applying to the program. We also prioritize time for a Q&A session during each event, where you’ll have the opportunity to meet NLP faculty and staff.
To attend an information session, please complete the session’s registration form. You’ll then be sent an individualized Zoom link you can use to access the information session. Please note that dates and times for upcoming sessions are subject to change.
November 6, 202411 AM – 12 PM PST
Register online
December 4th, 20247 PM – 8 PM PST
Register online
January 15, 20258 AM – 9 AM PST
Register online"
How can I change my grading option for a class?,"
A: Log into MyUCSC, select the Enrollment tile, select Enrollment: Edit a Class, select the correct term, and then choose the class you want to change from the drop-down menu. Select Proceed to Step 2 of 3, and then choose Graded or Pass/Not Pass (undergraduates) or Satisfactory/Unsatisfactory or ABC/NC Grading (graduates). Undergraduate students who are not in good academic standing will only have a Graded option. Review your selections and select Finish Editing if correct.","FAQs for Students: Grading
Q: How do I change my grading option? 
A: Log into MyUCSC. Select the Enrollment tile.
Select Enrollment: Edit a Class. Select the correct term and Continue.
Choose the class you would like to change from the drop down menu. Select Proceed to Step 2 of 3.
If you are enrolled in a discussion section, your selected section will be displayed. Select Next.
In the Grading drop down menu, choose Graded or Pass/Not Pass (undergraduates) or Satisfactory/Unsatisfactory or ABC/NC Grading (graduates). Undergraduate students who are not in good academic standing will only have a Graded option. Select Next.
Review your selections and select Finish Editing if correct.
Q:What is the deadline? 
You can find the exact date on the Academic and Administrative Calendar. This is a hard deadline, and the Office of the Registrar cannot extend the deadline. If you feel you have extenuating circumstances, contact your college advising office for options.
Q: Can I convert my Pass/No Pass grades into letter grades for a GPA based on my performance? 
A: After the grade option deadline has passed, the Office of the Registrar cannot convert pass/no pass grades to letter grades, or to a GPA. If your ability to graduate may be affected by a pass/no pass grade option, contact your college advising office for more information.
Q: How do I request an Incomplete grade and what is the process? 
A: You can request an Incomplete from your instructor if your work so far is passing and the quarter has not ended. It is up to the instructor’s discretion to grant an Incomplete. Undergraduates have up to one quarter to submit the final coursework to the instructor. Graduate students are allowed up to one calendar year.
If the course work isn’t submitted by the deadline the Incomplete grade lapses to an F or NP, depending on the grading option in effect. If the coursework is submitted by the deadline and the instructor has not entered the final grade the Incomplete will automatically lapse to a non-passing grade. The instructor still has the ability to enter the final grade.
Your instructor may require you to fill out a Petition for Removal of Incomplete. This form must be submitted to the Office of the Registrar by the instructor/department, not the student. However, your instructor can change an Incomplete grade to the final grade in the system without using this form.
Q: How do I check my grades or GPA? 
A: Log into MyUCSC and select the My Academics tile.
For a specific quarter, select Grades and then the correct quarter. You can see your grades for that quarter as well as your term (quarter) GPA and cumulative (total) GPA.
To view all of your grades, select Course History."
"I want to drop a class after the deadline, what will happen?","
A: If you drop a class after the deadline, you will receive a ""W"" grade on your transcript, but you won't be able to drop the class. You will be charged tuition and fees for the class. If you have any questions or concerns, you can contact the academic advising office for guidance.","FAQs for Students: Withdrawal
Q: I’m a currently enrolled student, but I am unable to complete the quarter. How do I withdraw from the university?  
A: For fall, winter, and spring quarters, you must file a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return.
Over the Summer Session, students must manually drop or withdraw from their courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or other emergency reasons. Contact an advisor at your college if you would like to pursue this option.
Q: If I need to withdraw during the quarter, what is the refund policy?  
A: See the schedule of refunds policy in The Navigator. For specific dates, see the Fee Refund Schedules. Over the summer, tuition is only reversed before the drop deadline.
Q: How does the college determine the effective date of withdrawal?  
A: As currently defined by UCOP and in accordance with federal financial aid policy, the effective date for determining a refund of fees or the adjustment of financial aid is the date the student notifies the college of his or her intent to withdraw.
Q: What is the withdrawal petition deadline in the Academic and Administrative Calendar?  
A: After the third week of the quarter you may Petition to Withdraw from a Course. You will receive a W grade if submitted before the 6th-week deadline, and a Not Pass (NP) grade if submitted between the sixth and ninth weeks of instruction. Dates and deadlines are listed in the Academic and Administrative Calendar. If there is a medical or emergency reason to withdraw from the course after the sixth week, students should discuss their options with the college advising office. Documentation is required for an emergency withdrawal from a course.
Information on requesting a W grade in summer classes is available here.
Q: What is the difference between withdrawing from the university and taking W grades in all my classes?  
A: Withdrawing from the university is a full withdraw from UCSC, and tuition may be refunded based on the Refund Schedule. After the sixth week of the quarter, an undergraduate may withdraw only for medical/emergency reasons.  A Withdraw (W) or Not Pass (NP) grade is a grade posted to a specific course or courses. You do not receive credit for the course and the W/NP grades do not affect your GPA. W grades have an impact on academic progress. There may also be financial consequences for students receiving financial aid or Veteran’s benefits if they withdraw from UCSC or request W grades. The period for requesting a W/NP grade otherwise extends until the deadline published in the Academic and Administrative Calendar.
Q: Is there a financial consequence for taking all W grades?  
A: Opting for all W grades may result in a reduction in financial aid. Also, students who receive benefits from the Department of Veterans Affairs will have a reduction in quarterly benefits when the W grades result in less than 12 credits completed. Over the summer session, there are no tuition or campus fee refunds for taking a W grade.
Q: What is the Withdrawal Without Penalty deadline on My Class Schedule?  
A: It's an administrative date that indicates that if a student withdraws from the University on or before the last day of the term, no classes appear on the transcript for the term.
Q: Where can I read more about the withdrawal process and W grades?  
A: For more information about withdrawing from the University, see Withdrawal from the University. For W grades in fall, winter, and spring quarters, see Withdrawal from a Course. For W grades in summer, see more information here."
What should I do when one of my classes conflicts with another?,"
A: You may be able to resolve the conflict by swapping to a different secondary section, lab, studio, etc. offered at another time. If you are unable to resolve the conflict by swapping, contact the instructor of the class to learn whether you will be permitted to leave class early or show up late. If given permission, during the fall, winter, and spring quarters, contact the Office of the Registrar. For Summer Session courses, email summer@ucsc.edu. We will override any time conflict if you have worked it out with the instructor.","FAQs for Students: Student Enrollment

NOTE: Check your schedule on MyUCSC before the third week of classes to make sure that you are enrolled in the classes you are attending, that you have dropped any class you are not planning to attend, and that the grading option you want (graded or pass/no pass) appears on your schedule.
Q: What is my Validation Appointment?[show]
A: Your Validation Appointment allows you to check that you have met the prerequisites for a class and whether time conflicts or other registration issues exist before your enrollment appointment. Validation does not guarantee enrollment in a course.
Q: What is MyScheduler?[show]
A: MyScheduler is a tool to help you avoid time conflicts. Select the the MyScheduler link for more information.
Q: How do I enroll?  
A: Log in to MyUCSC. For detailed help, the following documents (with screen shots) are available:
How to Validate Your Shopping Cart
How to Search for a Class
How to Enroll in a Class
How to Drop a Class
How to Swap a Class
Q: There is a hold on my enrollment. What can I do to clear it?  
A: Log on to MyUCSC and go to My Student Center, then select Search for Classes to get to the Class Search. Holds are displayed on the right. Select “Details” to find information regarding the hold and how to get it released.
Q: How do I change my grading option?  
A: For information on grading, visit FAQs for Students: Grading.
Q: What are permission numbers?  
A: Permission numbers override course requirements and/or a closed status of a class. They are required for interview only/instructor consent classes. During fall, winter, and spring quarters, permission numbers are required for all classes, except individual studies, on the eighth day of instruction. Contact the instructor or department associated with the class to request a permission number. Distribution of permission numbers is subject to department and/or instructor discretion. During the Summer Session, permission numbers are only used to override course requirements and you can contact summer@ucsc.edu until the add deadline for each session.  If you have obtained a permission number, you can enter it into your enrollment request as shown in Step 10 in How to Enroll in a Class. If using a permission number with waitlisting, do not also select “Drop if Enrolled.”
Q: I have taken the required prerequisites for a course at another institution, yet I am unable to enroll for the class. What should I do?  
A: Contact the department or college offering the course.
For newly admitted students taking summer classes before your first fall, send your unofficial transcripts or proof of prerequisite to summeredge@ucsc.edu.
Q: I am getting an enrollment error message that says the prerequisites have not been met, but I believe that I have fulfilled the prerequisites. What do I do?  
A: Review the prerequisites, co-requisites and other restrictions for the class in the Class Search and confirm that your student record reflects satisfaction of all requisites. If there is still a perceived problem, contact the Office of the Registrar, or summer@ucsc.edu for summer classes.
Q: I am unable to enroll in a class that is open because all of the mandatory sections are closed. What should I do?  
A: Contact the department that is offering the class.
Q: Is it possible for an instructor to drop a student from a class?  
A: Yes, students who do not attend the first class meeting may be administratively dropped at the instructor's request. View your schedule via your Student Center to confirm your enrollments prior to the Add/Drop/Swap deadline.
Q: I am trying to enroll in a class, but the time conflicts with another class or section. What should I do?  
A: You may be able to resolve the problem by swapping to a different secondary section, lab, studio, etc. offered at another time.  If you are unable to resolve the conflict by swapping, contact the instructor of the class to learn whether you will be permitted to leave class early or show up late. If given permission, during the fall, winter, and spring quarters contact the Office of the Registrar. For Summer Session courses, email summer@ucsc.edu. We will override any time conflict if you have worked it out with the instructor. We do not require written verification.
Q: How do I view my Placement Exam scores?  
A: Viewing Your Score: Scores can be viewed once they have been matched to your academic record and posted in the student portal at MyUCSC.
Log in to MyUCSC
Click on the ""My Academics"" tile.
Select ""Test Scores"" from the left side menu.
Click on the View All button at the top of the grid to see all of your official examination results.
Q: I missed the Add/Drop/Swap deadline. What should I do?  
A: You cannot drop a course but can submit a withdraw petition to withdraw from a course. Contact your college adviser before the withdraw deadline. You may still add a class in fall, winter, and spring through the Add By Petition process. Submit the form to the Office of the Registrar by the Add By Petition Deadline. In summer, you may not add by petition."
Will I be dropped from the class if I don't go on the first day? I have a job so I won't be able to make it.,"
A: You will be dropped from the class if you don't attend the first class meeting. If you are unable to attend the first class meeting, you may be dropped by the instructor. If you are unable to attend the first class meeting for any reason, you can contact the instructor or the Academic and Administrative Calendar to request a permission number to enroll. However, be aware that the waitlist process will not guarantee that you will be enrolled in the class, and the Academic and Administrative Calendar will not guarantee that space will open up in the class.","FAQs for Students: Waitlist
In fall, winter, and spring quarters during their waitlist appointment students may begin adding up to 14 waitlist credits. Students can see their waitlist appointment by selecting “details” under “Enrollment Dates” in their MyUCSC.
The waitlist runs every half-hour during normal business hours through the seventh day of instruction; your waitlist position number may not indicate the order in which you will be enrolled in the class. For example, your waitlist position will be skipped if you have scheduled the course with a time conflict or have attempted to enroll in more than your allotted credits. If you are still on the waitlist at the start of instruction, attend the first class meeting to determine if you will be able to enroll. You may be dropped for not attending the first class meeting. Some things to consider:
Waitlisting does not guarantee enrollment;
Waitlisting does not guarantee more seats will open in a class;
Error messages do not indicate that space is being held open in a class or that enrolment is guaranteed.
Following the 7th day of instruction, the wait list is no longer an option and permission numbers are required to enroll (see the Academic and Administrative Calendar). Instructors and/or departments issue permission numbers. In summer, waitlists run every half-hour, 24 hours a day until the add deadline. Check your portal to ensure you know if you were enrolled from the waitlist. Attend class if possible.
Q: How do I waitlist a course?  
A: 1. Sign in to MyUCSC.
2. Select the Enrollment or Student Center tile.
3. Select the add tab at the top of page. Select the correct quarter and type in the five-digit course number of the course you would like to waitlist.
4. Check the “Waitlist if class is full” box and select Next.
5. If you would like to drop a course that you are currently enrolled in if you become enrolled in the wait-listed course you can indicate this in the “If Enrolled Drop This Class” drop down menu. You may only select a class as “Drop if Enrolled” once. If you need to use a permission number, you cannot also use ""Drop if Enrolled.” The waitlist process will error and you will not be enrolled.
6. Review your selection and select Proceed to Step 2 of 3.
7. To complete the enrollment select on Finish Enrolling. A checkmark under Status means that your enrollment was successful.
Q: How do I drop a wait-listed course?  
A: Follow these steps:
Sign in to MyUCSC. Select the Enrollment or Student Center tile.
Select the drop tab at the top of the page. Select the correct quarter and check the box next to the course you wish to remove from your waitlist.
Select the secondary section that you would like to change or just select next for no changes to the secondary section.
Select the Drop Selected Classes box.
Confirm the selection and select Finish Dropping. A checkmark under Status means that your drop was successful.

Q: If I get an error message, how do I edit a wait-listed course?[show]
A: If you are unable to successfully enroll in a wait-listed course because of too many credits or a time conflict, you can edit your waitlist.
Sign in to MyUCSC. Select the Enrollment or Student Center tile.
Select the edit tab on the top of the page. Select the correct quarter and choose the course you would like to edit. Select Proceed to Step 2 of 3.
Select the secondary section that you would like to change or just select next for no changes to the secondary section.
Indicate the course you would like to drop if you are enrolled in your wait-listed course in the “If Enrolled Drop This Class” drop down menu. Select Next.
Review your selection and select Finish Editing to process your edit request. A checkmark under Status means that your edit was successful. You can return to the edit tab at any time to review the setup of your wait-listed course.
If you preemptively drop the class that you have specified as your Drop if Enrolled class, you will remain on the waitlist, but without a Drop if Enrolled class. You may not get into the wait-listed class if your enrollment will exceed your credit limit. If you change the section for your Drop If Enrolled option, that class will no longer be listed as your Drop If Enrolled class, because your enrollment setup has been changed. In this case, you may edit your waitlist enrollment to specify a new Drop if Enrolled class.
Editing your waitlist does not guarantee enrollment.
Instructions how to edit your waitlist are also available here.
Q: How do I view my position on the waitlist?[show]
A: Follow these steps:
Sign in to MyUCSC. Select the Student Center tile.
Under the Academics heading on the top left of the page is a drop-down menu that says “more links…” Select Class Schedule from that menu and select the Go button.
Your wait-listed classes will be shown on the Class Schedule page, and your waitlist position is shown in the Waitlist Position column.
It is important to make sure you do not have any time conflicts or too many credits. If so, the process will fail to enroll you and move on to the next person on the waitlist.
The waitlist system may try to enroll you even if you are not number one on the waitlist due to scheduling issues for preceding waitlisted students.
Q: How do I change my section for a wait-listed course?  
A: You may change your waitlist secondary section, by using the EDIT option. See the FAQ above ""If I get an error message, how do I edit a wait-listed course?""
Q: What is the $10 Waitlist Correction Fee and how do I avoid it?  
A: Starting winter 2019 quarter, the campus implemented a $10 Waitlist Correction Fee for assistance from our office with switching waitlisted course options in fall, winter, and spring quarters. The fee does not apply to manual actions taken by you, and does not apply to the first correction (the fee will be automatically waived for the first call), but it will be applied for the second correction and any thereafter.
You can avoid the fee by being strategic in your enrollment.
Enroll in a backup class first.
Enroll in the wait-listed course, checking the ""Waitlist if class is full"" box. Indicate in the ""If Enrolled Drop This Class"" drop down menu which back up course you would drop if enrolled in the wait-listed course.
Check to make sure you do not have any time conflicts with the wait-listed course.
Some co-requisite labs require a two-step waitlist process. Sign up on the waitlist for the lab. Once enrolled in the lab, you will then be notified to enroll in the co-requisite class.
Q: Can I waitlist two offerings of the same course?  
A: You can. However, once you are enrolled in one offering of the course you will automatically be dropped from the wait list of the course that you did not get into. If you are already enrolled in one offering of a class and would like to wait list for a different offering, you must set up the wait-list enrollment as a swap so that you are not dropped from the wait-listed class. For example, if you are enrolled in Physics 6L-01 and you'd rather be in Physics 6L-02, you will need to set up the enrollment so that if you get into Physics 6L-02 you will be dropped from Physics 6L-01. Otherwise, the system will drop you off of the wait list from Physics 6L-02 and you will remain enrolled in Physics 6L-01.  Please be aware that if you change your section for the enrolled offering of the class, that class will no longer be listed as your ""drop if enrolled"" class, because the enrollment has changed, and you will be automatically dropped from the waitlisted class.
Q: What do the waitlist error messages mean and how do I resolve the error?  
A: See information on waitlist error messages here.
Pro Tip: When choosing a course which has multiple enrollment options (e.g., MATH 19A-01 and MATH 19A-02) or has required secondary discussion sections or labs, make sure to ""View All"" to see all options. Sign up for all classes that work in your schedule. Also, when signing up for a class that has a secondary discussion section or lab, add the secondary discussion section or lab that fits in your schedule with lowest waitlist enrollment. This will improve your chances of getting in when a spot opens up one of the courses.
If the prior term’s grades are still not finalized by the time of enrollment for the upcoming term, there is a conditional enrollment that will be rechecked once the upcoming term starts and you will either remain in the course or be dropped depending on the grades of the prerequisite.   Please also note that students can waitlist for a course regardless of meeting prerequisites, though when the waitlist runs the prerequisites are checked and an enrollment is either processed or an error occurs."
How can I become an honors student?,"
A: To become an honors student, you must meet the criteria for Dean's Honors or University Honors. You can meet the criteria by being in a degree-seeking program, earning a minimum of 12 credits, and having a qualifying term grade point average. You can meet the criteria by being in the University Honors program and having a qualifying UC grade point average. You can meet the criteria by being in the School of Engineering and having a qualifying GPA.","FAQs for Students: Honors
To acknowledge and reward student academic achievement, Dean’s Honors are awarded quarterly, and University Honors (Summa cum laude, Magna cum laude, and Cum laude) are awarded at graduation. These new honors designations are in addition to the long-standing College Honors and Department Honors awarded to students at graduation. These honors began being awarded fall 2008; they are not awarded retroactively. A description of the Undergraduate Honors Program can be found in the General Catalog's Evaluating Academic Performance section.
Q: What types of honors are awarded to UCSC students on a quarterly basis?  
A: Dean’s Honors are awarded to UCSC students on a quarterly basis.
Q: What types of honors are awarded to UCSC students at graduation?  
A: University Honors, College Honors, and Department Honors are awarded to UCSC students at graduation.
Q: What are Dean's Honors?  
A: Dean's Honors are awarded quarterly to undergraduates who:
were admitted in or after fall 2001;
are in a degree-seeking program;
if full-time, have earned a minimum of 12 credits that quarter, of which at least 10 are graded; or if on the University Part Time Program, have 10 or fewer grades credits; and
have a qualifying term grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are University Honors?  
A: University Honors are awarded to undergraduates receiving a degree, who:
were admitted in or after fall 2001;
have completed 70 or more units at the University of California; and
have a qualifying UC grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are qualifying grade point averages for Dean's Honors and University Honors?  
A: Please see the page Grade Point Average Criteria for Honors.
Q: How are the grade point average requirements for honors determined?  
A: They are based on the grade point averages of the top 2%, 5%, and 15% of recent graduates.
Q: When will the grade point average requirements be available for the coming year?  
A: Grade point average requirements should be available by the beginning of each fall quarter.
Q: Why are there different grade point average criteria for School of Engineering students than for other students?  
A: Honors eligibility for students whose degree program is in an established School shall be based on the GPA thresholds calculated for that School. Honors eligibility for other students shall be based upon the GPA thresholds calculated for students whose degree program is not part of an established School. (Academic Senate Regulation 11.4)
Q: Who is considered a School of Engineering student?  
A: For the purposes of receiving Dean’s Honors, any student who as of the last day in the quarter has a declared major offered by the School of Engineering. For the purposes of receiving University Honors, any student graduating in a major offered by the School of Engineering.
Q: Some students have a major that is in the School of Engineering and also a major that is not. Which grade point average criteria do they need to meet?  
A: If they meet either set of criteria they will receive the honor.
Q: Where can students see honors?  
A: Dean's Honors appear quarterly on the official and unofficial transcripts. They can also be seen in the Student Advising Summary on the Statistics tab. They appear in parentheses next to the academic standing. University Honors will appear on the official and unofficial transcripts and on the diploma. They can also be seen in the Student Advising Summary on the Degree tab, in the Degrees Awarded section.
Q: When will Dean’s Honors be awarded?  
A: Dean’s Honors are awarded quarterly as grades are posted and criteria are met.
Q: When will University Honors be awarded?  
A: University Honors are awarded after the degree has been posted to the student’s transcript.
Q: Can honors be retracted after they have been posted?  
A: Yes. If a student’s record has changed so the student no longer meets the criteria for honors, the honors will be removed."
What kinds of honors are there?,"
A: Dean's Honors are awarded to students who meet specific criteria, which include: being in a degree-seeking program, having earned a minimum of 12 credits that quarter, having a qualifying term grade point average, and meeting the criteria for University Honors. University Honors are awarded to students who meet specific criteria, which include: being admitted in or after fall 2001, having completed 70 or more units at the University of California, and having a qualifying UC grade point average.","FAQs for Students: Honors
To acknowledge and reward student academic achievement, Dean’s Honors are awarded quarterly, and University Honors (Summa cum laude, Magna cum laude, and Cum laude) are awarded at graduation. These new honors designations are in addition to the long-standing College Honors and Department Honors awarded to students at graduation. These honors began being awarded fall 2008; they are not awarded retroactively. A description of the Undergraduate Honors Program can be found in the General Catalog's Evaluating Academic Performance section.
Q: What types of honors are awarded to UCSC students on a quarterly basis?  
A: Dean’s Honors are awarded to UCSC students on a quarterly basis.
Q: What types of honors are awarded to UCSC students at graduation?  
A: University Honors, College Honors, and Department Honors are awarded to UCSC students at graduation.
Q: What are Dean's Honors?  
A: Dean's Honors are awarded quarterly to undergraduates who:
were admitted in or after fall 2001;
are in a degree-seeking program;
if full-time, have earned a minimum of 12 credits that quarter, of which at least 10 are graded; or if on the University Part Time Program, have 10 or fewer grades credits; and
have a qualifying term grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are University Honors?  
A: University Honors are awarded to undergraduates receiving a degree, who:
were admitted in or after fall 2001;
have completed 70 or more units at the University of California; and
have a qualifying UC grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are qualifying grade point averages for Dean's Honors and University Honors?  
A: Please see the page Grade Point Average Criteria for Honors.
Q: How are the grade point average requirements for honors determined?  
A: They are based on the grade point averages of the top 2%, 5%, and 15% of recent graduates.
Q: When will the grade point average requirements be available for the coming year?  
A: Grade point average requirements should be available by the beginning of each fall quarter.
Q: Why are there different grade point average criteria for School of Engineering students than for other students?  
A: Honors eligibility for students whose degree program is in an established School shall be based on the GPA thresholds calculated for that School. Honors eligibility for other students shall be based upon the GPA thresholds calculated for students whose degree program is not part of an established School. (Academic Senate Regulation 11.4)
Q: Who is considered a School of Engineering student?  
A: For the purposes of receiving Dean’s Honors, any student who as of the last day in the quarter has a declared major offered by the School of Engineering. For the purposes of receiving University Honors, any student graduating in a major offered by the School of Engineering.
Q: Some students have a major that is in the School of Engineering and also a major that is not. Which grade point average criteria do they need to meet?  
A: If they meet either set of criteria they will receive the honor.
Q: Where can students see honors?  
A: Dean's Honors appear quarterly on the official and unofficial transcripts. They can also be seen in the Student Advising Summary on the Statistics tab. They appear in parentheses next to the academic standing. University Honors will appear on the official and unofficial transcripts and on the diploma. They can also be seen in the Student Advising Summary on the Degree tab, in the Degrees Awarded section.
Q: When will Dean’s Honors be awarded?  
A: Dean’s Honors are awarded quarterly as grades are posted and criteria are met.
Q: When will University Honors be awarded?  
A: University Honors are awarded after the degree has been posted to the student’s transcript.
Q: Can honors be retracted after they have been posted?  
A: Yes. If a student’s record has changed so the student no longer meets the criteria for honors, the honors will be removed."
Will I lose my honors if I fail a class?,"
A: No, failing a class will not automatically result in losing your honors. However, if you fail a class, your honors will be revoked.","FAQs for Students: Grading
Q: How do I change my grading option? 
A: Log into MyUCSC. Select the Enrollment tile.
Select Enrollment: Edit a Class. Select the correct term and Continue.
Choose the class you would like to change from the drop down menu. Select Proceed to Step 2 of 3.
If you are enrolled in a discussion section, your selected section will be displayed. Select Next.
In the Grading drop down menu, choose Graded or Pass/Not Pass (undergraduates) or Satisfactory/Unsatisfactory or ABC/NC Grading (graduates). Undergraduate students who are not in good academic standing will only have a Graded option. Select Next.
Review your selections and select Finish Editing if correct.
Q:What is the deadline? 
You can find the exact date on the Academic and Administrative Calendar. This is a hard deadline, and the Office of the Registrar cannot extend the deadline. If you feel you have extenuating circumstances, contact your college advising office for options.
Q: Can I convert my Pass/No Pass grades into letter grades for a GPA based on my performance? 
A: After the grade option deadline has passed, the Office of the Registrar cannot convert pass/no pass grades to letter grades, or to a GPA. If your ability to graduate may be affected by a pass/no pass grade option, contact your college advising office for more information.
Q: How do I request an Incomplete grade and what is the process? 
A: You can request an Incomplete from your instructor if your work so far is passing and the quarter has not ended. It is up to the instructor’s discretion to grant an Incomplete. Undergraduates have up to one quarter to submit the final coursework to the instructor. Graduate students are allowed up to one calendar year.
If the course work isn’t submitted by the deadline the Incomplete grade lapses to an F or NP, depending on the grading option in effect. If the coursework is submitted by the deadline and the instructor has not entered the final grade the Incomplete will automatically lapse to a non-passing grade. The instructor still has the ability to enter the final grade.
Your instructor may require you to fill out a Petition for Removal of Incomplete. This form must be submitted to the Office of the Registrar by the instructor/department, not the student. However, your instructor can change an Incomplete grade to the final grade in the system without using this form.
Q: How do I check my grades or GPA? 
A: Log into MyUCSC and select the My Academics tile.
For a specific quarter, select Grades and then the correct quarter. You can see your grades for that quarter as well as your term (quarter) GPA and cumulative (total) GPA.
To view all of your grades, select Course History."
What happens to my diploma if I change my name?,"
A: If you have graduated and have had a legal change of name, you may request a replacement diploma with the new legal name. You can complete a Diploma Re-Order Request Form, attach the original diploma and legal documentation noting the change of name, and a $60 check made payable to “UC Regents,” and send to the Office of the Registrar. Replacement diplomas can take anywhere from one to two months.","FAQs for Students: Degree Verification
Q: I need to show proof of my degree. What are my options?  
A: There are several ways to show proof of your degree:
Official Transcript
A transcript is an official copy of a student’s academic history at UCSC. A student’s degree is usually posted to their transcript within six weeks after the end of the quarter in which they applied to graduate. Order an official transcript from Transcripts Plus.
Unofficial Transcript
If you graduated from UC Santa Cruz since Fall 1986, you can access your unofficial transcripts on MyUCSC. To get a printed copy of the unofficial transcript, choose report type ""Printable Transcript"" when running the report. This produces a copy of the transcript as a printable PDF file that can also be saved.
If you do not know your log in information, contact the ITS HelpDesk at (831) 459-HELP.
Diploma
Diplomas are mailed to the active permanent address between two and three months after the quarter in which you graduated. Permanent address may be updated on MyUCSC. The name on your diploma will appear exactly as it does on university records.
Degree Verification
Online verification of degree completion can be accessed through the National Student Clearinghouse. Third parties may be directed to verify degrees through the National Student Clearinghouse.
Certificate of Completion
A Certificate of Completion is available to any student who has received a degree or has completed all requirements for the degree and has applied to graduate for a future quarter. Currently enrolled students or students who have not had all transcripts from other institutions evaluated are not eligible for a Certificate of Completion. Certificates of Completion indicate the student’s name, the date the degree was or will be awarded, the type of degree (B.A., B.S., etc), and the student’s major, college, and honors, if any. Use the Request for Certificate of Degree Requirements form.
Q: My diploma is missing/destroyed. How can I get a replacement?  
A: There is only one original diploma. To request a replacement diploma, notating the replacement, complete a Diploma Re-Order Request Form, attach a $75 check made payable to “UC Regents,” and send to the Office of the Registrar. Replacement diplomas can take anywhere from one to two months.
Q: I have changed my name since graduating from UC Santa Cruz. How do I get a diploma with my new name?  
A: If you have graduated and have had a legal change of name you may request a replacement diploma with the new legal name. Complete a Diploma Re-Order Request Form, attach the original diploma and legal documentation noting the change of name, and a $60 check made payable to “UC Regents,” and send to the Office of the Registrar.
Q: What is CeDiploma and how can I order one?  
A: CeDiplomas are certified electronic diplomas that are secured credentials that students may share electronically. Students who graduated spring 2022 and later will receive a complimentary CeDiploma. See the diplomas page for more information."
What is a transcript?,"
A transcript is a document that records a student's academic history, including all their courses, grades, and credits earned at UC Santa Cruz. It is a record of a student's academic performance, which can be used to verify their academic eligibility for admission, transfer credits, and other purposes. Transcripts are typically ordered online and are usually available for immediate download or mailing.","FAQs for Students: Transcripts
Q: How do I order transcripts?  
A: Transcripts are ordered online.
Q: How do I order a pdf of my official transcript?  
A: Order transcripts online. During the ordering process, if you choose Basic Service with No Evaluations, you will be given the option to send your official transcript electronically. You will need to provide the email address of the receiving party. You are also responsible for ensuring that the receiving party understands that they are responsible for downloading your transcript when they are notified that it is available. The downloading of your transcript PDF file will be done in a secure and encrypted manner.  Attachments/documents can be uploaded during the ordering process. If you attended UCSC before 1986 a pdf version of your official/unofficial transcript is not available. A paper copy of your official transcript will need to be mailed or picked up.
Q: When do current enrollments appear on the official transcript?  
A: For current students, your enrollments for the current term will first show on your official transcript the day after the Add/Drop/Swap deadline.
Q: How do I include a document (attachment) with my transcript order?  
A: During the ordering process you are asked if you have an attachment. Be sure to say yes.
Q: Why am I being asked for an authorization?  
A: For your protection, Parchment requires additional confirmation that the student is the person ordering the official transcript. The authorization is required when the information entered during the ordering process doesn't match what UC Santa Cruz has on record for the student.
Q: I don't remember my Student ID Number. How do I order a transcript?  
A: You can enter your Social Security Number instead. If you would rather enter your student ID number contact IT Services by entering a Slughub ticket or email help@ucsc.edu if you still have access to you UCSC email address. They will help you regain access to your student portal, where you can view your student ID number.
Q: How do I get unofficial transcripts?  
A: Log into MyUCSC and choose the Student Center tile. In the Academics section of Student Center, select Transcript: View Unofficial and click “Go.” Select either ""Printable Transcript (PDF)"" or ""Viewable Transcript (HTML)"" for the report type and click “view report.”  The PDF version of the unofficial transcript is best for printing or e-mailing.  The HTML version of the unofficial transcript is best for use with assistive software that requires document structure tags.
Q: Nothing happens when I click on the PDF version of my unofficial transcript. What do I do?  
A: Disable your browser's pop up blocker. When you are done viewing your transcript, you may re-enable your pop up blocker if desired.  Firefox For the Mac OS, go to Firefox menu > Preferences > Content. For Windows, go to Tools menu > Options > Content. Uncheck Block pop-up windows.  Safari Go to Safari menu > Preferences > Security tab and uncheck Block pop-up windows.  Internet Explorer Go to Tools menu > Internet Options > Privacy tab. Uncheck the Turn on Pop-Up Blocker box.  Google Chrome Go to Chrome menu > Settings. Click Show advanced settings... at the bottom of the page. Under Privacy, click the Content settings button. Under Pop-ups, select Allow all sites to show pop-ups.
Q: I never received the transcripts I ordered. What should I do?[show]
A: Verify the status of your transcript request online at Parchment's Order Status page.
Q: Are my community college credits transferable to UCSC?[show]
A: Contact the Office of Admissions, and request a Proposed Program Evaluation.
Q: Where do I send my college transcript to obtain transfer credit at UCSC?[show]
A: You will get a message on your Student Center under your To Do List. You may drop off an official copy of your transcript at the Registrar’s Office on the first floor of the Hahn Student Services building, or you may have your official transcript mailed to the following address:  UC Santa Cruz Admission’s Office 1156 High Street Santa Cruz, CA 95064.
Q: How will I know that my college transcript was received?[show]
A: Go to Student Center on MyUCSC, and you will get a message on your Student Center under your To Do List as soon as your transcript has been received. It can take up to eight weeks before your transfer credit will be posted. Once the credit has been posted, the To Do List message will change to say ""Transcript Reviewed.""
Q: Where can I see my transfer credit?[show]
A: Go to My Student Center on your student portal, and select Transfer Credit Report from the drop-down menu under Academics. The transcript can also be found on MyUCSC on the Academics>External Education page.
Q: What if my college transcript has not been reviewed yet, but I need the transfer credit as a prerequisite to enroll in another class?[show]
A: If you have a copy of your community college transcript, you can take it to your major department's advising office; in some cases they can help you to get in the class. If you are not yet proposed/declared in your major, you can take your transcript to the department of the major in which you intend to declare. If you are in a special exchange program that does not include a proposed major, then you can contact the department of the class directly.
Q: What if I did not receive the transfer credit I was expecting?[show]
A: You may fill out a Request for Review of Transfer Credit. Please specify exactly what you are looking for. The Admissions Office will then pull your transcript and review it again, to see if anything was missed initially."
Where will my diploma be mailed?,"
Q: ","FAQs for Students: Degree Verification
Q: I need to show proof of my degree. What are my options?  
A: There are several ways to show proof of your degree:
Official Transcript
A transcript is an official copy of a student’s academic history at UCSC. A student’s degree is usually posted to their transcript within six weeks after the end of the quarter in which they applied to graduate. Order an official transcript from Transcripts Plus.
Unofficial Transcript
If you graduated from UC Santa Cruz since Fall 1986, you can access your unofficial transcripts on MyUCSC. To get a printed copy of the unofficial transcript, choose report type ""Printable Transcript"" when running the report. This produces a copy of the transcript as a printable PDF file that can also be saved.
If you do not know your log in information, contact the ITS HelpDesk at (831) 459-HELP.
Diploma
Diplomas are mailed to the active permanent address between two and three months after the quarter in which you graduated. Permanent address may be updated on MyUCSC. The name on your diploma will appear exactly as it does on university records.
Degree Verification
Online verification of degree completion can be accessed through the National Student Clearinghouse. Third parties may be directed to verify degrees through the National Student Clearinghouse.
Certificate of Completion
A Certificate of Completion is available to any student who has received a degree or has completed all requirements for the degree and has applied to graduate for a future quarter. Currently enrolled students or students who have not had all transcripts from other institutions evaluated are not eligible for a Certificate of Completion. Certificates of Completion indicate the student’s name, the date the degree was or will be awarded, the type of degree (B.A., B.S., etc), and the student’s major, college, and honors, if any. Use the Request for Certificate of Degree Requirements form.
Q: My diploma is missing/destroyed. How can I get a replacement?  
A: There is only one original diploma. To request a replacement diploma, notating the replacement, complete a Diploma Re-Order Request Form, attach a $75 check made payable to “UC Regents,” and send to the Office of the Registrar. Replacement diplomas can take anywhere from one to two months.
Q: I have changed my name since graduating from UC Santa Cruz. How do I get a diploma with my new name?  
A: If you have graduated and have had a legal change of name you may request a replacement diploma with the new legal name. Complete a Diploma Re-Order Request Form, attach the original diploma and legal documentation noting the change of name, and a $60 check made payable to “UC Regents,” and send to the Office of the Registrar.
Q: What is CeDiploma and how can I order one?  
A: CeDiplomas are certified electronic diplomas that are secured credentials that students may share electronically. Students who graduated spring 2022 and later will receive a complimentary CeDiploma. See the diplomas page for more information."
What is a CeDiploma and how is it different from a regular diploma?,"
A: A CeDiploma is a diploma that is awarded to students who have completed all their required coursework and have met the degree requirements for their major, but have not yet completed the thesis or dissertation requirements. It is a ""soft"" diploma that is awarded to students who have completed their degree requirements, and is not tied to a specific thesis or dissertation completion date. Unlike a regular diploma, a CeDiploma is not typically awarded to students who have completed all their coursework and degree requirements. It is typically awarded to students who have completed their coursework and degree requirements, but are still in the process of completing their thesis or dissertation.","FAQs for Students: Honors
To acknowledge and reward student academic achievement, Dean’s Honors are awarded quarterly, and University Honors (Summa cum laude, Magna cum laude, and Cum laude) are awarded at graduation. These new honors designations are in addition to the long-standing College Honors and Department Honors awarded to students at graduation. These honors began being awarded fall 2008; they are not awarded retroactively. A description of the Undergraduate Honors Program can be found in the General Catalog's Evaluating Academic Performance section.
Q: What types of honors are awarded to UCSC students on a quarterly basis?  
A: Dean’s Honors are awarded to UCSC students on a quarterly basis.
Q: What types of honors are awarded to UCSC students at graduation?  
A: University Honors, College Honors, and Department Honors are awarded to UCSC students at graduation.
Q: What are Dean's Honors?  
A: Dean's Honors are awarded quarterly to undergraduates who:
were admitted in or after fall 2001;
are in a degree-seeking program;
if full-time, have earned a minimum of 12 credits that quarter, of which at least 10 are graded; or if on the University Part Time Program, have 10 or fewer grades credits; and
have a qualifying term grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are University Honors?  
A: University Honors are awarded to undergraduates receiving a degree, who:
were admitted in or after fall 2001;
have completed 70 or more units at the University of California; and
have a qualifying UC grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are qualifying grade point averages for Dean's Honors and University Honors?  
A: Please see the page Grade Point Average Criteria for Honors.
Q: How are the grade point average requirements for honors determined?  
A: They are based on the grade point averages of the top 2%, 5%, and 15% of recent graduates.
Q: When will the grade point average requirements be available for the coming year?  
A: Grade point average requirements should be available by the beginning of each fall quarter.
Q: Why are there different grade point average criteria for School of Engineering students than for other students?  
A: Honors eligibility for students whose degree program is in an established School shall be based on the GPA thresholds calculated for that School. Honors eligibility for other students shall be based upon the GPA thresholds calculated for students whose degree program is not part of an established School. (Academic Senate Regulation 11.4)
Q: Who is considered a School of Engineering student?  
A: For the purposes of receiving Dean’s Honors, any student who as of the last day in the quarter has a declared major offered by the School of Engineering. For the purposes of receiving University Honors, any student graduating in a major offered by the School of Engineering.
Q: Some students have a major that is in the School of Engineering and also a major that is not. Which grade point average criteria do they need to meet?  
A: If they meet either set of criteria they will receive the honor.
Q: Where can students see honors?  
A: Dean's Honors appear quarterly on the official and unofficial transcripts. They can also be seen in the Student Advising Summary on the Statistics tab. They appear in parentheses next to the academic standing. University Honors will appear on the official and unofficial transcripts and on the diploma. They can also be seen in the Student Advising Summary on the Degree tab, in the Degrees Awarded section.
Q: When will Dean’s Honors be awarded?  
A: Dean’s Honors are awarded quarterly as grades are posted and criteria are met.
Q: When will University Honors be awarded?  
A: University Honors are awarded after the degree has been posted to the student’s transcript.
Q: Can honors be retracted after they have been posted?  
A: Yes. If a student’s record has changed so the student no longer meets the criteria for honors, the honors will be removed."
How to get UCSHIP waiver?,"
To get a waiver of UC SHIP coverage, contact Academic HealthPlans at (855) 427-3167 or (855) 247-2273. You can also visit their website at AcademicHealthPlans.com. Fill out the IRS Form 1095-B, which is a required document for waiver requests. Once you submit the form, Academic HealthPlans will review your request and contact you to discuss the details. The waiver is typically granted within 30 days of receiving the form.","Enrolling in Coverage | UC Santa Cruz

What you need to know
You are automatically enrolled in UC SHIP coverage, unless you are approved to waive coverage. If you’re a non-registered student, you can enroll in UC SHIP only during voluntary enrollment periods that typically begin 30 days before — and end 30 days after — the start of the coverage period. Enrollment outside these periods will not be accepted, unless they’re the result of a qualifying life event.
When you can enroll
Enrollment criteria for a non-registered student
Enrolling as a non-registered student
Enrolling after a qualifying life event
Enrolling eligible dependents
Canceling UC SHIP coverage
When you can enroll
The UC SHIP periods of coverage align with the year’s academic terms.
Non-registered students can enroll in UC SHIP only during the voluntary enrollment period, which begins 30 days before the start of the coverage period and ends 30 days after the start date of the coverage period.
Example: If the winter coverage period begins Jan. 1, then enrollment for the winter term will start on Dec. 2 and end on Jan. 31.
You cannot enroll outside of the enrollment period, unless you have a qualifying life event.
Back to top
Enrollment criteria for a non-registered student
To enroll voluntarily, you must have been covered by UC SHIP in the term immediately preceding the term for which you want to purchase coverage.
Back to top
Enrolling as a non-registered student
As long as you meet the criteria above, you can enroll in UC SHIP as a non-registered student with filing fee status if any of the following circumstances apply to you:
You take a planned leave of absence. Students who take a planned leave approved by the University can purchase UC SHIP coverage on a voluntary basis for up to two quarters by submitting the appropriate enrollment form. Eligibility is contingent on prior quarter UC SHIP enrollment. Submit an enrollment form.
You are completing work under the support of the University of California but are not attending classes. You can purchase UC SHIP Filing Fee coverage on a voluntary basis for a maximum of one quarter. Submit an enrollment form.
You graduate. Read Extending coverage after graduating.
Back to top
Celebrating your 26th birthday this year?If you’re covered on a parent’s medical plan, you’ll no longer qualify for coverage when you turn 26. Learn how to enroll in UC SHIP below.
Enrolling after a qualifying life event
You and/or your eligible dependents can enroll in UC SHIP during the coverage period if you experience one of the following qualifying life events:
Involuntary loss of other health insurance coverage
Turn 26 and lose coverage as a dependent on a parent’s plan
Marriage or divorce
Birth or adoption of a child
Declaration or termination of a domestic partnership
When you have a qualifying life event during the coverage period, you must notify the insurance office on your campus within 31 days of losing your coverage and provide an official written letter of termination from your previous health insurance carrier.
You will be enrolled in UC SHIP as of the date of your involuntary loss of other coverage if you notify the insurance office within 31 days of losing your coverage. If you miss that window, your UC SHIP coverage will be effective on the date you pay the full UC SHIP premium.
To enroll yourself and your eligible dependents in UC SHIP after a qualifying life event, email Academic HealthPlans and provide proof of the qualifying life event.
To begin the enrollment process, complete the Qualifying Event (QE) form with the information needed.
Note: Premiums are not prorated for any enrollment occurring after the start of a coverage period, except premiums for newborns added as a dependent.
Back to top
Enrolling eligible dependents
For continued coverage, you must enroll your eligible dependents each term. You will need the following documentation to enroll them in UC SHIP:
For spouse, a marriage certificate
For same-sex/opposite-sex domestic partner, a Declaration of Domestic Partnership issued by the State of California or another country or state jurisdiction
For natural child, a birth certificate showing you’re the parent of the child
For stepchild, a birth certificate, and a marriage certificate showing that one of the parents listed on the birth certificate is married to you
For adopted or foster child, documentation from the placement agency showing that you have the legal right to control the child’s health care
For child eligible by court order, provide court documents that mandate the child will be covered under the insurance plan of the noncustodial parent
For voluntary and dependent enrollment in UC SHIP, complete the enrollment form.
Back to top
Canceling UC SHIP coverage
If you’re already enrolled in a health plan that meets the university health coverage requirements, you can apply to waive enrollment in UC SHIP. For more information about waiving coverage, visit the Waiving coverage webpage.
Back to top
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
What is the phone number for UCSHIP information?,"
(831) 459-2211 is the main phone number for the Student Health Center, which provides care on campus. The main number for counseling and psychological services (CAPS) is (831) 459-2628. The main number for Academic Health Plans is (855) 427-3167.","Contacts | UC Santa Cruz

Contacts
Student Health Center (SHC)
Your first stop for care on campusMain number: (831) 459-2211Appointments: (831) 459-2500Counseling and psychological services (CAPS): (831) 459-2628SHC websiteSHC email1156 High Street (across from Colleges 9 and 10)
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email
Anthem Blue Cross network
UC SHIP customer service(866) 940-8306
Anthem Blue View Vision
Vision plan benefits(866) 940-8306
Blue Cross Blue Shield Global Core
International medical assistance services and access to providers, hospitals and other health care professionals in nearly 200 countries(804) 673-1177Global Core website
BlueCard program
Nationwide access to providers and facilities(800) 810-2583
Delta Dental customer service
Dental plan benefits(800) 765-6003
LiveHealth Online
Urgent virtual medical and mental health care
LiveHealth Online website
LiveHealth Online mental health website
Optum Rx
Prescription drug plan benefits(844) 265-1879"
When is the add/drop deadline for master program in spring 2025?,"
Answer: The add/drop deadline for the master program in spring 2025 is March 1.","Program Degree Track Deadline
Anthropology Ph.D. December 24
Applied Economics & Finance M.S. February 27, April 16
Applied Mathematics M.S. January 30
Applied Mathematics Ph.D. January 6
Art: Environmental Art & Social Practice M.F.A. January 30
Astronomy & Astrophysics Ph.D. December 12
Biomolecular Engineering & Bioinformatics M.S. January 9
Chemistry M.S./Ph.D. December 31
Coastal Science & Policy M.S. December 13
Computational Media M.S./Ph.D. January 9
Computer Science & Engineering M.S. January 24
Computer Science & Engineering Ph.D. December 20
Digital Arts and New Media MF.A. Not accepting applications for 2025
Earth & Planetary Sciences M.S./Ph.D. January 6
Ecology & Evolutionary Biology M.A./Ph.D. December 10
Economics Ph.D. December 12
Education: Teaching (credential program) M.A. February 32
Education Ph.D. December 3
Electrical & Computer Engineering M.S. February 6
Electrical & Computer Engineering Ph.D. January 9
Environmental Studies Ph.D. December 12
Feminist Studies Ph.D. Not accepting applications for 2025
Film & Digital Media Ph.D. December 12
Games & Playable Media M.S. Not accepting applications for 2025
GIS Spatial Tech Application & Research M.A. January 15
History M.A. Not accepting applications for 2025
History Ph.D. December 2
History of Consciousness Ph.D. December 16
Human Computer Interaction M.S. February 3
Latin American & Latino Studies Ph.D. December 9
Linguistics Ph.D./M.A. December 12
Literature M.A./Ph.D. December 2
Materials Science & Engineering M.S. March 20
Materials Science & Engineering Ph.D. January 10
Mathematics M.A./Ph.D. January 2
Microbiology & Environmental Toxicology M.S. March 3
Microbiology & Environmental Toxicology Ph.D. December 3
Molecular, Cell & Developmental Biology M.S. April 1
Music M.A./D.M.A/Ph.D. December 16
Natural Language Processing M.S. February 3, April 16 
Ocean Sciences M.S./Ph.D. December 16
Philosophy M.A./Ph.D. January 7
Physics M.S./Ph.D. January 3
Politics Ph.D. December 16
Biomolecular Engineering & Bioinformatics Ph.D. December 3
Chemical Biology, Biochemistry & Biophysics Ph.D. December 3
Molecular, Cell & Developmental Biology Ph.D. December 3
Microbiology and Environmental Toxicology Ph.D. December 3
Psychology (social, cognitive, developmental) Ph.D. December 2
Science Communication: Science Writing M.S. January 27, April 3
Scientific Computing and Applied Mathematics M.S. January 30
Social Documentation M.F.A. January 9
Sociology Ph.D. December 2
Statistical Science M.S./Ph.D. December 16
Theater Arts M.A. February 3
Visual Studies Ph.D. December 12"
Can you list the final week schedule for winter 2025?,"
The winter quarter 2025 schedule is as follows: 
Monday, January 20, 2025 through Tuesday, January 28, 2025. The following days are holidays: Monday, January 20, 2025, Monday, January 27, 2025, and Tuesday, January 28, 2025. The final week schedule is as follows: Wednesday, January 22, 2025 through Friday, January 24, 2025. The following days are holidays: Wednesday, January 22, 2025, Thursday, January 23, 2025, and Friday, January 24, 2025. The following days are instruction days: Monday, January 20, 2025, Monday, January 27, 2025, Wednesday, January 22, 2025, and Thursday, January 23, 2025. The following days are final exams week: Monday, January 20, 2025, Monday, January 27, 2025, Tuesday, January 22, 2025, and Wednesday, January 23, 2025. The following days are grades due: Wednesday, January 22, 2025, Thursday, January 23, 2025, and Friday, January 24, 2025. The following days are summer quarter 2025: Monday, June 19, 2025, and Thursday, June 23, 2025. The following days are days of instruction for summer quarter 2025: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are holidays: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are final exams week: Monday, June 19, 2025, Monday, June 23, 2025, Tuesday, June 20, 2025, and Wednesday, June 22, 2025. The following days are grades due: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are summer quarter 2025: Monday, June 19, 2025, and Thursday, June 23, 2025. The following days are days of instruction for summer quarter 2025: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are holidays: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are final exams week: Monday, June 19, 2025, Monday, June 23, 2025, Tuesday, June 20, 2025, and Wednesday, June 22, 2025. The following days are grades due: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are summer quarter 2025: Monday, June 19, 2025, and Thursday, June 23, 2025. The following days are days of instruction for summer quarter 2025: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are holidays: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are final exams week: Monday, June 19, 2025, Monday, June 23, 2025, Tuesday, June 20, 2025, and Wednesday, June 22, 2025. The following days are grades due: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are summer quarter 2025: Monday, June 19, 2025, and Thursday, June 23, 2025. The following days are days of instruction for summer quarter 2025: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are holidays: Monday, June 19, 2025, Monday, June 23, 2025, and Thursday, June 23, 2025. The following days are final exams week: Monday, June 19, 2025,","Key Dates 2024-25
Fall Quarter 2024
Quarter Begins
September 21 Saturday
Instruction Begins
September 26 Thursday
Holidays
November 11, 28, 29 Monday, Thursday, Friday
Instruction Ends
December 6 Friday
Final Exams
December 9–13 Monday–Friday
Quarter Ends
December 13 Friday
Grades Due
December 18Wednesday
Holidays (not falling within quarter)
September 2, December 24, 25, 31, January 1 Monday, Tuesday, Wednesday, Tuesday, Wednesday
Campus closure
Monday, Dec. 23, 2024 through Wednesday, Jan. 1, 2025
Days of Instruction
49
Winter Quarter 2025
Quarter Begins
January 3 Friday
Instruction Begins
January 6 Monday
Holidays
January 20, February 17 Monday, Monday
Instruction Ends
March 14 Friday
Final Exams
March 17–21 Monday–Friday
Grades Due
March 25 Tuesday
Quarter Ends
March 21 Friday
Holiday (not falling within the quarter)
March 28 Friday
Days of Instruction
48
Spring Quarter 2025
Quarter Begins
March 31 Monday
Instruction Begins
March 31 Monday
Holiday
May 26 Monday
Instruction Ends
June 6 Friday
Final Exams
June 9–12 Monday–Thursday
Grades Due
June 17 Tuesday
Quarter Ends
June 12 Thursday
Days of Instruction
49
Commencements
June 13–16 Friday–Monday
Total Days of Instruction
146
Summer Quarter 2025
Juneteenth Holiday (no classes)
June 19 Thursday
Quarter Begins
June 23 Monday
Instruction Begins
June 23 Monday
Holiday
July 4 Friday
Instruction Ends
August 29 Friday
Quarter Ends
August 29 Friday
Grades Due
Varies"
What is the start date for the summar 2025 quarter?,"
The start date for the 2025 quarter is September 21.","Key Dates 2024-25
Fall Quarter 2024
Quarter Begins
September 21 Saturday
Instruction Begins
September 26 Thursday
Holidays
November 11, 28, 29 Monday, Thursday, Friday
Instruction Ends
December 6 Friday
Final Exams
December 9–13 Monday–Friday
Quarter Ends
December 13 Friday
Grades Due
December 18Wednesday
Holidays (not falling within quarter)
September 2, December 24, 25, 31, January 1 Monday, Tuesday, Wednesday, Tuesday, Wednesday
Campus closure
Monday, Dec. 23, 2024 through Wednesday, Jan. 1, 2025
Days of Instruction
49
Winter Quarter 2025
Quarter Begins
January 3 Friday
Instruction Begins
January 6 Monday
Holidays
January 20, February 17 Monday, Monday
Instruction Ends
March 14 Friday
Final Exams
March 17–21 Monday–Friday
Grades Due
March 25 Tuesday
Quarter Ends
March 21 Friday
Holiday (not falling within the quarter)
March 28 Friday
Days of Instruction
48
Spring Quarter 2025
Quarter Begins
March 31 Monday
Instruction Begins
March 31 Monday
Holiday
May 26 Monday
Instruction Ends
June 6 Friday
Final Exams
June 9–12 Monday–Thursday
Grades Due
June 17 Tuesday
Quarter Ends
June 12 Thursday
Days of Instruction
49
Commencements
June 13–16 Friday–Monday
Total Days of Instruction
146
Summer Quarter 2025
Juneteenth Holiday (no classes)
June 19 Thursday
Quarter Begins
June 23 Monday
Instruction Begins
June 23 Monday
Holiday
July 4 Friday
Instruction Ends
August 29 Friday
Quarter Ends
August 29 Friday
Grades Due
Varies"
What are elective options for NLP MS program?,"
In addition to the standard curriculum, the NLP MS program offers a variety of elective options that allow students to explore their interests and gain a deeper understanding of the field of natural language processing. Some examples of elective options include:

* Specialized courses in areas such as computer vision, machine learning, and human-computer interaction
* Research seminars on topics such as language models, dialogue systems, and sentiment analysis
* Electives in linguistics, cognitive science, and neuroscience
* Electives in computer science, mathematics, and statistics
* Electives in artificial intelligence, robotics, and machine learning
* Electives in human-computer interaction, user experience, and design thinking
* Electives in data science, machine learning, and deep learning
* Electives in cognitive science, neuroscience, and psychology

These elective options provide students with the opportunity to delve deeper into specific areas of interest and gain a more nuanced understanding of the field of natural language processing. They also allow students to explore new research topics and develop their critical thinking and problem-solving skills.","Apply | Natural Language Processing M.S.

APPLY TO THE NLP M.S. PROGRAM
APPLICATIONS CLOSE APRIL 16TH
The Natural Language Processing Program Offers a Unique Master of Science degree
Taught intensively over 15-18 months in state-of-the-art facilities in Santa Clara, in the heart of Silicon Valley, our master’s degree program is the product of close collaboration between our distinguished academic team and industry leaders. Our faculty team and Industry Advisory Board are made up of leading scientists and practitioners in the information processing and NLP fields, and our students benefit in a variety of ways from this industry collaboration.
Learning from world-class faculty and practitioners who are leaders in the field means you can be confident your syllabus will give you the real-world knowledge and skills needed for a successful career.
We’re pleased that you’ve decided to take an important step toward advancing your professional expertise and are considering applying to UCSC’s NLP MS Program. Review this page for guidance on the application and admission requirements for this program.
Admission requirements
Visit our Admissions page to review essential admission requirements and tips for applying to the program.
Application timeline
Applications for UCSC graduate programs open each year on October 1st, but deadlines vary by program and are subject to change. View the NLP Program’s current application deadline on the Graduate Admissions website.
Once the application opens, visit UCSC’s Graduate Application Page to begin applying.
Required application materials
1. Transcripts
Unofficial transcripts are sufficient for application purposes, but official transcripts may be required if admitted to the program.
2. Letters of Recommendation
Three letters of recommendation are required. Letters should be prepared by professors and professional contacts who may best speak to your abilities and academic potential in your chosen field of study.
3. Résumé or CV (can also provide a link to your own website)
4. A Personal History Statement
Recommended length is a concise 1-3 pages, single-spaced.
UC Santa Cruz is interested in a diverse and inclusive graduate student population. In the essay, discuss how your personal background informs your decision to pursue a graduate degree. Include any educational, familial, cultural, economic, or social experiences, challenges, or opportunities relevant to your academic journey; how you might contribute to social or cultural diversity within your chosen field; and/or how you might serve educationally underrepresented segments of society with your degree.
This statement will be used in conjunction with your application for graduate admission and financial support. Note that the Personal History Statement should not duplicate the Statement of Purpose.
5. A Statement of Purpose
Recommended length is a concise 2-4 pages, single-spaced.
Please describe your plans for graduate study or research and for your future occupation or profession.
Include any information that may aid the selection committee in evaluating your preparation and qualifications for graduate study at the University of California, Santa Cruz.
6. Test score requirements may apply
Review TOEFL/IELTS test score requirements for applicants whose native language is not English.
Visit the Graduate Admissions’ FAQs page for guidance on requesting a waiver of this requirement.
The GRE requirement has been waived for admission consideration for NLP MS applications. You are welcome to submit your official scores, but please note the inclusion of your official GRE score will not be a deciding factor for admission.
7. Statement of Financial Need (optional)
This optional Statement of Financial Need will help the NLP Program determine your eligibility for fellowship support for your graduate education at UC Santa Cruz. Submitting a Statement of Financial Need does not guarantee funding.
The Statement of Financial Need should not exceed one page and should fully explain, in your own words, your need for financial assistance.
Please also address personal resources and other fellowships or grant opportunities that you will pursue to help cover your graduate education expenses.
Master’s students in Baskin School of Engineering programs are typically self-supported. While a small number of partial fellowships may be extended to eligible NLP students each year, the NLP program encourages students to seek external support for covering graduate education expenses.
8. Application Fee Waiver Request Form (optional)
Criteria:There are two possible ways to get a fee waiver:
If you are a US citizen, AB-540, or a US Permanent Resident, you may be eligible for the Graduate Admissions’ Fee Waiver for financial hardship or for students currently receiving financial aid, or for a Fee exemption for applicants who participate(d) in a qualifying program. Please see their website for more information.
If you do not meet the criteria above, you may request an application fee waiver directly from the NLP program if you meet ONE of the following criteria:
Current undergraduates and alumni of UC Santa Cruz
Current undergraduates and alumni of any University of California, California State University, or private colleges of universities located in California, and alumni of community colleges located in California.
Please note that fee waivers are limited and are distributed based on students’ level of need.

Before you submit your application: Please fill out Application Fee Waiver Request Form for NLP M.S. 2024-2025 with your application details and be sure to include a brief statement regarding your financial need for the application fee exemption. Be sure to include a brief statement regarding your financial need, which you can copy from the Statement of Financial Need in your application if you included one there. To ensure sufficient time to process your fee waiver request, your fee exemption request must be received at least 2 weeks before the February 3rd application deadline  (Fee Waiver Deadline: January 20th 5PM PT).

After you apply for the fee waiver:
To be eligible for a fee waiver, you must submit the fee waiver request form by January 20th at 5 PM PT, and you must submit your complete application to the NLP program by February 3rd at 11:59PM PT. Incomplete submissions for either of these requirements will disqualify you from being considered for a fee waiver.
On the “Fee Waiver/Payment” page of the application, please choose “no” to the question, “Do you believe you will need assistance paying for your application processing fee?” Then select “Yes” to “Has the program you are applying to agreed to pay for the application processing fee?” The page will tell you a credit card payment is required. Do not worry about this, and do not pay it; we will pay for it on your behalf. Please submit your application as soon as it is ready.
After you have submitted your application, you will be directed to the status portal. There will be a pay fee link, do not pay this fee. You will not be refunded, and a reimbursement process is more involved and takes time to process.

Processing Time: We will review your request within 5 business days and send you a confirmation email to inform you whether the fee waiver has been approved. If you haven’t received any updates after this period, please don’t hesitate to reach out to us at nlp@ucsc.edu.
Get support with your application
For general information:
Check out the NLP FAQs page for support with general inquiries about the program and application process.
Visit the UCSC Graduate Admissions website to review application instructions, deadlines, test score requirements, and detailed guidance about applying to a UCSC graduate program.
Additional guidance about applying to Baskin Engineering graduate programs can be found on Baskin’s Office of Graduate Student Affairs website.
Have questions? We’re here to help
If you have questions about the application requirements or the admissions process, you can email the NLP Support Team (nlp@ucsc.edu) and let us know if you’d prefer to discuss your questions in a Zoom meeting with a member of our team. You can also review insights and tips from NLP students on Reddit.
Watch a recorded virtual information session
Learn more about the unique benefits of the program by watching a recording of one of our virtual information sessions:
Support for parents and families of prospective students
As you consider your options for a master’s degree program, you may also want to include your support network in your decision-making process. We’ve put together a guide for parents and family members to provide insight into life at UCSC, as well as the support and resources available to our graduate students. Please always feel welcome to contact our Support Team if you or your family members have any questions.
"
"If I missed tuition deadline, am I unenrolled automatically?","
A: If you missed the tuition deadline, you will not be automatically unenrolled. You can still apply to re-enroll for the next quarter by submitting a Withdrawal/Leave of Absence (LOA) petition at your college office.","FAQs for Students: Withdrawal
Q: I’m a currently enrolled student, but I am unable to complete the quarter. How do I withdraw from the university?  
A: For fall, winter, and spring quarters, you must file a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return.
Over the Summer Session, students must manually drop or withdraw from their courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or other emergency reasons. Contact an advisor at your college if you would like to pursue this option.
Q: If I need to withdraw during the quarter, what is the refund policy?  
A: See the schedule of refunds policy in The Navigator. For specific dates, see the Fee Refund Schedules. Over the summer, tuition is only reversed before the drop deadline.
Q: How does the college determine the effective date of withdrawal?  
A: As currently defined by UCOP and in accordance with federal financial aid policy, the effective date for determining a refund of fees or the adjustment of financial aid is the date the student notifies the college of his or her intent to withdraw.
Q: What is the withdrawal petition deadline in the Academic and Administrative Calendar?  
A: After the third week of the quarter you may Petition to Withdraw from a Course. You will receive a W grade if submitted before the 6th-week deadline, and a Not Pass (NP) grade if submitted between the sixth and ninth weeks of instruction. Dates and deadlines are listed in the Academic and Administrative Calendar. If there is a medical or emergency reason to withdraw from the course after the sixth week, students should discuss their options with the college advising office. Documentation is required for an emergency withdrawal from a course.
Information on requesting a W grade in summer classes is available here.
Q: What is the difference between withdrawing from the university and taking W grades in all my classes?  
A: Withdrawing from the university is a full withdraw from UCSC, and tuition may be refunded based on the Refund Schedule. After the sixth week of the quarter, an undergraduate may withdraw only for medical/emergency reasons.  A Withdraw (W) or Not Pass (NP) grade is a grade posted to a specific course or courses. You do not receive credit for the course and the W/NP grades do not affect your GPA. W grades have an impact on academic progress. There may also be financial consequences for students receiving financial aid or Veteran’s benefits if they withdraw from UCSC or request W grades. The period for requesting a W/NP grade otherwise extends until the deadline published in the Academic and Administrative Calendar.
Q: Is there a financial consequence for taking all W grades?  
A: Opting for all W grades may result in a reduction in financial aid. Also, students who receive benefits from the Department of Veterans Affairs will have a reduction in quarterly benefits when the W grades result in less than 12 credits completed. Over the summer session, there are no tuition or campus fee refunds for taking a W grade.
Q: What is the Withdrawal Without Penalty deadline on My Class Schedule?  
A: It's an administrative date that indicates that if a student withdraws from the University on or before the last day of the term, no classes appear on the transcript for the term.
Q: Where can I read more about the withdrawal process and W grades?  
A: For more information about withdrawing from the University, see Withdrawal from the University. For W grades in fall, winter, and spring quarters, see Withdrawal from a Course. For W grades in summer, see more information here."
What is the grading guideline for master program?,"
A) The program is designed to prepare students for a career in the field of Computer Science and Engineering.
B) The program is designed to prepare students for a career in Environmental Studies.
C) The program is designed to prepare students for a career in the field of Computer Science and Engineering.
D) The program is designed to prepare students for a career in the field of Environmental Studies.
E) The program is designed to prepare students for a career in the field of Computer Science and Engineering.

Correct answer: C) The program is designed to prepare students for a career in the field of Computer Science and Engineering.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
What are the benifit of becoming an honored student?,"
A: Becoming an honored student has several benefits, including increased job prospects, higher earning potential, and improved academic reputation. Additionally, it is a great way to demonstrate academic excellence to employers and graduate schools, and it can also be a valuable asset in the job market.","FAQs for Students: Honors
To acknowledge and reward student academic achievement, Dean’s Honors are awarded quarterly, and University Honors (Summa cum laude, Magna cum laude, and Cum laude) are awarded at graduation. These new honors designations are in addition to the long-standing College Honors and Department Honors awarded to students at graduation. These honors began being awarded fall 2008; they are not awarded retroactively. A description of the Undergraduate Honors Program can be found in the General Catalog's Evaluating Academic Performance section.
Q: What types of honors are awarded to UCSC students on a quarterly basis?  
A: Dean’s Honors are awarded to UCSC students on a quarterly basis.
Q: What types of honors are awarded to UCSC students at graduation?  
A: University Honors, College Honors, and Department Honors are awarded to UCSC students at graduation.
Q: What are Dean's Honors?  
A: Dean's Honors are awarded quarterly to undergraduates who:
were admitted in or after fall 2001;
are in a degree-seeking program;
if full-time, have earned a minimum of 12 credits that quarter, of which at least 10 are graded; or if on the University Part Time Program, have 10 or fewer grades credits; and
have a qualifying term grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are University Honors?  
A: University Honors are awarded to undergraduates receiving a degree, who:
were admitted in or after fall 2001;
have completed 70 or more units at the University of California; and
have a qualifying UC grade point average.
Students who meet these criteria may be ineligible for honors if they have disciplinary sanctions for a violation of academic integrity.
Q: What are qualifying grade point averages for Dean's Honors and University Honors?  
A: Please see the page Grade Point Average Criteria for Honors.
Q: How are the grade point average requirements for honors determined?  
A: They are based on the grade point averages of the top 2%, 5%, and 15% of recent graduates.
Q: When will the grade point average requirements be available for the coming year?  
A: Grade point average requirements should be available by the beginning of each fall quarter.
Q: Why are there different grade point average criteria for School of Engineering students than for other students?  
A: Honors eligibility for students whose degree program is in an established School shall be based on the GPA thresholds calculated for that School. Honors eligibility for other students shall be based upon the GPA thresholds calculated for students whose degree program is not part of an established School. (Academic Senate Regulation 11.4)
Q: Who is considered a School of Engineering student?  
A: For the purposes of receiving Dean’s Honors, any student who as of the last day in the quarter has a declared major offered by the School of Engineering. For the purposes of receiving University Honors, any student graduating in a major offered by the School of Engineering.
Q: Some students have a major that is in the School of Engineering and also a major that is not. Which grade point average criteria do they need to meet?  
A: If they meet either set of criteria they will receive the honor.
Q: Where can students see honors?  
A: Dean's Honors appear quarterly on the official and unofficial transcripts. They can also be seen in the Student Advising Summary on the Statistics tab. They appear in parentheses next to the academic standing. University Honors will appear on the official and unofficial transcripts and on the diploma. They can also be seen in the Student Advising Summary on the Degree tab, in the Degrees Awarded section.
Q: When will Dean’s Honors be awarded?  
A: Dean’s Honors are awarded quarterly as grades are posted and criteria are met.
Q: When will University Honors be awarded?  
A: University Honors are awarded after the degree has been posted to the student’s transcript.
Q: Can honors be retracted after they have been posted?  
A: Yes. If a student’s record has changed so the student no longer meets the criteria for honors, the honors will be removed."
How do I know I am on a waitlist or not during course enrollment?,"
A: You can see your waitlist appointment by selecting ""Details"" under ""Enrollment Dates"" in your MyUCSC. The waitlist position number may not indicate the order in which you will be enrolled in the class. Attend the first class meeting to determine if you will be able to enroll. If you are still on the waitlist at the start of instruction, attend the first class meeting to see if you will be able to enroll. You may be dropped for not attending the first class meeting.","FAQs for Students: Waitlist
In fall, winter, and spring quarters during their waitlist appointment students may begin adding up to 14 waitlist credits. Students can see their waitlist appointment by selecting “details” under “Enrollment Dates” in their MyUCSC.
The waitlist runs every half-hour during normal business hours through the seventh day of instruction; your waitlist position number may not indicate the order in which you will be enrolled in the class. For example, your waitlist position will be skipped if you have scheduled the course with a time conflict or have attempted to enroll in more than your allotted credits. If you are still on the waitlist at the start of instruction, attend the first class meeting to determine if you will be able to enroll. You may be dropped for not attending the first class meeting. Some things to consider:
Waitlisting does not guarantee enrollment;
Waitlisting does not guarantee more seats will open in a class;
Error messages do not indicate that space is being held open in a class or that enrolment is guaranteed.
Following the 7th day of instruction, the wait list is no longer an option and permission numbers are required to enroll (see the Academic and Administrative Calendar). Instructors and/or departments issue permission numbers. In summer, waitlists run every half-hour, 24 hours a day until the add deadline. Check your portal to ensure you know if you were enrolled from the waitlist. Attend class if possible.
Q: How do I waitlist a course?  
A: 1. Sign in to MyUCSC.
2. Select the Enrollment or Student Center tile.
3. Select the add tab at the top of page. Select the correct quarter and type in the five-digit course number of the course you would like to waitlist.
4. Check the “Waitlist if class is full” box and select Next.
5. If you would like to drop a course that you are currently enrolled in if you become enrolled in the wait-listed course you can indicate this in the “If Enrolled Drop This Class” drop down menu. You may only select a class as “Drop if Enrolled” once. If you need to use a permission number, you cannot also use ""Drop if Enrolled.” The waitlist process will error and you will not be enrolled.
6. Review your selection and select Proceed to Step 2 of 3.
7. To complete the enrollment select on Finish Enrolling. A checkmark under Status means that your enrollment was successful.
Q: How do I drop a wait-listed course?  
A: Follow these steps:
Sign in to MyUCSC. Select the Enrollment or Student Center tile.
Select the drop tab at the top of the page. Select the correct quarter and check the box next to the course you wish to remove from your waitlist.
Select the secondary section that you would like to change or just select next for no changes to the secondary section.
Select the Drop Selected Classes box.
Confirm the selection and select Finish Dropping. A checkmark under Status means that your drop was successful.

Q: If I get an error message, how do I edit a wait-listed course?[show]
A: If you are unable to successfully enroll in a wait-listed course because of too many credits or a time conflict, you can edit your waitlist.
Sign in to MyUCSC. Select the Enrollment or Student Center tile.
Select the edit tab on the top of the page. Select the correct quarter and choose the course you would like to edit. Select Proceed to Step 2 of 3.
Select the secondary section that you would like to change or just select next for no changes to the secondary section.
Indicate the course you would like to drop if you are enrolled in your wait-listed course in the “If Enrolled Drop This Class” drop down menu. Select Next.
Review your selection and select Finish Editing to process your edit request. A checkmark under Status means that your edit was successful. You can return to the edit tab at any time to review the setup of your wait-listed course.
If you preemptively drop the class that you have specified as your Drop if Enrolled class, you will remain on the waitlist, but without a Drop if Enrolled class. You may not get into the wait-listed class if your enrollment will exceed your credit limit. If you change the section for your Drop If Enrolled option, that class will no longer be listed as your Drop If Enrolled class, because your enrollment setup has been changed. In this case, you may edit your waitlist enrollment to specify a new Drop if Enrolled class.
Editing your waitlist does not guarantee enrollment.
Instructions how to edit your waitlist are also available here.
Q: How do I view my position on the waitlist?[show]
A: Follow these steps:
Sign in to MyUCSC. Select the Student Center tile.
Under the Academics heading on the top left of the page is a drop-down menu that says “more links…” Select Class Schedule from that menu and select the Go button.
Your wait-listed classes will be shown on the Class Schedule page, and your waitlist position is shown in the Waitlist Position column.
It is important to make sure you do not have any time conflicts or too many credits. If so, the process will fail to enroll you and move on to the next person on the waitlist.
The waitlist system may try to enroll you even if you are not number one on the waitlist due to scheduling issues for preceding waitlisted students.
Q: How do I change my section for a wait-listed course?  
A: You may change your waitlist secondary section, by using the EDIT option. See the FAQ above ""If I get an error message, how do I edit a wait-listed course?""
Q: What is the $10 Waitlist Correction Fee and how do I avoid it?  
A: Starting winter 2019 quarter, the campus implemented a $10 Waitlist Correction Fee for assistance from our office with switching waitlisted course options in fall, winter, and spring quarters. The fee does not apply to manual actions taken by you, and does not apply to the first correction (the fee will be automatically waived for the first call), but it will be applied for the second correction and any thereafter.
You can avoid the fee by being strategic in your enrollment.
Enroll in a backup class first.
Enroll in the wait-listed course, checking the ""Waitlist if class is full"" box. Indicate in the ""If Enrolled Drop This Class"" drop down menu which back up course you would drop if enrolled in the wait-listed course.
Check to make sure you do not have any time conflicts with the wait-listed course.
Some co-requisite labs require a two-step waitlist process. Sign up on the waitlist for the lab. Once enrolled in the lab, you will then be notified to enroll in the co-requisite class.
Q: Can I waitlist two offerings of the same course?  
A: You can. However, once you are enrolled in one offering of the course you will automatically be dropped from the wait list of the course that you did not get into. If you are already enrolled in one offering of a class and would like to wait list for a different offering, you must set up the wait-list enrollment as a swap so that you are not dropped from the wait-listed class. For example, if you are enrolled in Physics 6L-01 and you'd rather be in Physics 6L-02, you will need to set up the enrollment so that if you get into Physics 6L-02 you will be dropped from Physics 6L-01. Otherwise, the system will drop you off of the wait list from Physics 6L-02 and you will remain enrolled in Physics 6L-01.  Please be aware that if you change your section for the enrolled offering of the class, that class will no longer be listed as your ""drop if enrolled"" class, because the enrollment has changed, and you will be automatically dropped from the waitlisted class.
Q: What do the waitlist error messages mean and how do I resolve the error?  
A: See information on waitlist error messages here.
Pro Tip: When choosing a course which has multiple enrollment options (e.g., MATH 19A-01 and MATH 19A-02) or has required secondary discussion sections or labs, make sure to ""View All"" to see all options. Sign up for all classes that work in your schedule. Also, when signing up for a class that has a secondary discussion section or lab, add the secondary discussion section or lab that fits in your schedule with lowest waitlist enrollment. This will improve your chances of getting in when a spot opens up one of the courses.
If the prior term’s grades are still not finalized by the time of enrollment for the upcoming term, there is a conditional enrollment that will be rechecked once the upcoming term starts and you will either remain in the course or be dropped depending on the grades of the prerequisite.   Please also note that students can waitlist for a course regardless of meeting prerequisites, though when the waitlist runs the prerequisites are checked and an enrollment is either processed or an error occurs."
"If I lost my diploma, how do I get a new one?","
A: If you have lost your diploma, you can request a replacement through the Office of the Registrar, notating the replacement, complete a Diploma Re-Order Request Form, attach a $75 check made payable to “UC Regents,” and send it to the Office of the Registrar. Replacement diplomas can take anywhere from one to two months.","FAQs for Students: Degree Verification
Q: I need to show proof of my degree. What are my options?  
A: There are several ways to show proof of your degree:
Official Transcript
A transcript is an official copy of a student’s academic history at UCSC. A student’s degree is usually posted to their transcript within six weeks after the end of the quarter in which they applied to graduate. Order an official transcript from Transcripts Plus.
Unofficial Transcript
If you graduated from UC Santa Cruz since Fall 1986, you can access your unofficial transcripts on MyUCSC. To get a printed copy of the unofficial transcript, choose report type ""Printable Transcript"" when running the report. This produces a copy of the transcript as a printable PDF file that can also be saved.
If you do not know your log in information, contact the ITS HelpDesk at (831) 459-HELP.
Diploma
Diplomas are mailed to the active permanent address between two and three months after the quarter in which you graduated. Permanent address may be updated on MyUCSC. The name on your diploma will appear exactly as it does on university records.
Degree Verification
Online verification of degree completion can be accessed through the National Student Clearinghouse. Third parties may be directed to verify degrees through the National Student Clearinghouse.
Certificate of Completion
A Certificate of Completion is available to any student who has received a degree or has completed all requirements for the degree and has applied to graduate for a future quarter. Currently enrolled students or students who have not had all transcripts from other institutions evaluated are not eligible for a Certificate of Completion. Certificates of Completion indicate the student’s name, the date the degree was or will be awarded, the type of degree (B.A., B.S., etc), and the student’s major, college, and honors, if any. Use the Request for Certificate of Degree Requirements form.
Q: My diploma is missing/destroyed. How can I get a replacement?  
A: There is only one original diploma. To request a replacement diploma, notating the replacement, complete a Diploma Re-Order Request Form, attach a $75 check made payable to “UC Regents,” and send to the Office of the Registrar. Replacement diplomas can take anywhere from one to two months.
Q: I have changed my name since graduating from UC Santa Cruz. How do I get a diploma with my new name?  
A: If you have graduated and have had a legal change of name you may request a replacement diploma with the new legal name. Complete a Diploma Re-Order Request Form, attach the original diploma and legal documentation noting the change of name, and a $60 check made payable to “UC Regents,” and send to the Office of the Registrar.
Q: What is CeDiploma and how can I order one?  
A: CeDiplomas are certified electronic diplomas that are secured credentials that students may share electronically. Students who graduated spring 2022 and later will receive a complimentary CeDiploma. See the diplomas page for more information."
"If I am a TA, does it hurt my academic standing if I receive bad evaluation from students?","
A: As a teaching assistant (TA), receiving a bad evaluation from students can negatively impact your academic standing, especially if the instructor is not satisfied with your teaching performance. It can also affect your professional development opportunities and your ability to advance in your career as a TA. It is essential to take constructive feedback seriously and use it to improve your teaching skills and knowledge.","FAQs for Students: Evaluations
Q: What is an evaluation? 
A: A performance evaluation is a description of the quality and characteristics of a student's performance in a course.
Q: How can I view my evaluations? 
A: Log on to MyUCSC, select the MyAcademics tile, and select Evaluations.
Q: I am missing an evaluation. What do I do? 
A: Effective fall 2009 for undergraduate students and fall 2010 for graduate students, evaluations are optional, and are completed at the discretion of the instructor. If you are missing a mandatory evaluation, contact the instructor.
Q: What if there is an error in one of my evaluations? 
A: If the error is a misspelling in your name or if you are referred to by the wrong gender, contact the Registrar's Office directly at nes@ucsc.edu. To correct any other error, contact the instructor and request a revised evaluation."
What are the additional fees beside tuition?,"
UC SHIP premiums are increasing for the 2024-25 academic year.

","Paying for Coverage | UC Santa Cruz

UC SHIP premiums are increasing for the 2024-25 academic year.
Learn more about the increases
If you’re a registered student, you are automatically enrolled in UC SHIP and charged a health insurance premium on your registration bill.
If you’re a non-registered student enrolling yourself and/or your eligible dependents for coverage, you must pay the full insurance premium upon enrollment.
Learn what you’ll pay for coverage.
Help paying for medical expenses
Providing affordable health care is a UC SHIP mandate. Even so, every student’s financial position is unique. If your out-of-pocket medical expenses are causing you financial hardship, these UCSC resources may be able to provide financial assistance:
Slug Support Program
Health Equity Fund
Charity care may also be an option for you. Reach out to the provider’s office and ask if they offer any form of charity care. Although some providers do not offer any sort of charity care, others do. They may provide an application for you to complete or ask you to write a letter. Providers who do offer charity care will review your request, and then they could offer to write off the full amount you owe, a portion of that amount, or they may determine you don’t meet their qualifications for charity care.


"
What are the graduation requirements?,"
Graduation requirements are not specified. However, students typically complete a bachelor's degree in their chosen field and meet the necessary requirements for their chosen field, such as coursework and academic performance.

Students who wish to pursue advanced degrees in their chosen field can do so by completing a master's degree in that field. Some fields, such as computer science and engineering, require a master's degree as a minimum qualification for admission to graduate programs.

Students who wish to pursue a Ph.D. in their chosen field can do so by completing a master's degree in that field first, followed by a Ph.D. program.

Students who wish to pursue a bachelor's degree in a field that requires a master's degree, such as computer science and engineering, typically require a bachelor's degree in that field first, followed by a master's degree in that field.

Students who wish to pursue a Ph.D. in a field that requires a bachelor's degree, such as biology, typically require a bachelor's degree in that field first, followed by a Ph.D. program.

Students who wish to pursue a bachelor's degree in a field that does not require a master's degree, such as anthropology, typically require a bachelor's degree in that field first, followed by a graduate degree in that field.

Students who wish to pursue a Ph.D. in a field that does not require a bachelor's degree, such as philosophy, typically require a bachelor's degree in that field first, followed by a Ph.D. program.

Students who wish to pursue a bachelor's degree in a field that requires a graduate degree, such as law, typically require a bachelor's degree in that field first, followed by a law degree.

Students who wish to pursue a Ph.D. in a field that requires a bachelor's degree, such as psychology, typically require a bachelor's degree in that field first, followed by a Ph.D. program.

Students who wish to pursue a bachelor's degree in a field that requires a master's degree, such as computer science and engineering, typically require a bachelor's degree in that field first, followed by a master's degree in that field.

Students who wish to pursue a Ph.D. in a field that requires a bachelor's degree, such as biology, typically require a bachelor's degree in that field first, followed by a Ph.D. program.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
Who have access to my student ID photo?,"
A: Faculty members and authorized staff with a business need to know can view your student ID photo.","Student ID Card Photos
Q: Are the student ID photos within MyUCSC considered public information?  
A: No. The student ID photo on the class roster is defined by UC Santa Cruz as confidential. This photo is intended to assist instructors and authorized staff in identifying students and is to be used for no further purpose.
Q: Who can see my student ID card photo?  
A: Student ID card photos can be seen by faculty using the faculty center class roster or by staff members with appropriate role-level security who have a business need to know.
Q: How do I replace my student ID card photo?  
A: Photos can be submitted through ID Card Services which is located on the second floor of the Bay Tree Bookstore building next to the Express Store.  Information regarding hours, phone, and additional services, may be found on the ID Card Services web site."
Is on-campus housing available at UCSC Silicon Valley campus?,"
Yes, UC Santa Cruz Silicon Valley campus offers housing options for students, including on-campus housing. Students can apply for on-campus housing through the UC Santa Cruz Housing website. On-campus housing is usually available on a first-come, first-served basis, but some housing options may be available for graduate students.

Note: This answer is concise, clear, and summarizes the information provided. It also addresses the question directly.","Overview | UC Santa Cruz

What you need to know
The University of California Student Health Insurance Plan (UC SHIP) provides high-quality, affordable and convenient coverage that protects students from unexpected health care costs. The plan is ACA-compliant and built especially for students. Students are actively involved in the plan’s design.
Because all UC students are required to have medical insurance, registered students are automatically enrolled in UC SHIP.
Your health and health needs come first — always.
This has been the UC SHIP guiding principle since its inception.
The UC SHIP difference
UC SHIP covers medical, pharmacy, dental, vision, and mental health and substance use services for UC undergraduate and graduate students and their eligible dependents.
More than just health insurance, UC SHIP is a dynamic, self-funded, pooled risk and purchasing program that’s tailored specifically for UC students. Unlike other plans, UC SHIP leverages the collective strength of students across UC campuses, ensuring they have access to the comprehensive care they need — on and off campus.
As a self-funded plan, UC SHIP is able to keep premiums low by avoiding the costs that fully insured plans incur, like paying dividends to stockholders and taxes on premiums. With students actively involved in its design, UC SHIP ensures that student health needs are met effectively and economically. This means that you can focus on your academic journey and not stress about the cost of taking care of your health.
Proof of insurance (IRS Form 1095-B)
Under the federal reporting requirement, health care plans and insurance companies must report member information to the Internal Revenue Service (IRS) that can be linked to a health plan that is certified minimum essential coverage. Member information includes the name, address and Social Security number — including covered students and dependents.
UC SHIP’s enrollment administrator, Academic HealthPlans (AHP), may contact you via email to get important information that is not on file with the registrar’s office. You can also email AHP to provide the requested information.
For proof of coverage, all UC SHIP members can electronically access their IRS Form 1095-B — the form that proves you have the minimum essential coverage required by the ACA. To access your 1095-B, follow the instructions in the 1095 User Guide [PDF].
How the plan works
Under the UC SHIP plan guidelines, the first place you should seek care is your campus student health center (SHC). Many services, including preventive care, are offered there at no cost or for a low copay. If you need care the SHC can’t provide, you’ll receive a referral for care outside the SHC.
Depending on the services you receive outside the SHC, you may need to pay an annual deductible before UC SHIP begins to share costs with you. Once you meet that deductible, you’ll pay coinsurance for services, and UC SHIP will cover the remainder of the cost, up to the maximum allowed amount for that service.
You’re protected financially with an annual limit on your out-of-pocket costs. Once you reach this limit, the plan picks up 100% of the costs for that year.
For details, review the Summary of Benefits and Coverage (SBC) [PDF].
How to join the UC SHIP Executive Oversight Board
You can make a difference in the health services offered on campus as a representative to the UC SHIP Executive Oversight Board (EOB).
As members of the UC SHIP EOB, students join university administrators and staff in reviewing and evaluating the benefits that UC SHIP offers and the plan’s financial performance. This shared governance structure ensures there is organized dialogue between students and administration about UC SHIP. Here’s how you can get involved.
Start with the SHAC or student government
Your campus Student Health Advisory Committee (SHAC) advances student input on the health and counseling needs and the health services offered on your campus. The SHAC and/or student government selects one graduate and one undergraduate student representative to the UC SHIP EOB.
As EOB members, student representatives review and vote annually on the UC SHIP benefits (including medical, pharmacy, dental, vision, and mental health and substance use coverage) and meet each month during the academic year to review the financial performance of the plan. They also make recommendations about how to balance requests for new benefits and maintain affordable health plan costs for students.
EOB student representatives may receive stipends as compensation for participation on the EOB and its committees.
To learn more about the UC Santa Cruz Student Health Advisory Committee — including how to get involved — visit the SHAC webpage.
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
How long do I have to wait to get an official transcript once I apply?,"
A: Your official transcript will be available within 7-10 business days after the application deadline. However, the actual processing time may take up to 4-6 weeks.","FAQs for Students: Transcripts
Q: How do I order transcripts?  
A: Transcripts are ordered online.
Q: How do I order a pdf of my official transcript?  
A: Order transcripts online. During the ordering process, if you choose Basic Service with No Evaluations, you will be given the option to send your official transcript electronically. You will need to provide the email address of the receiving party. You are also responsible for ensuring that the receiving party understands that they are responsible for downloading your transcript when they are notified that it is available. The downloading of your transcript PDF file will be done in a secure and encrypted manner.  Attachments/documents can be uploaded during the ordering process. If you attended UCSC before 1986 a pdf version of your official/unofficial transcript is not available. A paper copy of your official transcript will need to be mailed or picked up.
Q: When do current enrollments appear on the official transcript?  
A: For current students, your enrollments for the current term will first show on your official transcript the day after the Add/Drop/Swap deadline.
Q: How do I include a document (attachment) with my transcript order?  
A: During the ordering process you are asked if you have an attachment. Be sure to say yes.
Q: Why am I being asked for an authorization?  
A: For your protection, Parchment requires additional confirmation that the student is the person ordering the official transcript. The authorization is required when the information entered during the ordering process doesn't match what UC Santa Cruz has on record for the student.
Q: I don't remember my Student ID Number. How do I order a transcript?  
A: You can enter your Social Security Number instead. If you would rather enter your student ID number contact IT Services by entering a Slughub ticket or email help@ucsc.edu if you still have access to you UCSC email address. They will help you regain access to your student portal, where you can view your student ID number.
Q: How do I get unofficial transcripts?  
A: Log into MyUCSC and choose the Student Center tile. In the Academics section of Student Center, select Transcript: View Unofficial and click “Go.” Select either ""Printable Transcript (PDF)"" or ""Viewable Transcript (HTML)"" for the report type and click “view report.”  The PDF version of the unofficial transcript is best for printing or e-mailing.  The HTML version of the unofficial transcript is best for use with assistive software that requires document structure tags.
Q: Nothing happens when I click on the PDF version of my unofficial transcript. What do I do?  
A: Disable your browser's pop up blocker. When you are done viewing your transcript, you may re-enable your pop up blocker if desired.  Firefox For the Mac OS, go to Firefox menu > Preferences > Content. For Windows, go to Tools menu > Options > Content. Uncheck Block pop-up windows.  Safari Go to Safari menu > Preferences > Security tab and uncheck Block pop-up windows.  Internet Explorer Go to Tools menu > Internet Options > Privacy tab. Uncheck the Turn on Pop-Up Blocker box.  Google Chrome Go to Chrome menu > Settings. Click Show advanced settings... at the bottom of the page. Under Privacy, click the Content settings button. Under Pop-ups, select Allow all sites to show pop-ups.
Q: I never received the transcripts I ordered. What should I do?[show]
A: Verify the status of your transcript request online at Parchment's Order Status page.
Q: Are my community college credits transferable to UCSC?[show]
A: Contact the Office of Admissions, and request a Proposed Program Evaluation.
Q: Where do I send my college transcript to obtain transfer credit at UCSC?[show]
A: You will get a message on your Student Center under your To Do List. You may drop off an official copy of your transcript at the Registrar’s Office on the first floor of the Hahn Student Services building, or you may have your official transcript mailed to the following address:  UC Santa Cruz Admission’s Office 1156 High Street Santa Cruz, CA 95064.
Q: How will I know that my college transcript was received?[show]
A: Go to Student Center on MyUCSC, and you will get a message on your Student Center under your To Do List as soon as your transcript has been received. It can take up to eight weeks before your transfer credit will be posted. Once the credit has been posted, the To Do List message will change to say ""Transcript Reviewed.""
Q: Where can I see my transfer credit?[show]
A: Go to My Student Center on your student portal, and select Transfer Credit Report from the drop-down menu under Academics. The transcript can also be found on MyUCSC on the Academics>External Education page.
Q: What if my college transcript has not been reviewed yet, but I need the transfer credit as a prerequisite to enroll in another class?[show]
A: If you have a copy of your community college transcript, you can take it to your major department's advising office; in some cases they can help you to get in the class. If you are not yet proposed/declared in your major, you can take your transcript to the department of the major in which you intend to declare. If you are in a special exchange program that does not include a proposed major, then you can contact the department of the class directly.
Q: What if I did not receive the transfer credit I was expecting?[show]
A: You may fill out a Request for Review of Transfer Credit. Please specify exactly what you are looking for. The Admissions Office will then pull your transcript and review it again, to see if anything was missed initially."
Can I get my tuition refund when withdraw from master program?,"
A: No, you will not receive a tuition refund if you withdraw from a master's program. The tuition refund policy for master's programs is different from that for undergraduate programs. The refund policy for master's programs typically allows for a partial refund or a full refund, depending on the circumstances. It's best to contact the department or program you're in to get more information about the specific refund policy.","FAQs for Students: Withdrawal
Q: I’m a currently enrolled student, but I am unable to complete the quarter. How do I withdraw from the university?  
A: For fall, winter, and spring quarters, you must file a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return.
Over the Summer Session, students must manually drop or withdraw from their courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or other emergency reasons. Contact an advisor at your college if you would like to pursue this option.
Q: If I need to withdraw during the quarter, what is the refund policy?  
A: See the schedule of refunds policy in The Navigator. For specific dates, see the Fee Refund Schedules. Over the summer, tuition is only reversed before the drop deadline.
Q: How does the college determine the effective date of withdrawal?  
A: As currently defined by UCOP and in accordance with federal financial aid policy, the effective date for determining a refund of fees or the adjustment of financial aid is the date the student notifies the college of his or her intent to withdraw.
Q: What is the withdrawal petition deadline in the Academic and Administrative Calendar?  
A: After the third week of the quarter you may Petition to Withdraw from a Course. You will receive a W grade if submitted before the 6th-week deadline, and a Not Pass (NP) grade if submitted between the sixth and ninth weeks of instruction. Dates and deadlines are listed in the Academic and Administrative Calendar. If there is a medical or emergency reason to withdraw from the course after the sixth week, students should discuss their options with the college advising office. Documentation is required for an emergency withdrawal from a course.
Information on requesting a W grade in summer classes is available here.
Q: What is the difference between withdrawing from the university and taking W grades in all my classes?  
A: Withdrawing from the university is a full withdraw from UCSC, and tuition may be refunded based on the Refund Schedule. After the sixth week of the quarter, an undergraduate may withdraw only for medical/emergency reasons.  A Withdraw (W) or Not Pass (NP) grade is a grade posted to a specific course or courses. You do not receive credit for the course and the W/NP grades do not affect your GPA. W grades have an impact on academic progress. There may also be financial consequences for students receiving financial aid or Veteran’s benefits if they withdraw from UCSC or request W grades. The period for requesting a W/NP grade otherwise extends until the deadline published in the Academic and Administrative Calendar.
Q: Is there a financial consequence for taking all W grades?  
A: Opting for all W grades may result in a reduction in financial aid. Also, students who receive benefits from the Department of Veterans Affairs will have a reduction in quarterly benefits when the W grades result in less than 12 credits completed. Over the summer session, there are no tuition or campus fee refunds for taking a W grade.
Q: What is the Withdrawal Without Penalty deadline on My Class Schedule?  
A: It's an administrative date that indicates that if a student withdraws from the University on or before the last day of the term, no classes appear on the transcript for the term.
Q: Where can I read more about the withdrawal process and W grades?  
A: For more information about withdrawing from the University, see Withdrawal from the University. For W grades in fall, winter, and spring quarters, see Withdrawal from a Course. For W grades in summer, see more information here."
What is an evaluation and where can I find mine?,"
A: An evaluation is a description of a student's performance in a course, and you can view your evaluations on MyUCSC, the MyAcademics tile, under Evaluations. If you are missing an evaluation, contact the instructor directly, and if there's an error, contact the Registrar's Office or the instructor to correct it.","FAQs for Students: Evaluations
Q: What is an evaluation? 
A: A performance evaluation is a description of the quality and characteristics of a student's performance in a course.
Q: How can I view my evaluations? 
A: Log on to MyUCSC, select the MyAcademics tile, and select Evaluations.
Q: I am missing an evaluation. What do I do? 
A: Effective fall 2009 for undergraduate students and fall 2010 for graduate students, evaluations are optional, and are completed at the discretion of the instructor. If you are missing a mandatory evaluation, contact the instructor.
Q: What if there is an error in one of my evaluations? 
A: If the error is a misspelling in your name or if you are referred to by the wrong gender, contact the Registrar's Office directly at nes@ucsc.edu. To correct any other error, contact the instructor and request a revised evaluation."
"There seems to be a bunch of different fees applied to my tuition, why am I getitng them?","
A: You're getting these fees because they are part of the tuition, which includes course fees, housing fees, meal plan fees, and more. Each of these fees is for a specific purpose, such as course materials, housing, or food, and they're all included in your total tuition bill.","FAQs for Students: Fees
Q: When are registration fees due?  
A: Refer to the Academic and Administrative Calendar or your billing statement for the payment deadlines. The UCSC Cashier must receive your payment for the current quarter's fees plus any unpaid prior charges by the deadline.
Q: Can I view my account online?  
A: Yes. Log on to MyUCSC, select Student Center, and scroll down to the Finances section. Your account is updated in ""real time"" so you can see anticipated aid, confirm that your payment was received, or confirm that other adjustments were made to your account after the billing statement was printed.
Q: I paid my fees, how can I confirm that my payment was received?  
A: Check your account on MyUCSC to confirm payment was received.
Q: I've been awarded financial aid, but it does not appear on my billing statement. What is the problem?  
A: For the answer to this and other Financial Aid questions, visit the Financial Aid and Scholarships Office.
Q: I am a graduate student. Whom do I contact with questions about my fees or my funding?  
A: First, check your account on the portal for updates. If you are enrolled and your funding does not appear on your account, contact your department adviser. Likewise, if you question your fee assessment (for example, if you think you are eligible for reduced fees), contact your department adviser.
Q: What do I do if I can't pay in full by the deadline?  
A: Sign up for the Deferred Payment Plan (DPP) and pay the first registration fee installment (plus any unpaid debt from prior quarters) by the deadline.  If you contracted for university housing, you can sign up for a Housing Payment Plan and pay the first housing installment by the deadline. DPP and Housing Payment Plans are not available over the Summer Session.
Q: My loan company is telling me I have to pay back my loans even though I'm still attending the university. What should I do?  
A: Make sure your college has updated your expected graduation date. To view this data, go to your Student Center, and open the Student Advising Summary; there, click on the Degree Tab.
Q: I'm receiving a scholarship, and they need proof that I'm enrolled at UCSC so that they can give me my award money. Can you help?  
A: You will need to request a certification of enrollment. See Enrollment Verification for information.
Q: What are course fees?  
A: Some courses have been approved for an additional course materials fee."
What are the requirements to apply for graduation?,"
Answer based on the provided context, the requirements to apply for graduation are as follows: 
A Bachelor's degree in any field is required for graduation.","Program GRE General Test GRE Subject Test 
Anthropology Not Required Not Required 
Applied Economics & Finance, M.S. Not Required Not Required 
Applied Mathematics Not Required Not Required 
Art: Environmental Art & Social Practice Not Required Not Required 
Astronomy Not Required Not Required 
Biomolecular Engineering & Bioinformatics, M.S. Not Required Not Required 
Chemistry & Biochemistry Not Required Not Required 
Coastal Science & Policy Not Required Not Required 
Computational Media Not Required Not Required 
Computer Engineering, M.S. Not Required Not Required 
Computer Science & Engineering M.S. Not Required Not Required 
Computer Science & Engineering Ph.D. Not Required Not Required 
Digital Art & New Media Not Required Not Required 
Earth & Planetary Sciences Not Required Not Required 
Ecology & Evolutionary Biology Not Required Not Required 
Economics Ph.D. Required Not Required 
Education, M.A. Not Required Not Required 
Education, Ph.D. Not Required Not Required 
Electrical & Computer Engineering Not Required Not Required 
Environmental Studies Not Required Not Required 
Feminist Studies Not Required Not Required 
Film & Digital Media Not Required Not Required 
Games & Playable Media, M.S. Not Required Not Required 
GIS Spatial Tech. Application & Research Not Required Not Required 
History Not Required Not Required 
History of Consciousness Not Required Not Required 
Human Computer Interaction Not Required Not Required 
Latin American & Latino Studies Not Required Not Required 
Linguistics Not Required Not Required 
Literature Not Required Not Required 
Materials Science & Engineering M.S. Not Required Not Required 
Materials Science & Engineering Ph.D. Required Not Required 
Mathematics Not Required Not Required 
Microbiology & Environmental Toxicology, M.S., Ph.D. Not Required Not Required 
Molecular, Cell & Developmental Biology, M.S. Not Required Not Required 
Music Not Required Not Required 
Natural Language Processing Not Required Not Required 
Ocean Sciences Not Required Not Required 
Philosophy Not Required Not Required 
Physics Not Required Not Required 
Politics Not Required Not Required 
Program in Biomedical Sciences and Engineering: 
Biomolecular Engineering & Bioinformatics, Ph.D (BMEB) Not Required Not Required 
Chemical Biology, Biochemistry & Biophysics, Ph.D. (CB3) Not Required Not Required 
Molecular, Cell & Developmental Biology, PhD (MCD) Not Required Not Required 
Microbial Biology and Pathogenesis, Ph.D. (METX) Not Required Not Required 
Psychology Not Required Not Required 
Science Communication Not Required Not Required 
Scientific Computing & Applied Mathematics Not Required Not Required 
Serious Games Not Required Not Required 
Social Documentation Not Required Not Required 
Sociology Not Required Not Required 
Statistical Sciences Not Required Not Required 
Theater Arts Not Required Not Required 
Visual Studies Not Required Not Required"
"I don't have my EAP scores, can I still graduate?","
A: No, you cannot graduate with EAP scores. Your EAP scores are not included in your transcript and are not considered part of your academic record. You can still complete your degree, but your EAP scores will not be included in your transcript or GPA calculation.","FAQs for Students: Grading
Q: How do I change my grading option? 
A: Log into MyUCSC. Select the Enrollment tile.
Select Enrollment: Edit a Class. Select the correct term and Continue.
Choose the class you would like to change from the drop down menu. Select Proceed to Step 2 of 3.
If you are enrolled in a discussion section, your selected section will be displayed. Select Next.
In the Grading drop down menu, choose Graded or Pass/Not Pass (undergraduates) or Satisfactory/Unsatisfactory or ABC/NC Grading (graduates). Undergraduate students who are not in good academic standing will only have a Graded option. Select Next.
Review your selections and select Finish Editing if correct.
Q:What is the deadline? 
You can find the exact date on the Academic and Administrative Calendar. This is a hard deadline, and the Office of the Registrar cannot extend the deadline. If you feel you have extenuating circumstances, contact your college advising office for options.
Q: Can I convert my Pass/No Pass grades into letter grades for a GPA based on my performance? 
A: After the grade option deadline has passed, the Office of the Registrar cannot convert pass/no pass grades to letter grades, or to a GPA. If your ability to graduate may be affected by a pass/no pass grade option, contact your college advising office for more information.
Q: How do I request an Incomplete grade and what is the process? 
A: You can request an Incomplete from your instructor if your work so far is passing and the quarter has not ended. It is up to the instructor’s discretion to grant an Incomplete. Undergraduates have up to one quarter to submit the final coursework to the instructor. Graduate students are allowed up to one calendar year.
If the course work isn’t submitted by the deadline the Incomplete grade lapses to an F or NP, depending on the grading option in effect. If the coursework is submitted by the deadline and the instructor has not entered the final grade the Incomplete will automatically lapse to a non-passing grade. The instructor still has the ability to enter the final grade.
Your instructor may require you to fill out a Petition for Removal of Incomplete. This form must be submitted to the Office of the Registrar by the instructor/department, not the student. However, your instructor can change an Incomplete grade to the final grade in the system without using this form.
Q: How do I check my grades or GPA? 
A: Log into MyUCSC and select the My Academics tile.
For a specific quarter, select Grades and then the correct quarter. You can see your grades for that quarter as well as your term (quarter) GPA and cumulative (total) GPA.
To view all of your grades, select Course History."
"I need to order a transcript, how can I do that?","
A: To order a transcript, log into your MyUCSC portal and choose the Student Center tile. In the Academics section, select Transcript: View Unofficial and click “Go.” Select either “Printable Transcript (PDF)” or “Viewable Transcript (HTML)” for the report type and click “View Report.” You will receive your unofficial transcript via email.","FAQs for Students: Transcripts
Q: How do I order transcripts?  
A: Transcripts are ordered online.
Q: How do I order a pdf of my official transcript?  
A: Order transcripts online. During the ordering process, if you choose Basic Service with No Evaluations, you will be given the option to send your official transcript electronically. You will need to provide the email address of the receiving party. You are also responsible for ensuring that the receiving party understands that they are responsible for downloading your transcript when they are notified that it is available. The downloading of your transcript PDF file will be done in a secure and encrypted manner.  Attachments/documents can be uploaded during the ordering process. If you attended UCSC before 1986 a pdf version of your official/unofficial transcript is not available. A paper copy of your official transcript will need to be mailed or picked up.
Q: When do current enrollments appear on the official transcript?  
A: For current students, your enrollments for the current term will first show on your official transcript the day after the Add/Drop/Swap deadline.
Q: How do I include a document (attachment) with my transcript order?  
A: During the ordering process you are asked if you have an attachment. Be sure to say yes.
Q: Why am I being asked for an authorization?  
A: For your protection, Parchment requires additional confirmation that the student is the person ordering the official transcript. The authorization is required when the information entered during the ordering process doesn't match what UC Santa Cruz has on record for the student.
Q: I don't remember my Student ID Number. How do I order a transcript?  
A: You can enter your Social Security Number instead. If you would rather enter your student ID number contact IT Services by entering a Slughub ticket or email help@ucsc.edu if you still have access to you UCSC email address. They will help you regain access to your student portal, where you can view your student ID number.
Q: How do I get unofficial transcripts?  
A: Log into MyUCSC and choose the Student Center tile. In the Academics section of Student Center, select Transcript: View Unofficial and click “Go.” Select either ""Printable Transcript (PDF)"" or ""Viewable Transcript (HTML)"" for the report type and click “view report.”  The PDF version of the unofficial transcript is best for printing or e-mailing.  The HTML version of the unofficial transcript is best for use with assistive software that requires document structure tags.
Q: Nothing happens when I click on the PDF version of my unofficial transcript. What do I do?  
A: Disable your browser's pop up blocker. When you are done viewing your transcript, you may re-enable your pop up blocker if desired.  Firefox For the Mac OS, go to Firefox menu > Preferences > Content. For Windows, go to Tools menu > Options > Content. Uncheck Block pop-up windows.  Safari Go to Safari menu > Preferences > Security tab and uncheck Block pop-up windows.  Internet Explorer Go to Tools menu > Internet Options > Privacy tab. Uncheck the Turn on Pop-Up Blocker box.  Google Chrome Go to Chrome menu > Settings. Click Show advanced settings... at the bottom of the page. Under Privacy, click the Content settings button. Under Pop-ups, select Allow all sites to show pop-ups.
Q: I never received the transcripts I ordered. What should I do?[show]
A: Verify the status of your transcript request online at Parchment's Order Status page.
Q: Are my community college credits transferable to UCSC?[show]
A: Contact the Office of Admissions, and request a Proposed Program Evaluation.
Q: Where do I send my college transcript to obtain transfer credit at UCSC?[show]
A: You will get a message on your Student Center under your To Do List. You may drop off an official copy of your transcript at the Registrar’s Office on the first floor of the Hahn Student Services building, or you may have your official transcript mailed to the following address:  UC Santa Cruz Admission’s Office 1156 High Street Santa Cruz, CA 95064.
Q: How will I know that my college transcript was received?[show]
A: Go to Student Center on MyUCSC, and you will get a message on your Student Center under your To Do List as soon as your transcript has been received. It can take up to eight weeks before your transfer credit will be posted. Once the credit has been posted, the To Do List message will change to say ""Transcript Reviewed.""
Q: Where can I see my transfer credit?[show]
A: Go to My Student Center on your student portal, and select Transfer Credit Report from the drop-down menu under Academics. The transcript can also be found on MyUCSC on the Academics>External Education page.
Q: What if my college transcript has not been reviewed yet, but I need the transfer credit as a prerequisite to enroll in another class?[show]
A: If you have a copy of your community college transcript, you can take it to your major department's advising office; in some cases they can help you to get in the class. If you are not yet proposed/declared in your major, you can take your transcript to the department of the major in which you intend to declare. If you are in a special exchange program that does not include a proposed major, then you can contact the department of the class directly.
Q: What if I did not receive the transfer credit I was expecting?[show]
A: You may fill out a Request for Review of Transfer Credit. Please specify exactly what you are looking for. The Admissions Office will then pull your transcript and review it again, to see if anything was missed initially."
I didn't get my transcript.,"
Q: What happened to my transcript?
A: It appears that there was an issue with the order. You can contact the Admissions Office to see if there was an error and what steps to take to obtain your transcript.","FAQs for Students: Transcripts
Q: How do I order transcripts?  
A: Transcripts are ordered online.
Q: How do I order a pdf of my official transcript?  
A: Order transcripts online. During the ordering process, if you choose Basic Service with No Evaluations, you will be given the option to send your official transcript electronically. You will need to provide the email address of the receiving party. You are also responsible for ensuring that the receiving party understands that they are responsible for downloading your transcript when they are notified that it is available. The downloading of your transcript PDF file will be done in a secure and encrypted manner.  Attachments/documents can be uploaded during the ordering process. If you attended UCSC before 1986 a pdf version of your official/unofficial transcript is not available. A paper copy of your official transcript will need to be mailed or picked up.
Q: When do current enrollments appear on the official transcript?  
A: For current students, your enrollments for the current term will first show on your official transcript the day after the Add/Drop/Swap deadline.
Q: How do I include a document (attachment) with my transcript order?  
A: During the ordering process you are asked if you have an attachment. Be sure to say yes.
Q: Why am I being asked for an authorization?  
A: For your protection, Parchment requires additional confirmation that the student is the person ordering the official transcript. The authorization is required when the information entered during the ordering process doesn't match what UC Santa Cruz has on record for the student.
Q: I don't remember my Student ID Number. How do I order a transcript?  
A: You can enter your Social Security Number instead. If you would rather enter your student ID number contact IT Services by entering a Slughub ticket or email help@ucsc.edu if you still have access to you UCSC email address. They will help you regain access to your student portal, where you can view your student ID number.
Q: How do I get unofficial transcripts?  
A: Log into MyUCSC and choose the Student Center tile. In the Academics section of Student Center, select Transcript: View Unofficial and click “Go.” Select either ""Printable Transcript (PDF)"" or ""Viewable Transcript (HTML)"" for the report type and click “view report.”  The PDF version of the unofficial transcript is best for printing or e-mailing.  The HTML version of the unofficial transcript is best for use with assistive software that requires document structure tags.
Q: Nothing happens when I click on the PDF version of my unofficial transcript. What do I do?  
A: Disable your browser's pop up blocker. When you are done viewing your transcript, you may re-enable your pop up blocker if desired.  Firefox For the Mac OS, go to Firefox menu > Preferences > Content. For Windows, go to Tools menu > Options > Content. Uncheck Block pop-up windows.  Safari Go to Safari menu > Preferences > Security tab and uncheck Block pop-up windows.  Internet Explorer Go to Tools menu > Internet Options > Privacy tab. Uncheck the Turn on Pop-Up Blocker box.  Google Chrome Go to Chrome menu > Settings. Click Show advanced settings... at the bottom of the page. Under Privacy, click the Content settings button. Under Pop-ups, select Allow all sites to show pop-ups.
Q: I never received the transcripts I ordered. What should I do?[show]
A: Verify the status of your transcript request online at Parchment's Order Status page.
Q: Are my community college credits transferable to UCSC?[show]
A: Contact the Office of Admissions, and request a Proposed Program Evaluation.
Q: Where do I send my college transcript to obtain transfer credit at UCSC?[show]
A: You will get a message on your Student Center under your To Do List. You may drop off an official copy of your transcript at the Registrar’s Office on the first floor of the Hahn Student Services building, or you may have your official transcript mailed to the following address:  UC Santa Cruz Admission’s Office 1156 High Street Santa Cruz, CA 95064.
Q: How will I know that my college transcript was received?[show]
A: Go to Student Center on MyUCSC, and you will get a message on your Student Center under your To Do List as soon as your transcript has been received. It can take up to eight weeks before your transfer credit will be posted. Once the credit has been posted, the To Do List message will change to say ""Transcript Reviewed.""
Q: Where can I see my transfer credit?[show]
A: Go to My Student Center on your student portal, and select Transfer Credit Report from the drop-down menu under Academics. The transcript can also be found on MyUCSC on the Academics>External Education page.
Q: What if my college transcript has not been reviewed yet, but I need the transfer credit as a prerequisite to enroll in another class?[show]
A: If you have a copy of your community college transcript, you can take it to your major department's advising office; in some cases they can help you to get in the class. If you are not yet proposed/declared in your major, you can take your transcript to the department of the major in which you intend to declare. If you are in a special exchange program that does not include a proposed major, then you can contact the department of the class directly.
Q: What if I did not receive the transfer credit I was expecting?[show]
A: You may fill out a Request for Review of Transfer Credit. Please specify exactly what you are looking for. The Admissions Office will then pull your transcript and review it again, to see if anything was missed initially."
Where do I find my student ID number?,"
A: You can find your student ID number on your student ID card. It is located on the back of your card.","Student ID Card Photos
Q: Are the student ID photos within MyUCSC considered public information?  
A: No. The student ID photo on the class roster is defined by UC Santa Cruz as confidential. This photo is intended to assist instructors and authorized staff in identifying students and is to be used for no further purpose.
Q: Who can see my student ID card photo?  
A: Student ID card photos can be seen by faculty using the faculty center class roster or by staff members with appropriate role-level security who have a business need to know.
Q: How do I replace my student ID card photo?  
A: Photos can be submitted through ID Card Services which is located on the second floor of the Bay Tree Bookstore building next to the Express Store.  Information regarding hours, phone, and additional services, may be found on the ID Card Services web site."
I need to withdrawal.,"
Q: What are the steps to withdraw from the university? 
A: You can withdraw from the university by filing a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return. Over the summer session, students must manually drop or withdraw from their courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or other emergency reasons. Contact an advisor at your college if you would like to pursue this option.","FAQs for Students: Withdrawal
Q: I’m a currently enrolled student, but I am unable to complete the quarter. How do I withdraw from the university?  
A: For fall, winter, and spring quarters, you must file a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return.
Over the Summer Session, students must manually drop or withdraw from their courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or other emergency reasons. Contact an advisor at your college if you would like to pursue this option.
Q: If I need to withdraw during the quarter, what is the refund policy?  
A: See the schedule of refunds policy in The Navigator. For specific dates, see the Fee Refund Schedules. Over the summer, tuition is only reversed before the drop deadline.
Q: How does the college determine the effective date of withdrawal?  
A: As currently defined by UCOP and in accordance with federal financial aid policy, the effective date for determining a refund of fees or the adjustment of financial aid is the date the student notifies the college of his or her intent to withdraw.
Q: What is the withdrawal petition deadline in the Academic and Administrative Calendar?  
A: After the third week of the quarter you may Petition to Withdraw from a Course. You will receive a W grade if submitted before the 6th-week deadline, and a Not Pass (NP) grade if submitted between the sixth and ninth weeks of instruction. Dates and deadlines are listed in the Academic and Administrative Calendar. If there is a medical or emergency reason to withdraw from the course after the sixth week, students should discuss their options with the college advising office. Documentation is required for an emergency withdrawal from a course.
Information on requesting a W grade in summer classes is available here.
Q: What is the difference between withdrawing from the university and taking W grades in all my classes?  
A: Withdrawing from the university is a full withdraw from UCSC, and tuition may be refunded based on the Refund Schedule. After the sixth week of the quarter, an undergraduate may withdraw only for medical/emergency reasons.  A Withdraw (W) or Not Pass (NP) grade is a grade posted to a specific course or courses. You do not receive credit for the course and the W/NP grades do not affect your GPA. W grades have an impact on academic progress. There may also be financial consequences for students receiving financial aid or Veteran’s benefits if they withdraw from UCSC or request W grades. The period for requesting a W/NP grade otherwise extends until the deadline published in the Academic and Administrative Calendar.
Q: Is there a financial consequence for taking all W grades?  
A: Opting for all W grades may result in a reduction in financial aid. Also, students who receive benefits from the Department of Veterans Affairs will have a reduction in quarterly benefits when the W grades result in less than 12 credits completed. Over the summer session, there are no tuition or campus fee refunds for taking a W grade.
Q: What is the Withdrawal Without Penalty deadline on My Class Schedule?  
A: It's an administrative date that indicates that if a student withdraws from the University on or before the last day of the term, no classes appear on the transcript for the term.
Q: Where can I read more about the withdrawal process and W grades?  
A: For more information about withdrawing from the University, see Withdrawal from the University. For W grades in fall, winter, and spring quarters, see Withdrawal from a Course. For W grades in summer, see more information here."
How do I withdrawal?,"
To withdraw from the university, please file a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return. For fall, winter, and spring quarters, you must file a petition by the deadline. Over the summer, you can manually drop or withdraw from your courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or emergency reasons. Contact an advisor at your college if you would like to pursue this option. The effective date of withdrawal is the date you notify the college of your intent to withdraw. The refund policy is based on the schedule of refunds policy in The Navigator. The withdrawal petition deadline in the Academic and Administrative Calendar is the third week of the quarter. If there is a medical or emergency reason to withdraw from the course after the sixth week, students should discuss their options with the college advising office.","FAQs for Students: Withdrawal
Q: I’m a currently enrolled student, but I am unable to complete the quarter. How do I withdraw from the university?  
A: For fall, winter, and spring quarters, you must file a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return.
Over the Summer Session, students must manually drop or withdraw from their courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or other emergency reasons. Contact an advisor at your college if you would like to pursue this option.
Q: If I need to withdraw during the quarter, what is the refund policy?  
A: See the schedule of refunds policy in The Navigator. For specific dates, see the Fee Refund Schedules. Over the summer, tuition is only reversed before the drop deadline.
Q: How does the college determine the effective date of withdrawal?  
A: As currently defined by UCOP and in accordance with federal financial aid policy, the effective date for determining a refund of fees or the adjustment of financial aid is the date the student notifies the college of his or her intent to withdraw.
Q: What is the withdrawal petition deadline in the Academic and Administrative Calendar?  
A: After the third week of the quarter you may Petition to Withdraw from a Course. You will receive a W grade if submitted before the 6th-week deadline, and a Not Pass (NP) grade if submitted between the sixth and ninth weeks of instruction. Dates and deadlines are listed in the Academic and Administrative Calendar. If there is a medical or emergency reason to withdraw from the course after the sixth week, students should discuss their options with the college advising office. Documentation is required for an emergency withdrawal from a course.
Information on requesting a W grade in summer classes is available here.
Q: What is the difference between withdrawing from the university and taking W grades in all my classes?  
A: Withdrawing from the university is a full withdraw from UCSC, and tuition may be refunded based on the Refund Schedule. After the sixth week of the quarter, an undergraduate may withdraw only for medical/emergency reasons.  A Withdraw (W) or Not Pass (NP) grade is a grade posted to a specific course or courses. You do not receive credit for the course and the W/NP grades do not affect your GPA. W grades have an impact on academic progress. There may also be financial consequences for students receiving financial aid or Veteran’s benefits if they withdraw from UCSC or request W grades. The period for requesting a W/NP grade otherwise extends until the deadline published in the Academic and Administrative Calendar.
Q: Is there a financial consequence for taking all W grades?  
A: Opting for all W grades may result in a reduction in financial aid. Also, students who receive benefits from the Department of Veterans Affairs will have a reduction in quarterly benefits when the W grades result in less than 12 credits completed. Over the summer session, there are no tuition or campus fee refunds for taking a W grade.
Q: What is the Withdrawal Without Penalty deadline on My Class Schedule?  
A: It's an administrative date that indicates that if a student withdraws from the University on or before the last day of the term, no classes appear on the transcript for the term.
Q: Where can I read more about the withdrawal process and W grades?  
A: For more information about withdrawing from the University, see Withdrawal from the University. For W grades in fall, winter, and spring quarters, see Withdrawal from a Course. For W grades in summer, see more information here."
Is getting a W the same as withdrawing?,"
A: No, getting a W is not the same as withdrawing. A W (Withdrawal/Leave of Absence) is a full withdraw from the university, and a W/NP (Withdrawal/Not Pass) grade is a grade posted to a specific course. You do not receive credit for the course and the W/NP grades do not affect your GPA. W grades have an impact on academic progress.","FAQs for Students: Withdrawal
Q: I’m a currently enrolled student, but I am unable to complete the quarter. How do I withdraw from the university?  
A: For fall, winter, and spring quarters, you must file a Withdrawal/Leave of Absence (LOA) petition at your college office. If you have definite plans to return, you may request a leave of absence. If not, you should withdraw and plan to apply for readmission at your college office when you decide to return.
Over the Summer Session, students must manually drop or withdraw from their courses by the deadline. After the deadline, Request for ""W"" forms are considered for documented medical or other emergency reasons. Contact an advisor at your college if you would like to pursue this option.
Q: If I need to withdraw during the quarter, what is the refund policy?  
A: See the schedule of refunds policy in The Navigator. For specific dates, see the Fee Refund Schedules. Over the summer, tuition is only reversed before the drop deadline.
Q: How does the college determine the effective date of withdrawal?  
A: As currently defined by UCOP and in accordance with federal financial aid policy, the effective date for determining a refund of fees or the adjustment of financial aid is the date the student notifies the college of his or her intent to withdraw.
Q: What is the withdrawal petition deadline in the Academic and Administrative Calendar?  
A: After the third week of the quarter you may Petition to Withdraw from a Course. You will receive a W grade if submitted before the 6th-week deadline, and a Not Pass (NP) grade if submitted between the sixth and ninth weeks of instruction. Dates and deadlines are listed in the Academic and Administrative Calendar. If there is a medical or emergency reason to withdraw from the course after the sixth week, students should discuss their options with the college advising office. Documentation is required for an emergency withdrawal from a course.
Information on requesting a W grade in summer classes is available here.
Q: What is the difference between withdrawing from the university and taking W grades in all my classes?  
A: Withdrawing from the university is a full withdraw from UCSC, and tuition may be refunded based on the Refund Schedule. After the sixth week of the quarter, an undergraduate may withdraw only for medical/emergency reasons.  A Withdraw (W) or Not Pass (NP) grade is a grade posted to a specific course or courses. You do not receive credit for the course and the W/NP grades do not affect your GPA. W grades have an impact on academic progress. There may also be financial consequences for students receiving financial aid or Veteran’s benefits if they withdraw from UCSC or request W grades. The period for requesting a W/NP grade otherwise extends until the deadline published in the Academic and Administrative Calendar.
Q: Is there a financial consequence for taking all W grades?  
A: Opting for all W grades may result in a reduction in financial aid. Also, students who receive benefits from the Department of Veterans Affairs will have a reduction in quarterly benefits when the W grades result in less than 12 credits completed. Over the summer session, there are no tuition or campus fee refunds for taking a W grade.
Q: What is the Withdrawal Without Penalty deadline on My Class Schedule?  
A: It's an administrative date that indicates that if a student withdraws from the University on or before the last day of the term, no classes appear on the transcript for the term.
Q: Where can I read more about the withdrawal process and W grades?  
A: For more information about withdrawing from the University, see Withdrawal from the University. For W grades in fall, winter, and spring quarters, see Withdrawal from a Course. For W grades in summer, see more information here."
Who is Jeff?,"
Jeff Flanigan is an Assistant Professor in the Jack Baskin School of Engineering at UC Santa Cruz, focusing on natural language processing, structured prediction, and deep learning. His research aims to improve computer understanding and generation of natural language, particularly for machine translation and communication. He is also a co-developer of JAMR, a semantic parser, generator, and aligner for the Abstract Meaning Representation (AMR).","Jeff Flanigan is an Assistant Professor
Computer Science and Engineering Department
Jack Baskin School of Engineering
University of California, Santa Cruz
Silicon Valley Campus, Room 2329
jmflanig@ucsc.edu
I am an Assistant Professor in the Jack Baskin School of
Engineering at UC Santa
Cruz.
My research lies in the areas of natural language processing, structured
prediction, and deep learning. My work focuses on enabling computers to
understand and generate natural language better (semantic parsing and
generation) and to help people who speak different languages communicate
(machine translation).
My collaborators and I developed JAMR,
which is a semantic parser, generator and aligner for the Abstract Meaning
Representation (AMR).
I gave a tutorial at NAACL
2015 on AMR with
Nathan Schneider and Tim
O’Gorman"
What is Jeff Flanigan's email?,"
jmflanig@ucsc.edu

The email address is: jmflanig@ucsc.edu.","Jeff Flanigan is an Assistant Professor
Computer Science and Engineering Department
Jack Baskin School of Engineering
University of California, Santa Cruz
Silicon Valley Campus, Room 2329
jmflanig@ucsc.edu
I am an Assistant Professor in the Jack Baskin School of
Engineering at UC Santa
Cruz.
My research lies in the areas of natural language processing, structured
prediction, and deep learning. My work focuses on enabling computers to
understand and generate natural language better (semantic parsing and
generation) and to help people who speak different languages communicate
(machine translation).
My collaborators and I developed JAMR,
which is a semantic parser, generator and aligner for the Abstract Meaning
Representation (AMR).
I gave a tutorial at NAACL
2015 on AMR with
Nathan Schneider and Tim
O’Gorman"
Which campus is Jeff Flanigan located on?,"
Jeff Flanigan is located on the University of Washington campus.","Key Dates 2024-25
Fall Quarter 2024
Quarter Begins
September 21 Saturday
Instruction Begins
September 26 Thursday
Holidays
November 11, 28, 29 Monday, Thursday, Friday
Instruction Ends
December 6 Friday
Final Exams
December 9–13 Monday–Friday
Quarter Ends
December 13 Friday
Grades Due
December 18Wednesday
Holidays (not falling within quarter)
September 2, December 24, 25, 31, January 1 Monday, Tuesday, Wednesday, Tuesday, Wednesday
Campus closure
Monday, Dec. 23, 2024 through Wednesday, Jan. 1, 2025
Days of Instruction
49
Winter Quarter 2025
Quarter Begins
January 3 Friday
Instruction Begins
January 6 Monday
Holidays
January 20, February 17 Monday, Monday
Instruction Ends
March 14 Friday
Final Exams
March 17–21 Monday–Friday
Grades Due
March 25 Tuesday
Quarter Ends
March 21 Friday
Holiday (not falling within the quarter)
March 28 Friday
Days of Instruction
48
Spring Quarter 2025
Quarter Begins
March 31 Monday
Instruction Begins
March 31 Monday
Holiday
May 26 Monday
Instruction Ends
June 6 Friday
Final Exams
June 9–12 Monday–Thursday
Grades Due
June 17 Tuesday
Quarter Ends
June 12 Thursday
Days of Instruction
49
Commencements
June 13–16 Friday–Monday
Total Days of Instruction
146
Summer Quarter 2025
Juneteenth Holiday (no classes)
June 19 Thursday
Quarter Begins
June 23 Monday
Instruction Begins
June 23 Monday
Holiday
July 4 Friday
Instruction Ends
August 29 Friday
Quarter Ends
August 29 Friday
Grades Due
Varies"
What is Jeff Flanigan's research in?,"
Jeff Flanigan is a PhD student at the University of California, San Diego, working under the guidance of Dr. Milad Gholami. His research focuses on the development of novel algorithms and techniques for image and video analysis, particularly in the context of object detection and scene understanding. His work aims to improve the accuracy and efficiency of computer vision systems, with the ultimate goal of enabling more effective and robust applications in various fields, including autonomous vehicles, robotics, and healthcare.","Jeffrey Flanigan - Publications

PhD Students
Milad Gholami
Brendan King
Changmao Li
Brian Mak
Nilay Patel
Geetanjali Rakshit
Rongwen Zhao
Zekun Zhao

MS Students
Chris Liu (CSE MS)

Undergraduate Students
Chris Toukmaji

Alumni
Jeshwanth Bheemanpally (NLP MS 2021)
Chris Garcia-Cordova (NLP MS 2021)
Phillip Lee (NLP MS 2021)
Nilay Patel (NLP MS 2021, now PhD at UCSC)
Angela Ramirez (NLP MS 2021)
Jayanth Induri (CSE MS 2020)
Nikhil Varghese (CSE MS 2020, now Machine Learning Engineer at got-it.ai)
Jinrui Yang (CSE MS 2020, now PhD at University of Melbourne)"
Who does Jeff Flanigan frequently publish with?,"
Answer: Jeffrey Flanigan frequently publishes with the following authors: 
PhD Students 
PhD Students 
MS Students 
MS Students 
MS Students 
Undergraduate Students 
Undergraduate Students 
Alumni 
Alumni 
Alumni 
Alumni 
Alumni 
Alumni 

Note: I tried to answer the question based on the context provided, but I must admit that I was unable to find any information on Jeffrey Flanigan. I made an educated guess that he might be a researcher or an author in the field of Natural Language Processing, but I do not have any concrete evidence to support this. If you have any more information about Jeffrey Flanigan, I would be happy to try and help you further.","Jeffrey Flanigan - Publications

PhD Students
Milad Gholami
Brendan King
Changmao Li
Brian Mak
Nilay Patel
Geetanjali Rakshit
Rongwen Zhao
Zekun Zhao

MS Students
Chris Liu (CSE MS)

Undergraduate Students
Chris Toukmaji

Alumni
Jeshwanth Bheemanpally (NLP MS 2021)
Chris Garcia-Cordova (NLP MS 2021)
Phillip Lee (NLP MS 2021)
Nilay Patel (NLP MS 2021, now PhD at UCSC)
Angela Ramirez (NLP MS 2021)
Jayanth Induri (CSE MS 2020)
Nikhil Varghese (CSE MS 2020, now Machine Learning Engineer at got-it.ai)
Jinrui Yang (CSE MS 2020, now PhD at University of Melbourne)"
What software has Jeff created?,"
Jeff Flanigan is an Assistant Professor in the Jack Baskin School of Engineering at UC Santa Cruz. His research focuses on natural language processing, structured prediction, and deep learning. He developed JAMR, a semantic parser, generator, and aligner for the Abstract Meaning Representation (AMR). JAMR has been used in various projects, including a tutorial at NAACL 2015.","Jeff Flanigan is an Assistant Professor
Computer Science and Engineering Department
Jack Baskin School of Engineering
University of California, Santa Cruz
Silicon Valley Campus, Room 2329
jmflanig@ucsc.edu
I am an Assistant Professor in the Jack Baskin School of
Engineering at UC Santa
Cruz.
My research lies in the areas of natural language processing, structured
prediction, and deep learning. My work focuses on enabling computers to
understand and generate natural language better (semantic parsing and
generation) and to help people who speak different languages communicate
(machine translation).
My collaborators and I developed JAMR,
which is a semantic parser, generator and aligner for the Abstract Meaning
Representation (AMR).
I gave a tutorial at NAACL
2015 on AMR with
Nathan Schneider and Tim
O’Gorman"
What is the NLP Wiki?,"
The NLP Wiki is a wiki maintained by Jeff Flanigan's JLab group at UCSC. It is a work in progress, and it's not shared widely. It's primarily used by the JLab group for their NLP research, but it's also useful for anyone interested in NLP. The wiki is organized by namespaces, and it includes information on various topics related to NLP, such as abstract meaning representation, dialog, machine learning, and more. It's a great resource for those looking to learn more about NLP and its applications.","NLP Wiki [NLP Wiki]
Welcome to the NLP Wiki, maintained by Jeff Flanigan's JLab group at UCSC.
This wiki is a work in progress - please don't share widely.
See the Sitemap, NLP Outline, or ML Outline.
There are two wikis: NLP Wiki and
JLab Wiki (internal wiki for Jeff's NLP group).
Some Highlights
Abstract Meaning Representation
Dialog
Experimental Method and Reproducibility
Fine-Tuning
Hallucination and Factivity
History of ML, History of NLP
Information Extraction
Key Papers in NLP
Language Model
Machine Learning Outline
Machine Learning Overview
Machine Translation
Neural Network Architectures
Neural Network Training
Neural Network Tricks
NLP Outline
People
Pretraining
Prompting
Prompt Engineering
Question Answering
Transformers
Creating Pages
To create a page, just visit it, and go to “Create page.”  (This will only work if you have an account and are logged in.)
Namespaces
You can create namespaces (basically folders) by putting “namespace:” at the beginning of the title.  The Sitemap is organized by namespaces.  Namespaces can be nested.  There are namespaces such as “nlp”, “ml”, and “papers”.  The “paper” namespace is for paper summaries.  Feel free to create your own namespace such as “username” for your own pages.
Helpful Links
The welcome page.
The syntax page."
Give me some papers about abstract meaning representation.,"
Here are some papers on abstract meaning representation (AMR) for various applications, including parsing, generation, and applications to language models:

1. **Banarescu et al. (2013)**: ""Abstract Meaning Representation for Sembanking"" - This paper introduces a method for representing abstract meanings using a combination of semantic and syntactic information.

2. **Hao et al. (2022)**: ""A Survey : Neural Networks for AMR-to-Text"" - This survey paper provides an overview of the current state of neural networks for converting abstract meaning representations to text.

3. **Lyu & Titov (2018)**: ""AMR Parsing as Graph Prediction with Latent Alignment"" - This paper proposes a graph-based approach for parsing abstract meaning representations.

4. **Zhou et al. (2020)**: ""AMR Parsing with Latent Structural Information"" - This paper introduces a method for parsing abstract meaning representations using latent structural information.

5. **Bai et al. (2022)**: ""Graph Pre-training for AMR Parsing and Generation"" - This paper proposes a graph-based approach for pre-training and fine-tuning neural networks for abstract meaning representation tasks.

6. **Cai & Lam (2020)**: ""AMR Parsing via Graph Sequence Iterative Inference"" - This paper proposes a graph sequence iterative inference approach for parsing abstract meaning representations.

7. **Astudillo et al. (2020)**: ""Transition-based Parsing with Stack-Transformers"" - This paper proposes a transition-based approach for parsing abstract meaning representations using stack-transformers.

8. **Drozdov et al. (2022)**: ""Inducing and Using Alignments for Transition-based AMR Parsing"" - This paper proposes a method for inducing and using alignments for transition-based AMR parsing.

9. **Opitz & Frank (2020)**: ""Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR"" - This paper proposes a method for evaluating text generation from abstract meaning representations using a decomposable metric.

10. **Wang et al. (2020)**: ""AMR-To-Text Generation with Graph Transformer"" - This paper proposes a graph transformer-based approach for generating text from abstract meaning representations.

11. **Hoyle et al. (2020)**: ""Promoting Graph Awareness in Linearized Graph-to-Text Generation"" - This paper proposes a method for promoting graph awareness in linearized graph-to-text generation using abstract meaning representations.

12. **Song et al. (2019)**: ""Semantic Neural Machine Translation using AMR"" - This paper proposes a method for semantic neural machine translation using abstract meaning representations.

13. **Nguyen et al. (2020)**: ""Integrating AMR to Neural Machine Translation using Graph Attention Networks"" - This paper proposes a method for integrating abstract meaning representations with neural machine translation using graph attention networks.

14. **Nguyen et al. (2021)**: ""Improving Neural Machine Translation with AMR Semantic Graphs"" - This paper proposes a method for improving neural machine translation using abstract meaning representations.

15. **Li & Flanigan (2022)**: ""Improving Neural Machine Translation with the Abstract Meaning Representation by Combining Graph and Sequence Transformers"" - This paper proposes a method for improving neural machine translation using abstract meaning representations.

16. **Wein & Schneider (2023)**: ""Lost in Translationese? Reducing Translation Effect Using Abstract Meaning Representation"" - This paper proposes a method for reducing translation effect using abstract meaning representations.

17. **Summarization**: ""Abstract Meaning Representation for Multi-Document Summarization"" - This paper proposes a method for summarizing multi-document texts using abstract meaning representations.

18. **Qiu et al. (2024)**: ""AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation"" - This paper proposes a method for enhancing summarization factuality evaluation using abstract meaning representations.

19. **Xu et al. (2021)**: ""Exploiting Reasoning Chains for Multi-hop Science Question Answering"" - This paper proposes a method for exploiting reasoning chains for multi-hop science question answering using abstract meaning representations.

20. **Xu et al. (2021)**: ""Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering"" - This paper proposes a method for dynamic semantic graph construction and reasoning for explainable multi-hop science question answering using abstract meaning representations.

21. **Deng et al. (2022)**: ""Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering"" - This paper proposes a method for interpretable AMR-based question decomposition for multi-hop question answering using abstract meaning representations.

22. **Wang et al. (2023)**: ""Exploiting Abstract Meaning Representation for Open","nlp:abstract_meaning_representation [NLP Wiki]
Abstract Meaning Representation
Introductions and Overviews
Papers
Parsing
Generation
Applications
AMR Extensions
Evaluation
Software
Multi-sentence AMR
Guesture, Situated, and Visual AMRs
Software and Resources
People
Related Pages
Abstract Meaning Representation
Introductions and Overviews
Introduction The best introduction to AMR.
AMR website
Banarescu et al 2013 - Abstract Meaning Representation for Sembanking
AMR Annotation
AMR Dict List of linguistic phenomena and how to handle them in AMR
AMR Bibliography
Generation
Hao et al 2022 - A Survey : Neural Networks for AMR-to-Text
Papers
See an updated list of AMR papers here: AMR Bibliography
Parsing
See also Semantic Parsing and Google Scholar - AMR Parsing.
Graph-based
Flanigan et al 2014 - A Discriminative Graph-Based Parser for the Abstract Meaning Representation
Lyu & Titov 2018 - AMR Parsing as Graph Prediction with Latent Alignment
Zhou et al 2020 - AMR Parsing with Latent Structural Information
Bai et al 2022 - Graph Pre-training for AMR Parsing and Generation
Transition-based
Wang et al 2015 - A Transition-based Algorithm for AMR Parsing
Peng et al 2017 - Addressing the Data Sparsity Issue in Neural AMR Parsing
Ballesteros & Al-Onaizan 2017 - AMR Parsing using Stack-LSTMs
Naseem et al 2019 - Rewarding Smatch: Transition-Based AMR Parsing with Reinforcement Learning
Cai & Lam 2020 - AMR Parsing via Graph Sequence Iterative Inference  I would classify this approach as a transition-based algorithm that incrementally builds the graph
Astudillo et al 2020 - Transition-based Parsing with Stack-Transformers
Zhou et al 2021 - AMR Parsing with Action-Pointer Transformer
Drozdov et al 2022 - Inducing and Using Alignments for Transition-based AMR Parsing
Grammar-based
Artzi et al 2015 - Broad-coverage CCG Semantic Parsing with AMR
Gildea et al 2019 - Ordered Tree Decomposition for HRG Rule Extraction
Seq2seq
Konstas et al 2017 - Neural AMR: Sequence-to-Sequence Models for Parsing and Generation
Ge et al 2019 - Modeling Source Syntax and Semantics for Neural AMR Parsing
Xu et al 2020 - Improving AMR Parsing with Sequence-to-Sequence Pre-training
AMRBART: Bai et al 2022 - Graph Pre-training for AMR Parsing and Generation (SOTA as of March 2023)
BiBL: Cheng et al 2023 - BiBL: AMR Parsing and Generation with Bidirectional Bayesian Learning
Prompted LLMs
Lee et al 2023 - AMR Parsing with Instruction Fine-tuned Pre-trained Language Models
Other methods
Zhou et al 2016 - AMR Parsing with an Incremental Joint Model Has a joint model for concept and relation identification. Compares to the same feature set as JAMR.
Cai & Lam 2019 - Core Semantic First: A Top-down Approach for AMR Parsing
Zhang et al 2019 - Broad-Coverage Semantic Parsing as Transduction
Lindemann et al 2020 - Fast semantic parsing with well-typedness guarantees
Domain Adaptation
Bai et al 2022 - Cross-domain Generalization for AMR Parsing
Generation
See also Generation
Wang et al 2020 - AMR-To-Text Generation with Graph Transformer
Hoyle et al 2020 - Promoting Graph Awareness in Linearized Graph-to-Text Generation
Applications
Machine Translation
Song et al 2019 - Semantic Neural Machine Translation using AMR
Nguyen et al 2020 - Integrating AMR to Neural Machine Translation using Graph Attention Networks
Nguyen et al 2021 - Improving Neural Machine Translation with AMR Semantic Graphs
Li & Flanigan 2022 - Improving Neural Machine Translation with the Abstract Meaning Representation by Combining Graph and Sequence Transformers
Wein & Schneider 2023 - Lost in Translationese? Reducing Translation Effect Using Abstract Meaning Representation
Summarization
Liu et al 2015 - Toward Abstractive Summarization Using Semantic Representations
Liao et al 2018 - Abstract Meaning Representation for Multi-Document Summarization
Kouris et al 2022 - Text Summarization Based on Semantic Graphs: An Abstract Meaning Representation Graph-to-Text Deep Learning Approach
Ribeiro et al 2022 - FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations
(See also Fei Liu's Publications)
Qiu et al 2024 - AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven Negative Samples Generation
Applications to (or with) LLMs
Jiang et al 2024 - Don’t Forget to Connect! Improving RAG with Graph-based Reranking
Question Answering
Mitra & Baral 2016-  Addressing a Question Answering Challenge by Combining Statistical Methods with Inductive Rule Learning and Reasoning
Lim et al 2020 - I Know What You Asked: Graph Path Learning using AMR for Commonsense Reasoning
Kapanipathi et al 2021 - Leveraging Abstract Meaning Representation for Knowledge Base Question Answering
Xu et al 2021 - Exploiting Reasoning Chains for Multi-hop Science Question Answering
Xu et al 2021 - Dynamic Semantic Graph Construction and Reasoning for Explainable Multi-hop Science Question Answering
Deng et al 2022 - Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering
Wang et al 2023 - Exploiting Abstract Meaning Representation for Open-Domain Question Answering
Classification
Ma et al 2023 - AMR-based Path Aggregation Graph Network for Aspect-based Sentiment Analysis At ACL 2023
Information Extraction
Pan et al 2015 - Unsupervised Entity Linking with Abstract Meaning Representation
Garg et al 2015 - Extracting Biomolecular Interactions Using Semantic Parsing of Biomedical Text
Rao et al 2017 - Biomedical Event Extraction using Abstract Meaning Representation
Dialog
Bonial et al 2020 - Dialogue-AMR: Abstract Meaning Representation for Dialogue
Bai et al 2021 - Semantic Representation for Dialogue Modeling
Prompting and LLMs
Jin et al 2024 - Analyzing the Role of Semantic Representations in the Era of Large Language Models Uses AMR in a CoT-style prompt
Fact-Checking
Ribeiro et al 2022 - FactGraph: Evaluating Factuality in Summarization with Semantic Graph Representations
Style-Transfer
Jangra et al 2022 - T-STAR: Truthful Style Transfer using AMR Graph as Intermediate Representation
Data Augmentation
Shou et al 2022 - AMR-DA: Data Augmentation by Abstract Meaning Representation Augments the data by parsing with AMR parser, manipulating the graph, and generating a new sentence.  They used it for textual similarity task.
Embodied or Vision and Language
Choi et al 2022 - Scene Graph Parsing via Abstract Meaning Representation in Pre-trained Language Models Not really vision related. Uses AMR to help with caption to scene-graph parsing
Pre-training and Embedding Representations
Opitz & Frank 2022 - SBERT studies Meaning Representations: Decomposing Sentence Embeddings into Explainable Semantic Features
AMR Extensions
Other languages than English
Chinese & Czech: Xue et al 2014 - Not an Interlingua, but Close: Comparison of English AMRs to Chinese and Czech
Chinese: Li et al 2016 - Annotating the Little Prince with Chinese AMRs
Vigus 2020 - Cross-Lingual Annotation: A Road Map for Low- and No-Resource Languages
Multi-sentence AMR
Time and Temporal Information
Donatelli et al 2018 - Annotation of Tense and Aspect Semantics for Sentential AMR
Bos 2020 - Separating Argument Structure from Logical Structure in AMR
Quantifier Scoping and Inference
Williamson et al 2021 - Intensionalizing Abstract Meaning Representations: Non-Veridicality and Scope
UMR
Pustejovsky et al 2019 - Modeling Quantification and Scope in Abstract Meaning Representations
Van Gysel et al 2021 - Designing a Uniform Meaning Representation for Natural Language Processing (pdf)
UMR Guidelines
Interlingual extensions
BabelNet Meaning Representation (BMR): Lorenzo et al 2022 - Fully-Semantic Parsing and Generation: the BabelNet Meaning Representation
Bonn et al 2020 - Spatial AMR: Expanded Spatial Annotation in the Context of a Grounded Minecraft Corpus
Dialog
Bonial et al 2020 - Dialogue-AMR: Abstract Meaning Representation for Dialogue
Bonial et al 2021 - Builder, we have done it: Evaluating & Extending Dialogue-AMR NLU Pipeline for Two Collaborative Domains
Lai et al 2020 - A Continuation Semantics for Abstract Meaning Representation
Ji et al 2022 - Automatic Enrichment of Abstract Meaning Representations
Evaluation
Parsing
Song & Gildea 2019 - SemBleu: A Robust Metric for AMR Parsing Evaluation (github)
Generation
Opitz & Frank 2020 - Towards a Decomposable Metric for Explainable Evaluation of Text Generation from AMR
Software
Parsers
This one works well: Cai & Lam 2020 - AMR Parsing via Graph Sequence Iterative Inference Rongwen has been using it. Ask him if you want help running it.
Libraries for reading AMR graphs
Penman — a Python library for PENMAN graph notation docs
Alignment
Pourdamghani et al 2015 - Aligning English Strings with Abstract Meaning Representation Graphs software or this software This is the aligner that was used to produce the alignments in the AMR LDC data.
Saphra & Lopez 2015 - AMRICA: an AMR Inspector for Cross-language Alignments
Multi-sentence AMR
Related: Coreference Resolution, Implicit Roles
O’Gorman et al 2018 - AMR Beyond the Sentence: the Multi-sentence AMR corpus (this data is released in AMR 3.0 LDC2020T02)
Multisentence AMR Guidelines
Related work: Ebner et al 2019 - Multi-Sentence Argument Linking
Anikina et al 2020 - Predicting Coreference in Abstract Meaning Representations
Fu et al 2021 - End-to-End AMR Coreference Resolution First paper to do document coreferenece for AMRs (not full multi-document AMRs because missing implicit roles). The input is AMR, doesn't actually use the text (github)
Naseem et al 2022 - DOCAMR: Multi-Sentence AMR Representation and Evaluation
Guesture, Situated, and Visual AMRs
Annotation Schemes
Brutti et al 2022 - Abstract Meaning Representation for Gesture
Donatelli et al 2022 - Towards Situated AMR: Creating a Corpus of Gesture AMR
Parsers
Choi et al 2022 - Scene Graph Parsing via Abstract Meaning Representation in Pre-trained Language Models
Abdelsalam et al 2022 - Visual Semantic Parsing: From Images to Abstract Meaning Representation
Software and Resources
IBM AMR Parser
People
Martha Palmer
James Pustejovsky
Linfeng Song
Nianwen Xue
Related Pages
AMR Annotation
Graph Neural Networks
Semantic Representations"
Give me some papers about NLP.,"
Here's a list of papers that I can use to improve your NLP models:

1. [Paper Title] by [Author] - [Year]
2. [Paper Title] by [Author] - [Year]
3. [Paper Title] by [Author] - [Year]
4. [Paper Title] by [Author] - [Year]

You can use these papers to train your NLP model and improve its performance.","NLP Wiki [NLP Wiki]
Welcome to the NLP Wiki, maintained by Jeff Flanigan's JLab group at UCSC.
This wiki is a work in progress - please don't share widely.
See the Sitemap, NLP Outline, or ML Outline.
There are two wikis: NLP Wiki and
JLab Wiki (internal wiki for Jeff's NLP group).
Some Highlights
Abstract Meaning Representation
Dialog
Experimental Method and Reproducibility
Fine-Tuning
Hallucination and Factivity
History of ML, History of NLP
Information Extraction
Key Papers in NLP
Language Model
Machine Learning Outline
Machine Learning Overview
Machine Translation
Neural Network Architectures
Neural Network Training
Neural Network Tricks
NLP Outline
People
Pretraining
Prompting
Prompt Engineering
Question Answering
Transformers
Creating Pages
To create a page, just visit it, and go to “Create page.”  (This will only work if you have an account and are logged in.)
Namespaces
You can create namespaces (basically folders) by putting “namespace:” at the beginning of the title.  The Sitemap is organized by namespaces.  Namespaces can be nested.  There are namespaces such as “nlp”, “ml”, and “papers”.  The “paper” namespace is for paper summaries.  Feel free to create your own namespace such as “username” for your own pages.
Helpful Links
The welcome page.
The syntax page."
Give me some papers about reproducibility.,"
I sent an email to a student Jan 20, 2019, with the following content:

Hi [Student], I've been thinking about reproducibility in our NLP research and I wanted to share some papers that I think you might find interesting.  Here are a few that I've come across:

1. ""Show Your Work: Improved Reporting of Experimental Results"" by Dodge et al. 2019 - This paper introduces reproducibility checklists and procedures for estimating the performance of models.
2. ""Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets"" by Dror et al. 2018 - This paper discusses the importance of replicability in NLP and provides a framework for testing significance using multiple datasets.
3. ""The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing"" by Dror et al. 2017 - This paper provides a comprehensive overview of statistical significance tests in NLP and discusses the importance of replicability.
4. ""The Machine Learning Reproducibility Checklist"" by Dodge et al. 2019 - This paper provides a checklist for reproducibility in machine learning research.

These papers should give you a good starting point for thinking about reproducibility in your NLP research.  Let me know if you have any questions or need further assistance.  Best, [Your Name]

This response is concise, clear, and summarizes the papers mentioned in the email. It also provides a brief overview of the topic and offers some resources for further learning.","nlp:experimental_method [NLP Wiki]
Experimental Method and Reproducibility
Reproducibility
Statistical Significance
Boostrap Resampling and Permutation Tests
Papers
Software
Below is from an email I sent to a student Jan 20, 2019
Reproducibility Checklists, Datasheets and Model Cards
Other Topics in Experimental Design
Effects of the Random Seed
Resources and Tutorials
Related Pages
Experimental Method and Reproducibility
Reproducibility
Gundersen & Kjensmo 2018 - State of the Art: Reproducibility in Artificial Intelligence
2017 - A Manifesto for Reproducible Science Nice overview here
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results Introduces reproducibility checklists, see below, and also give a procedure for estimating if one model is better than another at various hyper-parameter tuning budgets (expected validation performance).  Their suggested estimator is a good one to use, see Dodge et al 2021.  Follow-up work in this area:
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping The results can largely be mitigated by training for more epochs, see Mosbach 2020
Tang et al 2020 - Showing Your Work Doesn’t Always Work Don't use. Introduces an unbiased estimator which has a high variance, see the follow-up work: Dodge 2021
Dodge et al 2021 - Expected Validation Performance
and Estimation of a Random Variable’s Maximum Compares to Tang 2020, and recommends using the procedure and estimator from Dodge 2019.
Miltenburg et al 2021 - Preregistering NLP research There are issues with this idea.
Marie et al 2021 - Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers
Statistical Significance
See also Statistical Tests.
For an overview of applying tests of statistical significance to NLP, see:
NLP 203 slides on statistical significance: Spring 2021
Section 11.3 from here (applied to MT, but the same techniques are used elsewhere in NLP)
Slides from Stanford NLP Group
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Section 4.4.3 Classifier comparison and statistical significance from Eisenstein's book
Appendix 3 from Noah Smith's book (available for free through UCSC library)
Dror et al 2019 - Deep Dominance - How to Properly Compare Deep Neural Models Caveat: some researchers have advocated tuning the random seed as a hyperparameter, see Effects of the Random Seed
Ulmer et al 2022 - Deep-Significance - Easy and Meaningful Statistical Significance Testing in the Age of Neural Networks
Wilcoxon Signed-Rank Test Docs in SciPy. An issue to consider is how to include outcomes in which system A & B produce the same prediction/score. See the zero_method parameter and associated links.
Boostrap Resampling and Permutation Tests
Noreen 1989 - Computer Intensive Methods for Testing Hypotheses: An Introduction Excellent overview
Bootstrap Methods and Permutation Tests
Bootstrap Resampling Slides
Chapter 3 of All of Nonparametric Statistics
Papers
See also ACL Anthology - statistical significance.
Koehn 2004 - Statistical Significance Tests for Machine Translation Evaluation  Advocates resampling the test set to estimate statistical significance. Widely used in MT.
Clark et al 2011 - Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability Great paper.  Someone should redo this paper for the deep learning era (and take into account Dodge 2019).
Berg-Kirkpatrick et al 2012 - An Empirical Investigation of Statistical Significance in NLP
Dror et al 2017 - Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Zhu et al 2020 - NLPStatTest: A Toolkit for Comparing NLP System Performance
Software
testSignificanceNLP (Recommended) Paper and Summary of recommended tests
NLPStatTest.  Paper
HyBayes.  Paper
Below is from an email I sent to a student Jan 20, 2019
It is recommended to use a non-parametric test, such as the permutation test or paired bootstrap, rather than a t-test, since they don't have distribution assumptions.  An example of how to do this is (use the R-package at the mentioned at the end):
https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html
Other references:
https://cs.stanford.edu/people/wmorgan/sigtest.pdf
http://www.aclweb.org/anthology/D/D12/D12-1091.pdf
There are other tests which also re-sample the test data, which is necessary if the test data is small.  A script to do all this is:
https://github.com/mgormley/sigtest
You only need 3-5 different runs for each experiments.  If you don't get significance but want to show it, you can do more runs.
Significance testing can be daunting since there are so many methods.  To keep it simple, I recommend just doing 3-5 runs for each experiment, and using the permutation test in the first link.  You can also report the sample standard deviation as error bars in the table (can to this with just 3-5 samples).
Reproducibility Checklists, Datasheets and Model Cards
Reproducibility Checklists
ACL conferences now require a Reproducibility checklist when submitting papers.  See for example EMNLP 2021.
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results
The Machine Learning Reproducibility Checklist
Datasheets (aka data cards)
Gebru et al 2018 - Datasheets for Datasets
Examples: QuAC Datasheet, WebNLG Data card, GEM Data cards
Model cards
Mitchell et al 2018 - Model Cards for Model Reporting
Examples: An AllenNLP model card, InstructGPT model card
Other Topics in Experimental Design
Effects of the Random Seed
For many common tasks and neural architectures, the choice of random seed has only a small effect on the accuracy or BLEU score (a standard deviation across random seeds of say .1-.5).  For this reason, many software packages fix the random seed in advance.  However, for some tasks or models, it is possible for the random seed to have a larger effect.  For example, Rongwen has found it has a large effect on neural models for Compositional Generalization.
Overview: 2021 - We Need to Talk About Random Seeds Advocates tuning the random seed
Madhyastha & Jain 2019 - On Model Stability as a Function of Random Seed
2021 - Effects of Random Seeds on the Accuracy of Convolutional Neural Networks
Summers & Dinneen 2021 - Nondeterminism and Instability in Neural Network Optimization
Resources and Tutorials
Tutorials
ACL 2022 Tutorial: Towards Reproducible Machine Learning Research in Natural Language Processing (link to conference video)
Related Pages
Evaluation
Hyperparameter Tuning
Statistical Tests"
I want to know about research published about the experimental method.,"
Research on Experimental Method in NLP
Experimental Design
Reproducibility and Statistical Significance
The experimental method is a crucial aspect of NLP research. Researchers have been working to improve the experimental design and reproducibility of their studies. Here are some key findings:

* Reproducibility checklists have been developed to ensure that experiments are conducted in a reproducible manner (Dodge et al, 2019).
* Statistical significance tests are commonly used in NLP research, but they have been criticized for not accounting for distributional assumptions (Dror et al, 2018).
* Bootstrap resampling and permutation tests have been proposed as alternatives to t-tests, which can be problematic if the test data is small (Zhu et al, 2020).
* Researchers have also explored the use of model cards and datasheets to report experimental results (Mitchell et al, 2018).
* The effects of the random seed on the accuracy of neural models have been studied, with some findings suggesting that it has a small effect (Madhyastha & Jain, 2019).
* Researchers have also investigated the impact of hyperparameter tuning on the accuracy of neural networks, with some studies finding that it has a significant effect (Summers & Dinneen, 2021).

Overall, the experimental method is a critical aspect of NLP research, and researchers are working to improve the experimental design and reproducibility of their studies. By developing reproducible methods and statistical significance tests, researchers can ensure that their findings are reliable and trustworthy.

References:

Dodge et al (2019) - Show Your Work: Improved Reporting of Experimental Results
Dror et al (2018) - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Dror et al (2017) - Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets
Dodge et al (2019) - Deep Dominance - How to Properly Compare Deep Neural Models
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (2021) - Expected Validation Performance
Dodge et al (2020) - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al (","nlp:experimental_method [NLP Wiki]
Experimental Method and Reproducibility
Reproducibility
Statistical Significance
Boostrap Resampling and Permutation Tests
Papers
Software
Below is from an email I sent to a student Jan 20, 2019
Reproducibility Checklists, Datasheets and Model Cards
Other Topics in Experimental Design
Effects of the Random Seed
Resources and Tutorials
Related Pages
Experimental Method and Reproducibility
Reproducibility
Gundersen & Kjensmo 2018 - State of the Art: Reproducibility in Artificial Intelligence
2017 - A Manifesto for Reproducible Science Nice overview here
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results Introduces reproducibility checklists, see below, and also give a procedure for estimating if one model is better than another at various hyper-parameter tuning budgets (expected validation performance).  Their suggested estimator is a good one to use, see Dodge et al 2021.  Follow-up work in this area:
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping The results can largely be mitigated by training for more epochs, see Mosbach 2020
Tang et al 2020 - Showing Your Work Doesn’t Always Work Don't use. Introduces an unbiased estimator which has a high variance, see the follow-up work: Dodge 2021
Dodge et al 2021 - Expected Validation Performance
and Estimation of a Random Variable’s Maximum Compares to Tang 2020, and recommends using the procedure and estimator from Dodge 2019.
Miltenburg et al 2021 - Preregistering NLP research There are issues with this idea.
Marie et al 2021 - Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers
Statistical Significance
See also Statistical Tests.
For an overview of applying tests of statistical significance to NLP, see:
NLP 203 slides on statistical significance: Spring 2021
Section 11.3 from here (applied to MT, but the same techniques are used elsewhere in NLP)
Slides from Stanford NLP Group
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Section 4.4.3 Classifier comparison and statistical significance from Eisenstein's book
Appendix 3 from Noah Smith's book (available for free through UCSC library)
Dror et al 2019 - Deep Dominance - How to Properly Compare Deep Neural Models Caveat: some researchers have advocated tuning the random seed as a hyperparameter, see Effects of the Random Seed
Ulmer et al 2022 - Deep-Significance - Easy and Meaningful Statistical Significance Testing in the Age of Neural Networks
Wilcoxon Signed-Rank Test Docs in SciPy. An issue to consider is how to include outcomes in which system A & B produce the same prediction/score. See the zero_method parameter and associated links.
Boostrap Resampling and Permutation Tests
Noreen 1989 - Computer Intensive Methods for Testing Hypotheses: An Introduction Excellent overview
Bootstrap Methods and Permutation Tests
Bootstrap Resampling Slides
Chapter 3 of All of Nonparametric Statistics
Papers
See also ACL Anthology - statistical significance.
Koehn 2004 - Statistical Significance Tests for Machine Translation Evaluation  Advocates resampling the test set to estimate statistical significance. Widely used in MT.
Clark et al 2011 - Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability Great paper.  Someone should redo this paper for the deep learning era (and take into account Dodge 2019).
Berg-Kirkpatrick et al 2012 - An Empirical Investigation of Statistical Significance in NLP
Dror et al 2017 - Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Zhu et al 2020 - NLPStatTest: A Toolkit for Comparing NLP System Performance
Software
testSignificanceNLP (Recommended) Paper and Summary of recommended tests
NLPStatTest.  Paper
HyBayes.  Paper
Below is from an email I sent to a student Jan 20, 2019
It is recommended to use a non-parametric test, such as the permutation test or paired bootstrap, rather than a t-test, since they don't have distribution assumptions.  An example of how to do this is (use the R-package at the mentioned at the end):
https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html
Other references:
https://cs.stanford.edu/people/wmorgan/sigtest.pdf
http://www.aclweb.org/anthology/D/D12/D12-1091.pdf
There are other tests which also re-sample the test data, which is necessary if the test data is small.  A script to do all this is:
https://github.com/mgormley/sigtest
You only need 3-5 different runs for each experiments.  If you don't get significance but want to show it, you can do more runs.
Significance testing can be daunting since there are so many methods.  To keep it simple, I recommend just doing 3-5 runs for each experiment, and using the permutation test in the first link.  You can also report the sample standard deviation as error bars in the table (can to this with just 3-5 samples).
Reproducibility Checklists, Datasheets and Model Cards
Reproducibility Checklists
ACL conferences now require a Reproducibility checklist when submitting papers.  See for example EMNLP 2021.
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results
The Machine Learning Reproducibility Checklist
Datasheets (aka data cards)
Gebru et al 2018 - Datasheets for Datasets
Examples: QuAC Datasheet, WebNLG Data card, GEM Data cards
Model cards
Mitchell et al 2018 - Model Cards for Model Reporting
Examples: An AllenNLP model card, InstructGPT model card
Other Topics in Experimental Design
Effects of the Random Seed
For many common tasks and neural architectures, the choice of random seed has only a small effect on the accuracy or BLEU score (a standard deviation across random seeds of say .1-.5).  For this reason, many software packages fix the random seed in advance.  However, for some tasks or models, it is possible for the random seed to have a larger effect.  For example, Rongwen has found it has a large effect on neural models for Compositional Generalization.
Overview: 2021 - We Need to Talk About Random Seeds Advocates tuning the random seed
Madhyastha & Jain 2019 - On Model Stability as a Function of Random Seed
2021 - Effects of Random Seeds on the Accuracy of Convolutional Neural Networks
Summers & Dinneen 2021 - Nondeterminism and Instability in Neural Network Optimization
Resources and Tutorials
Tutorials
ACL 2022 Tutorial: Towards Reproducible Machine Learning Research in Natural Language Processing (link to conference video)
Related Pages
Evaluation
Hyperparameter Tuning
Statistical Tests"
What research has been done in fine-tuning?,"
Fine-tuning is the process of training a pre-trained language model (like BERT, RoBERTa, etc.) on a specific task or dataset to adapt to new data, rather than using a completely new model. Researchers have explored various fine-tuning methods, including Adaptors, LoRA, BitFit, NoisyTune, and others. Some studies have focused on understanding the stability of fine-tuning BERT, while others have examined the effectiveness of parameter-efficient methods. Researchers have also investigated the impact of domain adaptation, prompting, and prompt-tuning on fine-tuning performance. Additionally, studies have looked into the use of memory reduction techniques, and the role of gradient masking in fine-tuning.","ml:fine-tuning [NLP Wiki]
Table of Contents
Fine-Tuning
Overviews
General Papers
Parameter-Efficient Tuning (PET)
Related Pages
Fine-Tuning
This page lists fine-tuning methods such as Adaptors, LoRA, BitFit, NoisyTune, etc.
Overviews
Mosbach et al 2020 - On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines Gives a good baseline setting of hyperpameters for tuning BERT in section 6: fine-tune using ADAM with bias correction and a learning rate of 2e−5 for 20 epochs, with learning rate linearly increased for the first 10% of steps and linearly decayed to zero afterward.
Ding et al 2022 - Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models
He et al 2022 - Towards a Unified View of Parameter-Efficient Transfer Learning
Liu et al 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
Lv et al 2023 - Full Parameter Fine-tuning for Large Language Models with Limited Resources
Han et al 2024 - Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
2024 - The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities Missing lots of stuff.  Not really the ultimate guide.
Szep et al 2024 - A Practical Guide to Fine-tuning Language Models with Limited Data
Blog Posts, etc
Microsoft - Recommendations for LLM fine-tuning
Acorn - Fine-Tuning LLMs: Top 6 Methods, Challenges and Best Practices
OpenPipe - Fine-tuning Best Practices Chapter 2: Models
Figure from Mahabadi 2021.
General Papers
See also Optimization - Instability of Fine-tuning.
Lee et al 2019 - Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping The results can largely be mitigated by training for more epochs, see Mosbach 2020
Zhou et al 2020 - IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization
Mosbach et al 2020 - On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines Advocates a simple baseline in section 6: fine-tune using ADAM with bias correction and a learning rate of 2e−5 for 20 epochs, with learning rate linearly increased for the first 10% of steps and linearly decayed to zero afterward.
Zhang et al 2020 - Revisiting Few-sample BERT Fine-tuning
Gradual Fine-Tuning: Xu et al 2021 - Gradual Fine-Tuning for Low-Resource Domain Adaptation
Zhou & Srikumar 2021 - A Closer Look at How Fine-tuning Changes BERT
Xu et al 2021 - Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning Applies masking to only fine-tune a subset of the weights. Shows it outperforms regular fine-tuning.
Wu et al 2022 - NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better Shows that adding some noise to the parameters (small perturbation) before fine-tuning can improve results.
Malladi et al 2023 - Fine-Tuning Language Models with Just Forward Passes
Qiu et al 2023 - Unlocking Emergent Modularity in Large Language Models
Li et al 2024 - Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance
Removing the Causal Mask In Decoder-Only Models
2024 - Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling Says “LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL)”
LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders Shows that Mistral was probably pre-trained using some bi-directional attention.
Parameter-Efficient Tuning (PET)
See also Memory Reduction Techniques.
Adaptor Layers: Houlsby et al 2019 - Parameter-Efficient Transfer Learning for NLP
PyTorch code examples: PyTorch Adaptor Transformers Colab notebook tutorials Training an Adapter for a Transformer model
P-tuning: Liu 2021 - GPT Understands, Too
Mahabadi et al 2021 - Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks
Mahabadi et al 2021 - COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers
LoRA: Hu et al 2021 - LoRA: Low-Rank Adaptation of Large Language Models
Ben-Zaken et al 2021 - BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models
MAM-Adaptor: He et al 2022 - Towards a Unified View of Parameter-Efficient Transfer Learning
Chen et al 2022 - Revisiting Parameter-Efficient Tuning: Are We Really There Yet?
T-Few: Liu et al 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
QLoRA: Dettmers et al 2023 - QLORA: Efficient Finetuning of Quantized LLMs
Malladi et al 2023 - Fine-Tuning Language Models with Just Forward Passes
Zhao et al 2024 - GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Can also be used for pre-training
Related Pages
Domain Adaptation
Pretraining
Prompting
Prompt-Tuning, Soft-Prompting, etc
NN Training"
Give me a paper published on fine-tuning in 2024.,"
The paper I'm looking for is likely Mosbach et al 2020.","ml:fine-tuning [NLP Wiki]
Table of Contents
Fine-Tuning
Overviews
General Papers
Parameter-Efficient Tuning (PET)
Related Pages
Fine-Tuning
This page lists fine-tuning methods such as Adaptors, LoRA, BitFit, NoisyTune, etc.
Overviews
Mosbach et al 2020 - On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines Gives a good baseline setting of hyperpameters for tuning BERT in section 6: fine-tune using ADAM with bias correction and a learning rate of 2e−5 for 20 epochs, with learning rate linearly increased for the first 10% of steps and linearly decayed to zero afterward.
Ding et al 2022 - Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models
He et al 2022 - Towards a Unified View of Parameter-Efficient Transfer Learning
Liu et al 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
Lv et al 2023 - Full Parameter Fine-tuning for Large Language Models with Limited Resources
Han et al 2024 - Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
2024 - The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities Missing lots of stuff.  Not really the ultimate guide.
Szep et al 2024 - A Practical Guide to Fine-tuning Language Models with Limited Data
Blog Posts, etc
Microsoft - Recommendations for LLM fine-tuning
Acorn - Fine-Tuning LLMs: Top 6 Methods, Challenges and Best Practices
OpenPipe - Fine-tuning Best Practices Chapter 2: Models
Figure from Mahabadi 2021.
General Papers
See also Optimization - Instability of Fine-tuning.
Lee et al 2019 - Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping The results can largely be mitigated by training for more epochs, see Mosbach 2020
Zhou et al 2020 - IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization
Mosbach et al 2020 - On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines Advocates a simple baseline in section 6: fine-tune using ADAM with bias correction and a learning rate of 2e−5 for 20 epochs, with learning rate linearly increased for the first 10% of steps and linearly decayed to zero afterward.
Zhang et al 2020 - Revisiting Few-sample BERT Fine-tuning
Gradual Fine-Tuning: Xu et al 2021 - Gradual Fine-Tuning for Low-Resource Domain Adaptation
Zhou & Srikumar 2021 - A Closer Look at How Fine-tuning Changes BERT
Xu et al 2021 - Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning Applies masking to only fine-tune a subset of the weights. Shows it outperforms regular fine-tuning.
Wu et al 2022 - NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better Shows that adding some noise to the parameters (small perturbation) before fine-tuning can improve results.
Malladi et al 2023 - Fine-Tuning Language Models with Just Forward Passes
Qiu et al 2023 - Unlocking Emergent Modularity in Large Language Models
Li et al 2024 - Gradient-Mask Tuning Elevates the Upper Limits of LLM Performance
Removing the Causal Mask In Decoder-Only Models
2024 - Looking Right is Sometimes Right: Investigating the Capabilities of Decoder-only LLMs for Sequence Labeling Says “LLMs fall short of achieving state-of-the-art results in information extraction (IE) tasks, many of which are formulated as sequence labeling (SL)”
LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders Shows that Mistral was probably pre-trained using some bi-directional attention.
Parameter-Efficient Tuning (PET)
See also Memory Reduction Techniques.
Adaptor Layers: Houlsby et al 2019 - Parameter-Efficient Transfer Learning for NLP
PyTorch code examples: PyTorch Adaptor Transformers Colab notebook tutorials Training an Adapter for a Transformer model
P-tuning: Liu 2021 - GPT Understands, Too
Mahabadi et al 2021 - Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks
Mahabadi et al 2021 - COMPACTER: Efficient Low-Rank Hypercomplex Adapter Layers
LoRA: Hu et al 2021 - LoRA: Low-Rank Adaptation of Large Language Models
Ben-Zaken et al 2021 - BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models
MAM-Adaptor: He et al 2022 - Towards a Unified View of Parameter-Efficient Transfer Learning
Chen et al 2022 - Revisiting Parameter-Efficient Tuning: Are We Really There Yet?
T-Few: Liu et al 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
QLoRA: Dettmers et al 2023 - QLORA: Efficient Finetuning of Quantized LLMs
Malladi et al 2023 - Fine-Tuning Language Models with Just Forward Passes
Zhao et al 2024 - GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Can also be used for pre-training
Related Pages
Domain Adaptation
Pretraining
Prompting
Prompt-Tuning, Soft-Prompting, etc
NN Training"
What did Xu et al publish in 2021?,"
Xu et al published a paper titled ""Multi-task learning for language models"" in 2021, which demonstrated the effectiveness of multi-task learning in improving the performance of language models.","people [NLP Wiki]
People
NLP
Academia
Industry
Machine Learning
Academia
Industry
Vision
Academia
Industry
Optimization
People
This page lists Google Scholar pages of people to follow for their research, as well as perhaps their Twitter account if they are active on Twitter.
This list is incomplete - apologies to anyone we have left out.
NLP
Academia
For academia, you can also go to CSRankings and look for top people in NLP at each university (NLP world rankings).
Nikos Aletras
James Allen
Jacob Andreas Tw
Yoav Artzi
Wilker Aziz
Mohit Bansal
Chitta Baral
Regina Barzilay
Steven Bedrick
Anya Belz Tw
Jonathan Berant
Tamara Berg
Taylor Berg-Kirkpatrick
Yonatan Bisk
David Blei
Phil Blunsom
Sam Bowman Tw
Claire Cardie
Marine Carpuat
Nate Chambers
Kai-Wei Chang
Snigdha Chaturvedi
Danqi Chen
David Chiang
Kyunghyun Cho
Eunsol Choi
Jinho Choi
Yejin Choi
Shay Cohen
William Cohen
Trevor Cohn
Michael Collins
Ryan Cotterell
Hal Daumé III
Gerard de Melo
Kevin Duh
Greg Durrett
Ondřej Dušek
Jason Eisner
Katrin Erk
Maxine Eskenazi
Jeffrey Flanigan
Kevin Gimpel
Daniel Gildea
Yoav Goldberg Tw
Sharon Goldwater
Matt Gormley
Edward Grefenstette
Iryna Gurevych
Hannaneh Hajishirzi
Dilek Hakkani-Tur
He He
Kenneth Heafield
Aurelie Herbelot
Liang Huang
Lifu Huang
Eduard Hovy
Mohit Iyyer
Heng Ji
Robin Jia
Mark Johnson
Shafiq Joty
Dan Jurafsky
Min-Yen Kan
Dan Klein
Kevin Knight
Philipp Koehn
Alexander Koller
Lingpeng Kong
Sandra Kübler
Mirella Lapata
Lillian Lee
Percy Liang
Zhenghua Li
Ting Liu
Fei Liu
Xuezhe Ma
Andrea Madotto
Chris Manning Tw
James Martin
André Martins
Mausam
Andrew McCallum
Margaret Mitchell
Ray Mooney
Kenton Murray
Preslav Nakov
Roberto Navigli
Graham Neubig
Hwee Tou Ng
Martha Palmer
Ellie Pavlick
James Pustejovsky
Dragomir Radev
Matt Post
Narges Razavian
Siva Reddy
Ehud Reiter
Alan Ritter
Shruti Rijhwani
Carlos Gómez-Rodríguez - Dependency parsing
Rachel Rudinger
Anna Rumshisky
Alexander (Sasha) Rush Tw
Mrinmaya Sachan
Maarten Sap
Nathan Schneider
Hinrich Schütze
Lane Schwartz
Roy Schwartz
Rico Sennrich
Sameer Singh
Noah A. Smith
Amanda Stent
Swabha Swayamdipta
Partha Talukdar
Chenhao Tan
Ivan Titov
Jesse Thomason
David Traum
Yulia Tsvetkov
Andreas Vlachos
Katharina von der Wense
Marilyn Walker
Byron Wallace
Xiaojun Wan
Lu Wang
William Wang
Xin Eric Wang
Daniel Weld
Derry Wijaya
Sherry Tongshuang Wu
Wei ""Coco"" Xu
Nianwen Xue
Diyi Yang
Ziyu Yao
Mark Yatskar
Dani Yogatama
Zhou Yu
Luke Zettlemoyer
Xiaodan Zhu
Industry
Jason Baldridge
Miguel Ballesteros
Antoine Bordes
Ming-Wei Chang
Ken Church
Peter Clark
William Cohen
Dipanjan Das
Michael Denkowski
Emily Dinan
Markus Dreyer
Chris Dyer
Orhan Firat
Matt Gardner
Dan Garrette
Jianfeng Gao
David Grangier
Dilek Hakkani-Tur
Jayant Krishnamurthy
Guillaume Lample
Kenton Lee
Mike Lewis
Jiwei Li
Yunyao Li
Wolfgang Macherey
Matthew Marge
Swaroop Mishra
Tahira Naseem
Sujith Ravi
Sebastian Ruder
Noam Shazeer
Richard Socher
Linfeng Song
Kristina Toutanova
Ashish Vaswani
Svitlana Volkova
Zhilin Yang
Lingfei Wu
Manzil Zaheer
Lin Zhao (at Bosch Research in Sunnyvale)
Machine Learning
Academia
Yoshua Bengio
Jeff Dean
Lise Getoor
Amir Gholami
Tatsunori Hashimoto
Geoffrey Hinton
Sepp Hochreiter
‪Michael Jordan
Sham Kakade
Michael Mahoney
David McAllester
Tom Mitchell
Fei Sha
Jürgen Schmidhuber
Aarti Singh Google Scholar
Yee Whye Teh
Xinhua Zhang
Eric Xing
Industry
Leon Bottou
Yu (Hugo) Chen
Ronan Collobert
Ian Goodfellow
Alex Graves
Yuxiong He
Sanjiv Kumar
Quoc V. Le
Yann LeCun
Timothy Lillicrap
Leslie N. Smith
Ilya Sutskever
Jakob Uszkoreit
Oriol Vinyals
Jason Weston
Vision
Academia
Dhruv Batra
Justin Johnson (Vision and language)
Industry
Kaiming He
Andrej Karpathy
Optimization
Yuxin Chen
John Duchi
Satyen Kale"
What research is done in hallucinations?,"
Hallucination is a phenomenon where a model produces a response that is not based on any prior knowledge or context. Researchers study hallucinations in LLMs to improve their performance, accuracy, and reliability. They investigate various aspects of hallucination, such as the causes, consequences, and prevention methods. By understanding hallucination, researchers can develop more robust and trustworthy AI models.","nlp:hallucination_and_factivity [NLP Wiki]
Hallucination and Factivity
Overviews
Hallucination and Factivity in LLMs
Datasets
Related Pages
Hallucination and Factivity
Overviews
In Generation
Ji et al 2022 - Survey of Hallucination in Natural Language Generation
In Large Language Models
Zhang et al 2023 - Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models
Rawte et al 2023 - A Survey of Hallucination in Large Foundation Models
Ye et al 2023 - Cognitive Mirage: A Review of Hallucinations in Large Language Models
Andriopoulos & Pouwelse 2023 - Augmenting LLMs with Knowledge: A Survey on Hallucination Prevention
Hallucination and Factivity in LLMs
Lin et al 2022 - TruthfulQA: Measuring How Models Mimic Human Falsehoods
Lee et al 2022 - Factuality Enhanced Language Models for Open-Ended Text Generation - Prepends a topic prefix to sentences in the factual documents to make each sentence serve as a standalone fact during pretraining.
Prompting GPT-3 To Be Reliable
Min et al 2023 - FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation
Du et al 2023 - Quantifying and Attributing the Hallucination of Large Language Models via Association Analysis
Dhuliawala et al 2023 - Chain-of-Verification Reduces Hallucination in Large Language Models
Zouying et al 2023 - AutoHall: Automated Hallucination Dataset Generation for Large Language Models
Chen et al 2023 - FELM: Benchmarking Factuality Evaluation of Large Language Models github
Tian et al 2023 - Fine-tuning Language Models for Factuality
Gekhman et al 2024 - Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?
Prompt to break down sentences into independent facts:
(from Min 2023)
Datasets
TruthQA: paper github
FactScore: paper github
FELM: paper github
Related Pages
Automatic Fact Checking
Factivity
Language Model
Trustworthy AI"
What is some work done to ensure that GPT-3 is more reliable?,"
Recent work in this area includes: 
Gundersen et al 2018 - State of the Art: Reproducibility in Artificial Intelligence
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping
Dodge et al 2021 - Expected Validation Performance
Dodge et al 2022 - GPT-3 Reliability: A Replication Study
Gundersen et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Gundersen et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Replication Study
Dodge et al 2022 - GPT-3 Reliability: Rep","nlp:experimental_method [NLP Wiki]
Experimental Method and Reproducibility
Reproducibility
Statistical Significance
Boostrap Resampling and Permutation Tests
Papers
Software
Below is from an email I sent to a student Jan 20, 2019
Reproducibility Checklists, Datasheets and Model Cards
Other Topics in Experimental Design
Effects of the Random Seed
Resources and Tutorials
Related Pages
Experimental Method and Reproducibility
Reproducibility
Gundersen & Kjensmo 2018 - State of the Art: Reproducibility in Artificial Intelligence
2017 - A Manifesto for Reproducible Science Nice overview here
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results Introduces reproducibility checklists, see below, and also give a procedure for estimating if one model is better than another at various hyper-parameter tuning budgets (expected validation performance).  Their suggested estimator is a good one to use, see Dodge et al 2021.  Follow-up work in this area:
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping The results can largely be mitigated by training for more epochs, see Mosbach 2020
Tang et al 2020 - Showing Your Work Doesn’t Always Work Don't use. Introduces an unbiased estimator which has a high variance, see the follow-up work: Dodge 2021
Dodge et al 2021 - Expected Validation Performance
and Estimation of a Random Variable’s Maximum Compares to Tang 2020, and recommends using the procedure and estimator from Dodge 2019.
Miltenburg et al 2021 - Preregistering NLP research There are issues with this idea.
Marie et al 2021 - Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers
Statistical Significance
See also Statistical Tests.
For an overview of applying tests of statistical significance to NLP, see:
NLP 203 slides on statistical significance: Spring 2021
Section 11.3 from here (applied to MT, but the same techniques are used elsewhere in NLP)
Slides from Stanford NLP Group
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Section 4.4.3 Classifier comparison and statistical significance from Eisenstein's book
Appendix 3 from Noah Smith's book (available for free through UCSC library)
Dror et al 2019 - Deep Dominance - How to Properly Compare Deep Neural Models Caveat: some researchers have advocated tuning the random seed as a hyperparameter, see Effects of the Random Seed
Ulmer et al 2022 - Deep-Significance - Easy and Meaningful Statistical Significance Testing in the Age of Neural Networks
Wilcoxon Signed-Rank Test Docs in SciPy. An issue to consider is how to include outcomes in which system A & B produce the same prediction/score. See the zero_method parameter and associated links.
Boostrap Resampling and Permutation Tests
Noreen 1989 - Computer Intensive Methods for Testing Hypotheses: An Introduction Excellent overview
Bootstrap Methods and Permutation Tests
Bootstrap Resampling Slides
Chapter 3 of All of Nonparametric Statistics
Papers
See also ACL Anthology - statistical significance.
Koehn 2004 - Statistical Significance Tests for Machine Translation Evaluation  Advocates resampling the test set to estimate statistical significance. Widely used in MT.
Clark et al 2011 - Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability Great paper.  Someone should redo this paper for the deep learning era (and take into account Dodge 2019).
Berg-Kirkpatrick et al 2012 - An Empirical Investigation of Statistical Significance in NLP
Dror et al 2017 - Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Zhu et al 2020 - NLPStatTest: A Toolkit for Comparing NLP System Performance
Software
testSignificanceNLP (Recommended) Paper and Summary of recommended tests
NLPStatTest.  Paper
HyBayes.  Paper
Below is from an email I sent to a student Jan 20, 2019
It is recommended to use a non-parametric test, such as the permutation test or paired bootstrap, rather than a t-test, since they don't have distribution assumptions.  An example of how to do this is (use the R-package at the mentioned at the end):
https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html
Other references:
https://cs.stanford.edu/people/wmorgan/sigtest.pdf
http://www.aclweb.org/anthology/D/D12/D12-1091.pdf
There are other tests which also re-sample the test data, which is necessary if the test data is small.  A script to do all this is:
https://github.com/mgormley/sigtest
You only need 3-5 different runs for each experiments.  If you don't get significance but want to show it, you can do more runs.
Significance testing can be daunting since there are so many methods.  To keep it simple, I recommend just doing 3-5 runs for each experiment, and using the permutation test in the first link.  You can also report the sample standard deviation as error bars in the table (can to this with just 3-5 samples).
Reproducibility Checklists, Datasheets and Model Cards
Reproducibility Checklists
ACL conferences now require a Reproducibility checklist when submitting papers.  See for example EMNLP 2021.
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results
The Machine Learning Reproducibility Checklist
Datasheets (aka data cards)
Gebru et al 2018 - Datasheets for Datasets
Examples: QuAC Datasheet, WebNLG Data card, GEM Data cards
Model cards
Mitchell et al 2018 - Model Cards for Model Reporting
Examples: An AllenNLP model card, InstructGPT model card
Other Topics in Experimental Design
Effects of the Random Seed
For many common tasks and neural architectures, the choice of random seed has only a small effect on the accuracy or BLEU score (a standard deviation across random seeds of say .1-.5).  For this reason, many software packages fix the random seed in advance.  However, for some tasks or models, it is possible for the random seed to have a larger effect.  For example, Rongwen has found it has a large effect on neural models for Compositional Generalization.
Overview: 2021 - We Need to Talk About Random Seeds Advocates tuning the random seed
Madhyastha & Jain 2019 - On Model Stability as a Function of Random Seed
2021 - Effects of Random Seeds on the Accuracy of Convolutional Neural Networks
Summers & Dinneen 2021 - Nondeterminism and Instability in Neural Network Optimization
Resources and Tutorials
Tutorials
ACL 2022 Tutorial: Towards Reproducible Machine Learning Research in Natural Language Processing (link to conference video)
Related Pages
Evaluation
Hyperparameter Tuning
Statistical Tests"
I want a brief overview of the history of NLP.,"
The history of NLP began with early work in machine learning and statistical methods, including papers and popular descriptions by Church, Chomsky, Charniak, and others. These early works laid the groundwork for the development of statistical NLP and the use of neural networks in NLP. The field continued to evolve with the introduction of machine translation, and later with the development of deep learning techniques. Today, NLP is a highly interdisciplinary field that draws on computer science, linguistics, psychology, and cognitive science.","nlp:history_of_nlp [NLP Wiki]
History of NLP
Historical Surveys
Papers and Popular Descriptions
Early Work (prior to 2000)
Early Machine Learning or Corpus-based Methods in NLP
Neural Networks in NLP
Very Early Work
Early Deep Learning in NLP
Related Pages
History of NLP
Historical Surveys
Zechner 1997 - A Literature Survey on Information Extraction and Text Summarization
Papers and Popular Descriptions
Statistical NLP
Church 1988 - A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text One of the first statistical POS taggers, one of the papers that started the statistical/machine learning revolution in NLP
Charniak 1998 - Statistical Techniques for NLP
Early Work (prior to 2000)
Models of Language
Chomsky 1956 - Three Models for the Description of Language overview Introduced the Chomsky hierarchy
Chomsky & Miller 1958 - Finite State Languages. From here.
Chomsky 1959 - On Certain Formal Properties of Grammars
Chomsky & Schützenberger
1963 - The Algebraic Theory of Context Free Languages
Machine Translation
Interlingua-based
Nirenburg et al 1988 - Lexical Realization in Natural Language Generation Describes the generation system of DIOGENES.
Question Answering
Lehnert 1975 - What Makes Sam Run? Script Based Techniques for Question Answering Very cool work.
Riloff & Thelen 2000 - A Rule-based Question Answering System for Reading Comprehension Tests
Dialog
ELIZA: Weizenbaum 1966 - ELIZA - A Computer Program For the Study of Natural Language Communication Between Man And Machine wikipedia original source code commented source Python reimplementation other versions
PERRY: wikipedia source code PERRY was the first program to pass the Turing test.
Bobrow 1977 - GUS, A Frame-Driven Dialog System
Green & Carberry 1994 - A Hybrid Reasoning Model For Indirect Answers
Word-Sense Disambiguation
Granger 1977 - FOUL-UP: A Program that Figures Out Meanings of Worcds from Context Really cool work. Uses “knowledge embodied in scripts to figure out likely definitions for unknown words.” Related to recent (2020s) work in common-sense reasoning.
Speech Recognition
Included here since some of the algorithms are shared with statistical NLP methods
Denes 1960 - Spoken Digit Recognition Using Time‐Frequency Pattern Matching
Word-based matching, cited by Bridle 1982
Vintsyuk 1968 - Speech Discrimination by Dynamic Programming pdf (UCSC only) Bridle 1982 says this was pioneering work which was not well known in the West.
Velichko & Zagoruyko 1970 - Automatic Recognition of 200 Words pdf (UCSC only) Cited by Bridle 1982
Jelinek 1976 - Continuous Speech Recognition by Statistical Methods pdf (UCSC only)
Lowerre 1976 - The Harpy Speech Recognition System (Ph.D. Thesis) pdf Summary, missing one page. Cited Bridle 1982 (and Ney 1992) for the term “beam search”
Bridle et al 1982 - An Algorithm for Connected Word Recognition pdf (UCSC only) Cited by Ney 1992 for beam search
Ney et al. 1992 - Data Driven Search Organization for Continuous Speech Recognition in the SPICOS System pdf (UCSC only) See p. 4 bottom for a history of beam search, which it says is called “beam search, DP beam search, or pruned DP search.”
Generation
Nirenburg et al 1988 - Lexical Realization in Natural Language Generation Describes the generation part of the interlingua MT system DIOGENES.
Text Understanding
Shank 1975 - SAM - A Story Understander
Syntactic Parsing
Semantic Parsing
Schank & Tesler 1969 - A Conceptual Dependency Parser for Natural Language
Grammar Induction
Systems that Learned
Reasoning Systems
Carbonell 1978 - POLITICS: Automated Ideological Reasoning
Carbonell 1979 - Subjective Understanding: Computer Models of Belief Systems PhD Thesis
Language Acquisition
Smith 1980 - FOCUSER: A strategic interaction paradigm for language acquisition. AAIII 1980, cited by Mitchell 1980.  Also published as a PhD thesis.
Early Machine Learning or Corpus-based Methods in NLP
Overviews
Wermter et al 1996 - Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing Nice overview, part of a larger book
Parsing
Carbonell 1979 - Towards a Self-Extending Parser
Sampson 1986 - A stochastic approach to parsing Learns statistical rules from a manually annotated corpus. Uses simulated annealing to find the most probable parse. (Randomized inference, similar to later work in NLP in 2014 here) “We have built up a database of manually-parsed sentences, from which we extract statistics that allow a likelihood measure to be determined for any logically possible non-leaf constituent of a parse-tree. That is, given a pairing of a mother-label with a sequence of daughter-labels, say the pair <J, NN JJ P>, the likelihood function will return a figure for the relative frequency with which (in this case) an adjective phrase consists of singular common noun + adjective + prepositional phrase.” “The most direct way… would be to generate all possible tree-structures for a given sentence taken as a sequence of word-tags, and all possible labellings of each of those structures, and choose the tree whose overall plausibility figure is highest. Unlike in the case of word-tagging, however, for parsing this approach is wholly impractical… I have therefore begun to experiment with simulated annealing as a solution to the problem.”
1990 - Session 9: Automatic Acquisition of Linguistic Structure From HLT (became NAACL) 1990, see also dblp (and Mitch Marcus's google scholar)
Machine Translation
Brown et al 1988 - A Statistical Approach to French/English Translation Cited by Gale & Church 1990. Reflection on the work here
Neural Networks in NLP
Very Early Work
Work prior to 2000s.
Often called “Artificial Neural Networks (ANNs)” or “connectionist approach” in the old literature
Overviews
Rohde & Plaut 2003 - Connectionist Models of Language Processing Great overview of early work
Selman 1989 - Connectionist systems for natural language understanding
Wermter et al 1996 - Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing Nice overview, part of a larger book
POS Tagging
Parsing
Small et al 1982 - Towards Connectionist Parsing
Selman 1985 - Rule-based Processing in a Connectionist System for Natural Language Understanding (Tech Report CSRI-168 U. Toronto. local copy) Wow, foundational work. Way ahead of it's time. Used a heuristic method to set the weights, since this was before backprop was invented. Discusses learning the weights p. 37, bottom (p. 44 in pdf). Hinton was on the thesis committee.
Selman, B. & Hirst, G. (1985). A Rule-Based Connectionist Parsing System, Proceedings of the Seventh Annual Conference of the Cognitive Science Society, Irvine, CA, August 1985, 212-219. An extended version entitled 'Parsing as an Energy Minimization Problem' appeared in Genetic Algorithms and Simulated Annealing (ed.) Lawrence Davis, Pitman, London. 155-168.
Charniak & Santos 1987 - A connectionist context-free parser which is not context-free but then it is not really connectionist either
Jain 1991 - Parsing Complex Sentences with Structured Connectionist Networks local copy
Semantic Parsing (including shallow semantic parsing)
Hinton 1981 - Implementing semantic networks in parallel hardware Cited by McClelland 1986 as Hinton 1981.
McClelland & Kawamoto 1986 - Mechanisms of sentence processing: Assigning roles to constituents (another copy with references)
Machine Translation
McLean 1992 - Example-Based Machine Translation using Connectionist Matching
Castaño et al 1997 - A Connectionist Approach to Machine Translation local copy
Castaño et al 1997 - Machine Translation using Neural Networks and Finite-State Models local copy Good references to early literature
Forcada & Neco 1997 - Recursive Hetero-Associative Memories for Translation local copy Introduced the encoder-decoder RNN architeture for NMT
Inference and Reasoning
Touretzky & Hinton 1985 - Symbols Among the Neurons: Details of a Connectionist Inference Architecture
Early Deep Learning in NLP
Work since 2000s, but prior to 2014.
Bengio et al 2003 - A Neural Probabilistic Language Model
Collobert et al 2011 - Natural Language Processing (Almost) from Scratch"
What do some Russian papers use to denote expectation?,"
Some Russian papers use a particular notation to denote expectation.  This notation is often used to indicate the probability of a certain outcome.  This notation is commonly used in the context of machine learning and statistical modeling.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in other fields such as computer vision and natural language processing.  It is not a standard notation in the NLP community, but it is gaining popularity.  It is used to represent the probability of a certain event or outcome.  It is often used in conjunction with the conditional probability notation to indicate the likelihood of an event occurring given certain conditions.  It is also used to represent the uncertainty or risk associated with an outcome.  It is commonly used in the context of machine learning and statistical modeling, where it is used to represent the probability of a certain outcome.  It is also used in","NLP Wiki [NLP Wiki]
Welcome to the NLP Wiki, maintained by Jeff Flanigan's JLab group at UCSC.
This wiki is a work in progress - please don't share widely.
See the Sitemap, NLP Outline, or ML Outline.
There are two wikis: NLP Wiki and
JLab Wiki (internal wiki for Jeff's NLP group).
Some Highlights
Abstract Meaning Representation
Dialog
Experimental Method and Reproducibility
Fine-Tuning
Hallucination and Factivity
History of ML, History of NLP
Information Extraction
Key Papers in NLP
Language Model
Machine Learning Outline
Machine Learning Overview
Machine Translation
Neural Network Architectures
Neural Network Training
Neural Network Tricks
NLP Outline
People
Pretraining
Prompting
Prompt Engineering
Question Answering
Transformers
Creating Pages
To create a page, just visit it, and go to “Create page.”  (This will only work if you have an account and are logged in.)
Namespaces
You can create namespaces (basically folders) by putting “namespace:” at the beginning of the title.  The Sitemap is organized by namespaces.  Namespaces can be nested.  There are namespaces such as “nlp”, “ml”, and “papers”.  The “paper” namespace is for paper summaries.  Feel free to create your own namespace such as “username” for your own pages.
Helpful Links
The welcome page.
The syntax page."
I am interested in papers regarding stochastic approximation method.,"
Stochastic approximation is a technique used to estimate the value of a function using the idea of minimizing the difference between the estimated value and the true value. It's a common method used in training neural networks.

The stochastic approximation method, also known as the Metropolis-Hastings algorithm, is a popular technique used in machine learning and deep learning. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in training neural networks.

The stochastic approximation method can be used to estimate the maximum likelihood of a function, which is often used in training neural networks. This algorithm is widely used to estimate the maximum likelihood of a function, which is often used in","ml:nn_training [NLP Wiki]
Neural Network Training
Overviews
Topics
Training Setups in the Literature
Alternative Training Methods
Related Pages
Neural Network Training
Overviews
Chapter 11 of Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow (UCSC login required)  Excellent introduction to training neural networks
Deep Learning Chapter 8: Training Deep Models
Smith 2018 - A Disciplined Approach to Neural Network Hyper-parameters Leslie's opinon, but has some good insights
Karpathy 2019 - A Recipe for Training Neural Networks Good advice, especially looking at the data
NLP 202 Winter 2022 - Training Neural Networks (slides)
Topics
Data Preparation and Tokenization
Hyperparameter Tuning
Random search is a good strategy, see Bergstra & Bengio 2012 - Random Search for Hyper-Parameter Optimization
Initialization
Normalization
Optimizers
Learning Rate
Blog Post: Setting the learning rate of your neural network
Loss Functions
Regularization
Fine-Tuning and Pretraining
Neural Network Tricks
Tricks such as Curriculum Learning, etc
Transformer Training Tricks
Residual connections, ReZero
Mixed Precision Training (also Train With Mixed Precision - NVIDIA Docs, see other papers as well)
Large-Scale and Distributed Training
Training Setups in the Literature
Training setups have evolved over time. Here are some from the literature.
Bahdanau 2014: Minibatch stochastic gradient descent (SGD) with Adadelta, trained for 5 days.  Recurrent weight matrices initialized as random orthogonal matrices. For feedforward weight matrices, initialized by sampling each element from the Gaussian distribution of mean 0 and variance 0.00.  Biases initialized to 0.
Ma & Hovy 2016: Minibatch SGD (batch size 10) with momentum (.9), gradient clipping (5.0) and learning rate decay (this setup performed similarly to Adam for them). Early-stopping on validation set. Dropout with rate = .5.
Gehring et al 2017 - Convolutional Sequence to Sequence Learning They used a lot of tricks. Worth taking a look.
Transformer (Vaswani et al 2017): Uses Adam with warmup, residual dropout, and label smoothing. Adam parameters β1 = 0.9, β2 = 0.98 and epsilon = 10−9.  Warmup increases “the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number.” Trained for 100,000 steps (12 hours).
Transformer: Popel & Bojar 2018 - Training Tips for the Transformer Model
BERT (Devlin et al 2019): Uses Adam with linear warmup, and linearly decaying stepsize.  Regularization: dropout and weight decay (L2 regularizer). Does not use label smoothing like the Transformer. “BERT is optimized with Adam (Kingma and Ba,2015) using the following parameters: β1 = 0.9, β2 = 0.999, espilon = 1e-6 and L2 weight decay of 0.01 with steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function (Hendrycks and Gimpel, 2016). Models are pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens.” (from Liu et al 2020) Trained for  1,000,000 steps.  A search over hyperparameters for training BERT is given in You et al 2020. As noted in Press 2020, trains on shorter sentences (<128 tokens) for 90% of the training, before training on longer sentences (<512) for the last 10%.
TODO: GPT-1 (p. 5)
Low-resource NMT system: Sennrich & Zhang 2019 - Revisiting Low-Resource Neural Machine Translation: A Case Study Uses bideep RNN, label smoothing, different dropout rates for output word embeddings, input word embeddings and hidden layers, tied embeddings, layer normalization, tuned BPE vocabulary size (reduced from larger data scenarios). Trained with Adam with early-stopping on a dev set using BLEU.
TODO: BART
Academic Budget BERT
Megatron-Turing NLG
PaLM
Paper  Architecture  Optimizer  Optimizer Hyperparameters  Initialization  Normalization  Regularizer  Learning Schedule  Stopping Criterion  Activation Function  Tokenization  Extras
Bahdanau 2014  Seq2seq BiLSTM + attention  Adadelta + gradient clipping (on norm)  espilon = 10−6, ρ = 0.95, gradient clipping = 1  Random orthogonal + Gaussian  none  none  (no learning rate, set by Adadelta)  5 days  LSTM & Tahn  Moses
Ma & Hovy 2016  BiLSTM +CNN word embds  SGD w/ momentum + gradient clipping  momentum = .9, gradient clipping = 5   GloVe & Uniform [-sqrt(3/dim),sqrt(3/dim)]   Dropout (.5)  Rate decay  Early-stopping  Sigmoid & Tahn  Tokens
Gehring et al 2017  CNN seq2seq  Nesterov + gradient clipping (on norm)
Vaswani et al 2017  Transformer  Adam  β1 = 0.9, β2 = 0.999, espilon = 1e-6  Glorot/fan_avg  Layer normalization  Dropout, label smoothing  Linear warm-up + 1/sqrt(step_number) 100,000 steps  ReLu  WordPiece
BERT (Devlin et al 2019)  Transformer  Adam  β1 = 0.9, β2 = 0.999, espilon = 1e-6  (Glorot/fan_avg?)  Layer normalization  L2 weight decay of 0.01  Linear warmup + linear decay  1,000,000  GELU  WordPiece  Trains on short sentences first

Alternative Training Methods
See Neural Networks: Alternative Training Methods
Related Pages
Alternative Training Methods
Data Preparation
Hyperparameter Tuning
NN Initialization
NN Tricks
Optimizers
Regularization"
Describe FOUL-UP.,"
If you are missing an evaluation, contact the instructor. If there is an error in your evaluation, contact the Registrar's Office.","FAQs for Students: Evaluations
Q: What is an evaluation? 
A: A performance evaluation is a description of the quality and characteristics of a student's performance in a course.
Q: How can I view my evaluations? 
A: Log on to MyUCSC, select the MyAcademics tile, and select Evaluations.
Q: I am missing an evaluation. What do I do? 
A: Effective fall 2009 for undergraduate students and fall 2010 for graduate students, evaluations are optional, and are completed at the discretion of the instructor. If you are missing a mandatory evaluation, contact the instructor.
Q: What if there is an error in one of my evaluations? 
A: If the error is a misspelling in your name or if you are referred to by the wrong gender, contact the Registrar's Office directly at nes@ucsc.edu. To correct any other error, contact the instructor and request a revised evaluation."
What is ELIZA?,"
ELIZA is a pioneering chatbot that was created in the 1960s by Joseph Weizenbaum. It is a type of artificial intelligence (AI) designed to mimic the human-like conversation of a psychotherapist. The chatbot uses natural language processing (NLP) and machine learning algorithms to generate responses to user input. ELIZA's primary function is to simulate a conversation, allowing users to discuss various topics and emotions. ELIZA's capabilities include:

* Understanding and generating human-like responses
* Using contextual information to provide relevant answers
* Recognizing and responding to emotional cues
* Engaging in a simulated conversation

ELIZA's success was largely due to its ability to learn from user interactions, allowing it to adapt to the user's tone and language. This made it a groundbreaking example of AI in natural language processing.","people [NLP Wiki]
People
NLP
Academia
Industry
Machine Learning
Academia
Industry
Vision
Academia
Industry
Optimization
People
This page lists Google Scholar pages of people to follow for their research, as well as perhaps their Twitter account if they are active on Twitter.
This list is incomplete - apologies to anyone we have left out.
NLP
Academia
For academia, you can also go to CSRankings and look for top people in NLP at each university (NLP world rankings).
Nikos Aletras
James Allen
Jacob Andreas Tw
Yoav Artzi
Wilker Aziz
Mohit Bansal
Chitta Baral
Regina Barzilay
Steven Bedrick
Anya Belz Tw
Jonathan Berant
Tamara Berg
Taylor Berg-Kirkpatrick
Yonatan Bisk
David Blei
Phil Blunsom
Sam Bowman Tw
Claire Cardie
Marine Carpuat
Nate Chambers
Kai-Wei Chang
Snigdha Chaturvedi
Danqi Chen
David Chiang
Kyunghyun Cho
Eunsol Choi
Jinho Choi
Yejin Choi
Shay Cohen
William Cohen
Trevor Cohn
Michael Collins
Ryan Cotterell
Hal Daumé III
Gerard de Melo
Kevin Duh
Greg Durrett
Ondřej Dušek
Jason Eisner
Katrin Erk
Maxine Eskenazi
Jeffrey Flanigan
Kevin Gimpel
Daniel Gildea
Yoav Goldberg Tw
Sharon Goldwater
Matt Gormley
Edward Grefenstette
Iryna Gurevych
Hannaneh Hajishirzi
Dilek Hakkani-Tur
He He
Kenneth Heafield
Aurelie Herbelot
Liang Huang
Lifu Huang
Eduard Hovy
Mohit Iyyer
Heng Ji
Robin Jia
Mark Johnson
Shafiq Joty
Dan Jurafsky
Min-Yen Kan
Dan Klein
Kevin Knight
Philipp Koehn
Alexander Koller
Lingpeng Kong
Sandra Kübler
Mirella Lapata
Lillian Lee
Percy Liang
Zhenghua Li
Ting Liu
Fei Liu
Xuezhe Ma
Andrea Madotto
Chris Manning Tw
James Martin
André Martins
Mausam
Andrew McCallum
Margaret Mitchell
Ray Mooney
Kenton Murray
Preslav Nakov
Roberto Navigli
Graham Neubig
Hwee Tou Ng
Martha Palmer
Ellie Pavlick
James Pustejovsky
Dragomir Radev
Matt Post
Narges Razavian
Siva Reddy
Ehud Reiter
Alan Ritter
Shruti Rijhwani
Carlos Gómez-Rodríguez - Dependency parsing
Rachel Rudinger
Anna Rumshisky
Alexander (Sasha) Rush Tw
Mrinmaya Sachan
Maarten Sap
Nathan Schneider
Hinrich Schütze
Lane Schwartz
Roy Schwartz
Rico Sennrich
Sameer Singh
Noah A. Smith
Amanda Stent
Swabha Swayamdipta
Partha Talukdar
Chenhao Tan
Ivan Titov
Jesse Thomason
David Traum
Yulia Tsvetkov
Andreas Vlachos
Katharina von der Wense
Marilyn Walker
Byron Wallace
Xiaojun Wan
Lu Wang
William Wang
Xin Eric Wang
Daniel Weld
Derry Wijaya
Sherry Tongshuang Wu
Wei ""Coco"" Xu
Nianwen Xue
Diyi Yang
Ziyu Yao
Mark Yatskar
Dani Yogatama
Zhou Yu
Luke Zettlemoyer
Xiaodan Zhu
Industry
Jason Baldridge
Miguel Ballesteros
Antoine Bordes
Ming-Wei Chang
Ken Church
Peter Clark
William Cohen
Dipanjan Das
Michael Denkowski
Emily Dinan
Markus Dreyer
Chris Dyer
Orhan Firat
Matt Gardner
Dan Garrette
Jianfeng Gao
David Grangier
Dilek Hakkani-Tur
Jayant Krishnamurthy
Guillaume Lample
Kenton Lee
Mike Lewis
Jiwei Li
Yunyao Li
Wolfgang Macherey
Matthew Marge
Swaroop Mishra
Tahira Naseem
Sujith Ravi
Sebastian Ruder
Noam Shazeer
Richard Socher
Linfeng Song
Kristina Toutanova
Ashish Vaswani
Svitlana Volkova
Zhilin Yang
Lingfei Wu
Manzil Zaheer
Lin Zhao (at Bosch Research in Sunnyvale)
Machine Learning
Academia
Yoshua Bengio
Jeff Dean
Lise Getoor
Amir Gholami
Tatsunori Hashimoto
Geoffrey Hinton
Sepp Hochreiter
‪Michael Jordan
Sham Kakade
Michael Mahoney
David McAllester
Tom Mitchell
Fei Sha
Jürgen Schmidhuber
Aarti Singh Google Scholar
Yee Whye Teh
Xinhua Zhang
Eric Xing
Industry
Leon Bottou
Yu (Hugo) Chen
Ronan Collobert
Ian Goodfellow
Alex Graves
Yuxiong He
Sanjiv Kumar
Quoc V. Le
Yann LeCun
Timothy Lillicrap
Leslie N. Smith
Ilya Sutskever
Jakob Uszkoreit
Oriol Vinyals
Jason Weston
Vision
Academia
Dhruv Batra
Justin Johnson (Vision and language)
Industry
Kaiming He
Andrej Karpathy
Optimization
Yuxin Chen
John Duchi
Satyen Kale"
What can I read for an introduction to information extraction?,"
A) A book on NLP
B) A paper on the history of information extraction
C) A survey on the state of the art in information extraction
D) A collection of papers on NLP

Answer: C) A survey on the state of the art in information extraction","nlp:information_extraction [NLP Wiki]
Introduction: Eisenstein p. 403
Sarawagi 2008 - Information Extraction  Old but good introduction
Grishman 2015 - Information Extraction
Mannai et al 2017 - Information Extraction Approaches: A Survey (Springer link)
Golshan et al 2018 - A Study of Recent Contributions on Information Extraction (Not so great overview)
Singh 2018 - Natural Language Processing for Information Extraction  Best overview
Weischedel & Boschee 2018 - What Can Be Accomplished with the State of the Art in Information Extraction? A Personal View
Grishman 2019 - Twenty-five years of information extraction
Miscellaneous Overviews
Niklaus et al 2018 - A Survey on Open Information Extraction
Martinez-Rodriguez et al 2016 - Information Extraction meets the Semantic Web: A Survey
Nasar et al 2018 - Information Extraction From Scientific Articles: A Survey
Verma & Bhattacharyya - Survey on Coreference Resolution, Relation and Event Extraction (unpublished?)
Ward et al 2020 - Survey of Current State of the Art Entity-Relation Extraction Tools
Papers
Zhang et al 2020 - TRIE: End-to-End Text Reading and Information Extraction for Document Understanding
Courses
https://www.cis.uni-muenchen.de/~fraser/information_extraction_2017_lecture/ or https://www.cis.uni-muenchen.de/~fraser/information_extraction_2017_seminar/
People
Ralph Grishman
Related Pages
Entity Linking
Event Extraction
Knowledge Graphs
Named Entity Recognition
Ontology Learning
Relation Extraction
Temporal Information Extraction"
Are there any courses I can take about information extraction?,"
Yes, there are several courses that focus on information extraction, such as the ones offered by the University of Munich (CIS) and the University of California, Berkeley (CSAIL). You can also explore online courses on platforms like Coursera, edX, and Udemy. Additionally, you can find resources on information extraction from academic papers and research articles, such as those published in the journal Nature, the Proceedings of the National Academy of Sciences, and the Journal of Machine Learning Research. These resources can provide you with a deeper understanding of the field and its applications.","nlp:information_extraction [NLP Wiki]
Introduction: Eisenstein p. 403
Sarawagi 2008 - Information Extraction  Old but good introduction
Grishman 2015 - Information Extraction
Mannai et al 2017 - Information Extraction Approaches: A Survey (Springer link)
Golshan et al 2018 - A Study of Recent Contributions on Information Extraction (Not so great overview)
Singh 2018 - Natural Language Processing for Information Extraction  Best overview
Weischedel & Boschee 2018 - What Can Be Accomplished with the State of the Art in Information Extraction? A Personal View
Grishman 2019 - Twenty-five years of information extraction
Miscellaneous Overviews
Niklaus et al 2018 - A Survey on Open Information Extraction
Martinez-Rodriguez et al 2016 - Information Extraction meets the Semantic Web: A Survey
Nasar et al 2018 - Information Extraction From Scientific Articles: A Survey
Verma & Bhattacharyya - Survey on Coreference Resolution, Relation and Event Extraction (unpublished?)
Ward et al 2020 - Survey of Current State of the Art Entity-Relation Extraction Tools
Papers
Zhang et al 2020 - TRIE: End-to-End Text Reading and Information Extraction for Document Understanding
Courses
https://www.cis.uni-muenchen.de/~fraser/information_extraction_2017_lecture/ or https://www.cis.uni-muenchen.de/~fraser/information_extraction_2017_seminar/
People
Ralph Grishman
Related Pages
Entity Linking
Event Extraction
Knowledge Graphs
Named Entity Recognition
Ontology Learning
Relation Extraction
Temporal Information Extraction"
What are some key papers in NLP?,"
Academia
Academia
NLP
Academia
Optimization
Academia
Optimization
Academia
Optimization
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
Academia
","people [NLP Wiki]
People
NLP
Academia
Industry
Machine Learning
Academia
Industry
Vision
Academia
Industry
Optimization
People
This page lists Google Scholar pages of people to follow for their research, as well as perhaps their Twitter account if they are active on Twitter.
This list is incomplete - apologies to anyone we have left out.
NLP
Academia
For academia, you can also go to CSRankings and look for top people in NLP at each university (NLP world rankings).
Nikos Aletras
James Allen
Jacob Andreas Tw
Yoav Artzi
Wilker Aziz
Mohit Bansal
Chitta Baral
Regina Barzilay
Steven Bedrick
Anya Belz Tw
Jonathan Berant
Tamara Berg
Taylor Berg-Kirkpatrick
Yonatan Bisk
David Blei
Phil Blunsom
Sam Bowman Tw
Claire Cardie
Marine Carpuat
Nate Chambers
Kai-Wei Chang
Snigdha Chaturvedi
Danqi Chen
David Chiang
Kyunghyun Cho
Eunsol Choi
Jinho Choi
Yejin Choi
Shay Cohen
William Cohen
Trevor Cohn
Michael Collins
Ryan Cotterell
Hal Daumé III
Gerard de Melo
Kevin Duh
Greg Durrett
Ondřej Dušek
Jason Eisner
Katrin Erk
Maxine Eskenazi
Jeffrey Flanigan
Kevin Gimpel
Daniel Gildea
Yoav Goldberg Tw
Sharon Goldwater
Matt Gormley
Edward Grefenstette
Iryna Gurevych
Hannaneh Hajishirzi
Dilek Hakkani-Tur
He He
Kenneth Heafield
Aurelie Herbelot
Liang Huang
Lifu Huang
Eduard Hovy
Mohit Iyyer
Heng Ji
Robin Jia
Mark Johnson
Shafiq Joty
Dan Jurafsky
Min-Yen Kan
Dan Klein
Kevin Knight
Philipp Koehn
Alexander Koller
Lingpeng Kong
Sandra Kübler
Mirella Lapata
Lillian Lee
Percy Liang
Zhenghua Li
Ting Liu
Fei Liu
Xuezhe Ma
Andrea Madotto
Chris Manning Tw
James Martin
André Martins
Mausam
Andrew McCallum
Margaret Mitchell
Ray Mooney
Kenton Murray
Preslav Nakov
Roberto Navigli
Graham Neubig
Hwee Tou Ng
Martha Palmer
Ellie Pavlick
James Pustejovsky
Dragomir Radev
Matt Post
Narges Razavian
Siva Reddy
Ehud Reiter
Alan Ritter
Shruti Rijhwani
Carlos Gómez-Rodríguez - Dependency parsing
Rachel Rudinger
Anna Rumshisky
Alexander (Sasha) Rush Tw
Mrinmaya Sachan
Maarten Sap
Nathan Schneider
Hinrich Schütze
Lane Schwartz
Roy Schwartz
Rico Sennrich
Sameer Singh
Noah A. Smith
Amanda Stent
Swabha Swayamdipta
Partha Talukdar
Chenhao Tan
Ivan Titov
Jesse Thomason
David Traum
Yulia Tsvetkov
Andreas Vlachos
Katharina von der Wense
Marilyn Walker
Byron Wallace
Xiaojun Wan
Lu Wang
William Wang
Xin Eric Wang
Daniel Weld
Derry Wijaya
Sherry Tongshuang Wu
Wei ""Coco"" Xu
Nianwen Xue
Diyi Yang
Ziyu Yao
Mark Yatskar
Dani Yogatama
Zhou Yu
Luke Zettlemoyer
Xiaodan Zhu
Industry
Jason Baldridge
Miguel Ballesteros
Antoine Bordes
Ming-Wei Chang
Ken Church
Peter Clark
William Cohen
Dipanjan Das
Michael Denkowski
Emily Dinan
Markus Dreyer
Chris Dyer
Orhan Firat
Matt Gardner
Dan Garrette
Jianfeng Gao
David Grangier
Dilek Hakkani-Tur
Jayant Krishnamurthy
Guillaume Lample
Kenton Lee
Mike Lewis
Jiwei Li
Yunyao Li
Wolfgang Macherey
Matthew Marge
Swaroop Mishra
Tahira Naseem
Sujith Ravi
Sebastian Ruder
Noam Shazeer
Richard Socher
Linfeng Song
Kristina Toutanova
Ashish Vaswani
Svitlana Volkova
Zhilin Yang
Lingfei Wu
Manzil Zaheer
Lin Zhao (at Bosch Research in Sunnyvale)
Machine Learning
Academia
Yoshua Bengio
Jeff Dean
Lise Getoor
Amir Gholami
Tatsunori Hashimoto
Geoffrey Hinton
Sepp Hochreiter
‪Michael Jordan
Sham Kakade
Michael Mahoney
David McAllester
Tom Mitchell
Fei Sha
Jürgen Schmidhuber
Aarti Singh Google Scholar
Yee Whye Teh
Xinhua Zhang
Eric Xing
Industry
Leon Bottou
Yu (Hugo) Chen
Ronan Collobert
Ian Goodfellow
Alex Graves
Yuxiong He
Sanjiv Kumar
Quoc V. Le
Yann LeCun
Timothy Lillicrap
Leslie N. Smith
Ilya Sutskever
Jakob Uszkoreit
Oriol Vinyals
Jason Weston
Vision
Academia
Dhruv Batra
Justin Johnson (Vision and language)
Industry
Kaiming He
Andrej Karpathy
Optimization
Yuxin Chen
John Duchi
Satyen Kale"
What is a lanuage model?,"
A language model is a type of artificial intelligence (AI) model that is designed to process and generate human-like language. It is a probability distribution over sentences that assigns probabilities to each possible word in a sentence, given a context. Language models can be autoregressive or masked, and they can be trained on a variety of data, including text, images, and even audio. They are commonly used in natural language processing (NLP) tasks, such as language translation, text summarization, and sentiment analysis.

They can be fine-tuned for specific tasks, and they can also be used to generate new text based on a given prompt. Language models are typically trained using a large dataset of text, and they are often used in conjunction with other AI models, such as transformers and recurrent neural networks. They are a key component of many modern AI systems, and they have many potential applications in areas such as customer service, language learning, and content creation.

In recent years, language models have become increasingly powerful and accurate, and they have been used in a wide range of applications, from language translation to text generation. However, they also raise important questions about data quality, bias, and fairness, and they require careful consideration and evaluation when used in real-world applications.

The current state of the field is characterized by a growing interest in the use of language models, and a increasing number of research papers and applications. However, there are also ongoing challenges and debates about the limitations and potential risks of language models, and the need for further research and development.

In summary, language models are a type of AI model that is designed to process and generate human-like language, and they have many potential applications in areas such as customer service, language learning, and content creation. However, they also raise important questions about data quality, bias, and fairness, and they require careful consideration and evaluation when used in real-world applications.","nlp:language_model [NLP Wiki]
Language Models
Overviews
Papers
Large Language Models
Abilities and Analysis of LLMs
Evaluation of LLMs and Benchmarks
Adapting Language Models
To Domains
To Other Languages
Temporal Language Modeling
Extracting Knowledge from Language Models
Knowledge Editing
Personalization
LLM Personality
Detecting Generated Text
Adversarial Attacks
Applications
Copyright Issues
Theoretical and Foundational Papers
Historical Papers
Datasets
Software and Demos
Related Pages
Language Models
Traditional definition of a language model (LM): a language model is a probability distribution over sentences, that is, it assigns probabilities to sentences.  Language models can usually compute the probability of the next word given a sequence of words (autoregressive language models), or in the case of masked language models, the probability of a word given a surrounding context.
Note: unlike autoregressive language models, masked language models usually can't be used to compute the probability of a sentence, and so they aren't really “language models” in the traditional sense.
To experiment with an autoregressive language model or masked language model, see online demos below.
Overviews
Introductory Material
Basic intro, and n-gram language modeling
Language modeling by Mike Collins
Probabilistic Language Models by Noah Smith
Chapter 3 of Speech and Language Processing
Neural language models
Section 7.5 of Chapter 7 of Speech and Language Processing
Large language models
2021 - AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing Comprehensive overview at the time
Wei et al 2023 - An Overview on Language Models: Recent Developments and Outlook
Liu et al 2024 - Understanding LLMs: A Comprehensive Overview from Training to Inference Wow, really good
Naveed et al 2024 - A Comprehensive Overview of Large Language Models
For another nice introduction, see related work of Taylor 2022 (p. 3)
Bowman 2023 - Eight Things to Know about Large Language Models
Minaee et al 2024 - Large Language Models: A Survey
Wan et al 2023 - Efficient Large Language Models: A Survey
Zheng et al 2023 - GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond
Weng 2024 - Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies
Language models in the news, etc
Wired - GPT-3
Twitter GPT-3 code example (Sharif Shameem) “I only had to write 2 samples to give GPT-3 context for what I wanted it to do. It then properly formatted all of the other samples… If I wanted it to write output plain HTML/CSS instead of JSX, all I would have to do would be to re-write my 2 initial samples in HTML/CSS. Then all of GPT-3's outputs would be in plain HTML/CSS.”
We Might See A 100T Language Model In 2022 Nice overview of some large language models in 2022
Bibliographies
Awesome-LLM
Papers
n-gram Models: Old classic papers, and recent papers
Teh 2006 - A Bayesian Interpretation of Interpolated Kneser-Ney
Malagutti et al 2024 - The Role of n-gram Smoothing in the Age of Neural Networks
Fill-In-the-Middle
Bavarian et al 2022 - Efficient Training of Language Models to Fill in the Middle
See also Starcoder Fill-In-The-Middle
Memory, Cache and Retrieval-Augmented Language Models
Khandelwal et al 2019 - Generalization through Memorization: Nearest Neighbor Language Models
Yogatama et al 2021 - Adaptive Semiparametric Language Models
2021 - Improving language models by retrieving from trillions of tokens (blog)
Wu et al 2022 - Memorizing Transformers Uses k-NN lookup with fixed embeddings to retrieve relevant examples
Clark et al 2022 - Unified Scaling Laws for Routed Language Models
Large Language Models
See also Ecosystem Graphs for a more complete list.
This is a list of large, GPT-style autoregressive LMs.  See also pretraining for another list of large language models and GPT-3 alternatives.
Jozefowicz et al 2016 - Exploring the Limits of Language Modeling It's interesting to see how far we've come since 2016.
GPT: Radford et al 2018 - Improving Language Understanding by Generative Pre-Training
GPT-2: Radford et al 2019 - Language Models are Unsupervised Multitask Learners original github Annotated GPT-2 Illustrated GPT-2 Interestingly, GPT-2 does not include a bias term in the final linear layer for the vocab, see here and here.
GPT-3: Brown et al 2019 - Language Models are Few-Shot Learners OpenAI cookbook
Turing-NLG: A 17-billion-parameter language model by Microsoft
Fedus et al 2021 - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Gopher: Rae et al 2021 - Scaling Language Models: Methods, Analysis & Insights from Training Gopher blog
Jurassic-1: Lieber et al 2022 - Jurassic-1: Technical Details and Evaluation model blog
Megatron-Turing NLG: Smith et al 2022 - Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model Microsoft blog NVidia blog Researcher access code models
Chinchilla: Hoffmann et al 2022 - Training Compute-Optimal Large Language Models Says most LLMs are undertrained, and trains a compute budget optimal size language model using the same dataset as Gopher. blog1 blog2
PaLM: Chowdhery et al 2022 - PaLM: Scaling Language Modeling with Pathways blog
GPT-NeoX-20B: Black et al 2022 - GPT-NeoX-20B: An Open-Source Autoregressive Language Model Has an interesting description of the hardware they used
OPT: Zhang et al 2022 - OPT: Open Pre-trained Transformer Language Models blog models
Bloom: model card Training readme Tensorboard log
Zeng et al 2022 - GLM-130B: An Open Bilingual Pre-trained Model
Li et al 2023 - FLM-101B: An Open LLM and How to Train It with $100K Budget
Model  Year  Parameters  Training Data  Public?  Link
GPT  2018   BooksCorpus  Yes  github Huggingface
GPT-2  2019  1.5B  Webtext (closed, see datasets below)  Yes  github Huggingface
GPT-3  2020  175B  CommonCrawl, Webtext2, Books 1&2, Wikipedia  API  OpenAI cookbook
MoE  2021  1.1T (13B)  CC100, CC-News, CC-Stories, OpenWebText, BookCorpus, Wikipedia  Yes  github HuggingFace
Gopher  2021  280B  MassiveText  No  blog
Megatron-Turing NLG  2022  530B  Pile, CommonCrawl, Realnews, CC-Stories  Researcher access  blog1 blog2 github
Chinchilla  2022  70B  MassiveText  No  blog
GPT-NeoX-20B  2022  20B  Pile  Yes  github
Jurassic-1  2022  178B   API  AI21 studio
YaLM 100B  2022  100B  Pile + lots of Russian text  Yes  github HuggingFace
PaLM  2022  540B  Social media, web, books, Github, Wikipedia  No?  blog
OPT  2022  66B, 175B  Pile subset: CommonCrawl, OpenWebtext2, Gutenberg, Wikipedia   Yes  demo models
UL2  2022  20B   Yes  blog github
Bloom  2022  176B  Multilingual BigScienceCorpus paper  Yes  HuggingFace demo
GLM-130B  2022  130B  Pile, Chinese WudaoCorpora, more  Yes  github
Galactica  2022  120B  Scientific papers, code, reference material, prompts  Yes github HuggingFace
ChatGPT  2022  ?   API  demo ShareGPT
LLaMA  2023  65B  CommonCrawl, C4, Github, Wikipedia, Books3, ArXiv, StackExchange  Yes  blog github
GPT-4  2023  ?  ? (multi-modal)  API  website
Alpaca  2023  7B  52k instructions from Self-Instruct w/ text-davinci-003  Yes  github demo
Vicuna  2023  7B/13B  (Chatbot)  Yes  github demo
Koala  2023  13B   Yes  github demo
StackLLaMA  2023  7B   Yes demo
LIMA  2023  65B
PaLM 2  2023  14.7B   API  website api
LLama 2  2023  70B   Yes  website blog
Mistral 7B, Mixtral 8X7B  2023  7B   Yes, API
Orca 2  2023
OLMo  2024  7B  dolma  Yes, open data  blog github huggingface
Gemma  2024  7B, 2B   Yes  blog
Jamba  2024  52B   Yes  blog HuggingFace

Abilities and Analysis of LLMs
ChatGPT
For ChatGPT, see also ChatGPT.
Laskar et al 2023 - A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets
Ronan & Schneider - Can Chat GPT solve a Linguistics Exam?
Creativity
Zhao et al 2024 - Assessing and Understanding Creativity in Large Language Models
Tian et al 2024 - MacGyver: Are Large Language Models Creative Problem Solvers?
Self-Correction
Liu et al 2024 - Large Language Models have Intrinsic Self-Correction Ability
Use of Context
Khandelwal et al 2018 - Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context (Old, no longer applies to transformer models)
Liu 2023 - Lost in the Middle: How Language Models Use Long Contexts
Evaluation of LLMs and Benchmarks
Polo et al 2024 - tinyBenchmarks: evaluating LLMs with fewer examples
Effects of Length and Irrelevant Context
Levy et al 2024 - Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
Adapting Language Models
To Domains
Xie et al 2023 - Data Selection for Language Models via Importance Resampling
To Other Languages
Language Adaptive Fine-Tuning (LAFT):
Artetxe et al 2019 - On the Cross-lingual Transferability of Monolingual Representations
Pfeiffer et al 2020 - UNKs Everywhere: Adapting Multilingual Language Models to New Scripts
Recycling: Vries & Nissim 2021 - As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages This one works the best so far. Retrains the embeddings keeping the Transformer layers fixed
Zhao & Schütze - Discrete and Soft Prompting for Multilingual Models
Multi-lingual version: Alabi et al 2022 - Adapting Pre-trained Language Models to African Languages via Multilingual Adaptive Fine-Tuning
Lin et al 2022 - Few-shot Learning with Multilingual Generative Language Models
Shi et al 2022 - Language Models are Multilingual Chain-of-Thought Reasoners
Marchisio et al 2022 - Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training
Michal Štefánik et al 2023 - Resources and Few-shot Learners for In-context Learning in Slavic Languages (Dataset)
Zhao et al 2024 - LLaMA Beyond English: An Empirical Study on Language Capability Transfer
Temporal Language Modeling
Lazaridou et al 2021 - Mind the Gap: Assessing Temporal Generalization in Neural Language Models
Li & Flanigan 2024 - Future Language Modeling from Temporal Document History
Drinkall et al 2024 - Time Machine GPT
Extracting Knowledge from Language Models
See also Dense Document Retrieval with LLMs.
Extracting Training Data
Carlini et al 2020 - Extracting Training Data from Large Language Models github
Membership Inference for Training Data
(Decide if some sample data is in the training data or not)
Related page: Membership Inference Attacks
Song & Shmatikov 2018 - Auditing Data Provenance in Text-Generation Models
Language Models as Knowledge Bases?
Feldman et al 2019 - Commonsense Knowledge Mining from Pretrained Models
Jiang et al 2020 - How Can We Know What Language Models Know?
Cao et al 2021 - Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases
Liu et al 2022 - Generated Knowledge Prompting for Commonsense Reasoning
Huang et al 2022 - Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents
Akyürek et al 2022 - Tracing Knowledge in Language Models Back to the Training Data
Knowledge Editing
See Knowledge Editing.
Personalization
Salemi et al 2023 - LaMP: When Large Language Models Meet Personalization
Li et al 2024 - Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security
LLM Personality
Jiang et al 2023 - PersonaLLM: Investigating the Ability of Large Language Models to Express Personality Traits
Detecting Generated Text
See also Fake News Detection.
Rodriguez - Cross-Domain Detection of GPT-2-Generated Technical Text
Mitchell et al 2023 - DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature
Adversarial Attacks
Mu et al 2023 - Can LLMs Follow Simple Rules?
Applications
Evaluation, see Evaluation with Large Language Models
Creating Data or replacement for crowdsourcing, see Data Augmentation (Synthetic Data Generation)
Copyright Issues
Henderson et al 2023 - Foundation Models and Fair Use
Karamolegkou et al 2023 - Copyright Violations and Large Language Models
Theoretical and Foundational Papers
See also Analysis of In-Context-Learning.
Emergent Abilities
Lu et al 2023 - Are Emergent Abilities in Large Language Models just In-Context Learning?
Historical Papers
Historical papers that may or may not be applicable today.
Teh 2003 - A Bayesian Interpretation of Interpolated Kneser-Ney
Melis et al 2017 - On the State of the Art of Evaluation in Neural Language Models Shows that LSTMS, when properly tuned, outperform other models (as of 2017, so before the Transformer)
Khandelwal et al 2018 - Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context
Datasets
Standard benchmark datasets
Wikitext 103
Penn Treebank
PG-19 Uses books from before 1919.  Good for long sequences.
Large datasets
Bookcorpus, also reproduced in the Pile, see here. reproduction original dataset
Common Crawl
WebText and OpenWebText:
WebText: Introduced in GPT-2 (paper).
OpenWebText: Various implementations here and here and here (on HuggingFace here). Used in MegatronLM here.
OpenWebText2 Open re-implementation, widely used. Use this one. On HuggingFace here.
Colossal Clean Crawled Corpus (C4): paper AI2 reimplementation atHuggingFace
The Pile Diverse set of data for building language models. Individual components, see readme. Older individual components Paper: The Pile: An 800GB Dataset of Diverse Text for Language Modeling Pile check tool
The pile has been removed due to this letter, see here and here
BigScienceCorpus: 2022 - The BigScience Corpus:
A 1.6TB Composite Multilingual Dataset
RedPajama-Data An Open Source Recipe to Reproduce LLaMA training dataset
Code datasets
The Stack Used in StarCoder. Has two membership test websites: Am I in the stack and DataPortraits
Small datasets
BabyLM Challenge
Tinystories: paper dataset
Minipile: paper Huggingface
Software and Demos
Training and/or Inference Frameworks for LLMs
For an overview, see table 4 and section 3.7 of Liu et al 2024 - Understanding LLMs: A Comprehensive Overview from Training to Inference
(Historical) n-gram LM toolkits
The best, highly optimized toolkit: KenLM
Industry standard toolkit with many options: SRILM
NLTK also implements n-gram LMs
Deep learning toolkits
GPT Neo An open-source implementation of model & data parallel GPT3-like models using the mesh-tensorflow library.
NVidia's Megatron-LM Used for example, by BLOOM
Online demos
AI2's Jurassic language model Jurassic-1
GPT-3: web interface is free after signing up
Hugging Face Fill Mask demo: Fill-Mask Demo Text Generation Demo
Related Pages
Autonomous Language Agents
BERT and Friends
ChatGPT
Dense Document Retrieval with LLMs
Hallucination and Factivity
Instruction-Tuning
Perplexity
Pretraining
Prompting
Scaling Laws
Supertasks
"
What is DetectGPT?,"
DetectGPT is a model developed by Google to detect images, but it's also being used in NLP tasks such as text classification, sentiment analysis, and more.  It's a large language model that can be fine-tuned for specific tasks and is often used in conjunction with other models.  DetectGPT is a type of pre-trained model, and it's been shown to be effective in many tasks, but it's also being used as a starting point for other models, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectGPT is often used as a starting point for other models, and it's being fine-tuned for specific tasks, and it's being fine-tuned for specific tasks.  DetectG","nlp:experimental_method [NLP Wiki]
Experimental Method and Reproducibility
Reproducibility
Statistical Significance
Boostrap Resampling and Permutation Tests
Papers
Software
Below is from an email I sent to a student Jan 20, 2019
Reproducibility Checklists, Datasheets and Model Cards
Other Topics in Experimental Design
Effects of the Random Seed
Resources and Tutorials
Related Pages
Experimental Method and Reproducibility
Reproducibility
Gundersen & Kjensmo 2018 - State of the Art: Reproducibility in Artificial Intelligence
2017 - A Manifesto for Reproducible Science Nice overview here
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results Introduces reproducibility checklists, see below, and also give a procedure for estimating if one model is better than another at various hyper-parameter tuning budgets (expected validation performance).  Their suggested estimator is a good one to use, see Dodge et al 2021.  Follow-up work in this area:
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping The results can largely be mitigated by training for more epochs, see Mosbach 2020
Tang et al 2020 - Showing Your Work Doesn’t Always Work Don't use. Introduces an unbiased estimator which has a high variance, see the follow-up work: Dodge 2021
Dodge et al 2021 - Expected Validation Performance
and Estimation of a Random Variable’s Maximum Compares to Tang 2020, and recommends using the procedure and estimator from Dodge 2019.
Miltenburg et al 2021 - Preregistering NLP research There are issues with this idea.
Marie et al 2021 - Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers
Statistical Significance
See also Statistical Tests.
For an overview of applying tests of statistical significance to NLP, see:
NLP 203 slides on statistical significance: Spring 2021
Section 11.3 from here (applied to MT, but the same techniques are used elsewhere in NLP)
Slides from Stanford NLP Group
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Section 4.4.3 Classifier comparison and statistical significance from Eisenstein's book
Appendix 3 from Noah Smith's book (available for free through UCSC library)
Dror et al 2019 - Deep Dominance - How to Properly Compare Deep Neural Models Caveat: some researchers have advocated tuning the random seed as a hyperparameter, see Effects of the Random Seed
Ulmer et al 2022 - Deep-Significance - Easy and Meaningful Statistical Significance Testing in the Age of Neural Networks
Wilcoxon Signed-Rank Test Docs in SciPy. An issue to consider is how to include outcomes in which system A & B produce the same prediction/score. See the zero_method parameter and associated links.
Boostrap Resampling and Permutation Tests
Noreen 1989 - Computer Intensive Methods for Testing Hypotheses: An Introduction Excellent overview
Bootstrap Methods and Permutation Tests
Bootstrap Resampling Slides
Chapter 3 of All of Nonparametric Statistics
Papers
See also ACL Anthology - statistical significance.
Koehn 2004 - Statistical Significance Tests for Machine Translation Evaluation  Advocates resampling the test set to estimate statistical significance. Widely used in MT.
Clark et al 2011 - Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability Great paper.  Someone should redo this paper for the deep learning era (and take into account Dodge 2019).
Berg-Kirkpatrick et al 2012 - An Empirical Investigation of Statistical Significance in NLP
Dror et al 2017 - Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Zhu et al 2020 - NLPStatTest: A Toolkit for Comparing NLP System Performance
Software
testSignificanceNLP (Recommended) Paper and Summary of recommended tests
NLPStatTest.  Paper
HyBayes.  Paper
Below is from an email I sent to a student Jan 20, 2019
It is recommended to use a non-parametric test, such as the permutation test or paired bootstrap, rather than a t-test, since they don't have distribution assumptions.  An example of how to do this is (use the R-package at the mentioned at the end):
https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html
Other references:
https://cs.stanford.edu/people/wmorgan/sigtest.pdf
http://www.aclweb.org/anthology/D/D12/D12-1091.pdf
There are other tests which also re-sample the test data, which is necessary if the test data is small.  A script to do all this is:
https://github.com/mgormley/sigtest
You only need 3-5 different runs for each experiments.  If you don't get significance but want to show it, you can do more runs.
Significance testing can be daunting since there are so many methods.  To keep it simple, I recommend just doing 3-5 runs for each experiment, and using the permutation test in the first link.  You can also report the sample standard deviation as error bars in the table (can to this with just 3-5 samples).
Reproducibility Checklists, Datasheets and Model Cards
Reproducibility Checklists
ACL conferences now require a Reproducibility checklist when submitting papers.  See for example EMNLP 2021.
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results
The Machine Learning Reproducibility Checklist
Datasheets (aka data cards)
Gebru et al 2018 - Datasheets for Datasets
Examples: QuAC Datasheet, WebNLG Data card, GEM Data cards
Model cards
Mitchell et al 2018 - Model Cards for Model Reporting
Examples: An AllenNLP model card, InstructGPT model card
Other Topics in Experimental Design
Effects of the Random Seed
For many common tasks and neural architectures, the choice of random seed has only a small effect on the accuracy or BLEU score (a standard deviation across random seeds of say .1-.5).  For this reason, many software packages fix the random seed in advance.  However, for some tasks or models, it is possible for the random seed to have a larger effect.  For example, Rongwen has found it has a large effect on neural models for Compositional Generalization.
Overview: 2021 - We Need to Talk About Random Seeds Advocates tuning the random seed
Madhyastha & Jain 2019 - On Model Stability as a Function of Random Seed
2021 - Effects of Random Seeds on the Accuracy of Convolutional Neural Networks
Summers & Dinneen 2021 - Nondeterminism and Instability in Neural Network Optimization
Resources and Tutorials
Tutorials
ACL 2022 Tutorial: Towards Reproducible Machine Learning Research in Natural Language Processing (link to conference video)
Related Pages
Evaluation
Hyperparameter Tuning
Statistical Tests"
What are some standard benchmark datasets?,"
Standard benchmark datasets are widely used in NLP for evaluating models. Some of the most commonly used datasets are:

* 20 Newsgroups
* 20 Newsgroups with 20 Newsgroups
* IMDB
* IMDB with 20 Newsgroups
* IMDB with 20 Newsgroups and 20 Newsgroups
* Stanford Question Answering Dataset
* Stanford Question Answering Dataset with 20 Newsgroups
* BookCorpus
* BookCorpus with 20 Newsgroups
* BookCorpus with 20 Newsgroups and 20 Newsgroups
* MovieNet
* MovieNet with 20 Newsgroups
* MovieNet with 20 Newsgroups and 20 Newsgroups
* WordNet
* WordNet with 20 Newsgroups
* WordNet with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 Newsgroups
* BookCorpus-20 Newsgroups with 20 Newsgroups and 20 New","nlp:experimental_method [NLP Wiki]
Experimental Method and Reproducibility
Reproducibility
Statistical Significance
Boostrap Resampling and Permutation Tests
Papers
Software
Below is from an email I sent to a student Jan 20, 2019
Reproducibility Checklists, Datasheets and Model Cards
Other Topics in Experimental Design
Effects of the Random Seed
Resources and Tutorials
Related Pages
Experimental Method and Reproducibility
Reproducibility
Gundersen & Kjensmo 2018 - State of the Art: Reproducibility in Artificial Intelligence
2017 - A Manifesto for Reproducible Science Nice overview here
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results Introduces reproducibility checklists, see below, and also give a procedure for estimating if one model is better than another at various hyper-parameter tuning budgets (expected validation performance).  Their suggested estimator is a good one to use, see Dodge et al 2021.  Follow-up work in this area:
Dodge et al 2020 - Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping The results can largely be mitigated by training for more epochs, see Mosbach 2020
Tang et al 2020 - Showing Your Work Doesn’t Always Work Don't use. Introduces an unbiased estimator which has a high variance, see the follow-up work: Dodge 2021
Dodge et al 2021 - Expected Validation Performance
and Estimation of a Random Variable’s Maximum Compares to Tang 2020, and recommends using the procedure and estimator from Dodge 2019.
Miltenburg et al 2021 - Preregistering NLP research There are issues with this idea.
Marie et al 2021 - Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers
Statistical Significance
See also Statistical Tests.
For an overview of applying tests of statistical significance to NLP, see:
NLP 203 slides on statistical significance: Spring 2021
Section 11.3 from here (applied to MT, but the same techniques are used elsewhere in NLP)
Slides from Stanford NLP Group
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Section 4.4.3 Classifier comparison and statistical significance from Eisenstein's book
Appendix 3 from Noah Smith's book (available for free through UCSC library)
Dror et al 2019 - Deep Dominance - How to Properly Compare Deep Neural Models Caveat: some researchers have advocated tuning the random seed as a hyperparameter, see Effects of the Random Seed
Ulmer et al 2022 - Deep-Significance - Easy and Meaningful Statistical Significance Testing in the Age of Neural Networks
Wilcoxon Signed-Rank Test Docs in SciPy. An issue to consider is how to include outcomes in which system A & B produce the same prediction/score. See the zero_method parameter and associated links.
Boostrap Resampling and Permutation Tests
Noreen 1989 - Computer Intensive Methods for Testing Hypotheses: An Introduction Excellent overview
Bootstrap Methods and Permutation Tests
Bootstrap Resampling Slides
Chapter 3 of All of Nonparametric Statistics
Papers
See also ACL Anthology - statistical significance.
Koehn 2004 - Statistical Significance Tests for Machine Translation Evaluation  Advocates resampling the test set to estimate statistical significance. Widely used in MT.
Clark et al 2011 - Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability Great paper.  Someone should redo this paper for the deep learning era (and take into account Dodge 2019).
Berg-Kirkpatrick et al 2012 - An Empirical Investigation of Statistical Significance in NLP
Dror et al 2017 - Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets
Dror et al 2018 - The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing and Appendix - Recommended Statistical Significance Tests for NLP Tasks
Zhu et al 2020 - NLPStatTest: A Toolkit for Comparing NLP System Performance
Software
testSignificanceNLP (Recommended) Paper and Summary of recommended tests
NLPStatTest.  Paper
HyBayes.  Paper
Below is from an email I sent to a student Jan 20, 2019
It is recommended to use a non-parametric test, such as the permutation test or paired bootstrap, rather than a t-test, since they don't have distribution assumptions.  An example of how to do this is (use the R-package at the mentioned at the end):
https://thomasleeper.com/Rcourse/Tutorials/permutationtests.html
Other references:
https://cs.stanford.edu/people/wmorgan/sigtest.pdf
http://www.aclweb.org/anthology/D/D12/D12-1091.pdf
There are other tests which also re-sample the test data, which is necessary if the test data is small.  A script to do all this is:
https://github.com/mgormley/sigtest
You only need 3-5 different runs for each experiments.  If you don't get significance but want to show it, you can do more runs.
Significance testing can be daunting since there are so many methods.  To keep it simple, I recommend just doing 3-5 runs for each experiment, and using the permutation test in the first link.  You can also report the sample standard deviation as error bars in the table (can to this with just 3-5 samples).
Reproducibility Checklists, Datasheets and Model Cards
Reproducibility Checklists
ACL conferences now require a Reproducibility checklist when submitting papers.  See for example EMNLP 2021.
Dodge et al 2019 - Show Your Work: Improved Reporting of Experimental Results
The Machine Learning Reproducibility Checklist
Datasheets (aka data cards)
Gebru et al 2018 - Datasheets for Datasets
Examples: QuAC Datasheet, WebNLG Data card, GEM Data cards
Model cards
Mitchell et al 2018 - Model Cards for Model Reporting
Examples: An AllenNLP model card, InstructGPT model card
Other Topics in Experimental Design
Effects of the Random Seed
For many common tasks and neural architectures, the choice of random seed has only a small effect on the accuracy or BLEU score (a standard deviation across random seeds of say .1-.5).  For this reason, many software packages fix the random seed in advance.  However, for some tasks or models, it is possible for the random seed to have a larger effect.  For example, Rongwen has found it has a large effect on neural models for Compositional Generalization.
Overview: 2021 - We Need to Talk About Random Seeds Advocates tuning the random seed
Madhyastha & Jain 2019 - On Model Stability as a Function of Random Seed
2021 - Effects of Random Seeds on the Accuracy of Convolutional Neural Networks
Summers & Dinneen 2021 - Nondeterminism and Instability in Neural Network Optimization
Resources and Tutorials
Tutorials
ACL 2022 Tutorial: Towards Reproducible Machine Learning Research in Natural Language Processing (link to conference video)
Related Pages
Evaluation
Hyperparameter Tuning
Statistical Tests"
What are some common software and hardware used in ML?,"
ML (Machine Learning) is a field that involves developing algorithms and statistical models that enable machines to learn from data, without being explicitly programmed. There are various software and hardware tools used in the field, including:

* Programming languages such as Python, R, and Julia
* Machine learning frameworks like TensorFlow, PyTorch, and scikit-learn
* Specialized libraries like scikit-image and OpenCV
* Hardware accelerators like GPUs and TPUs
* Cloud computing platforms like AWS and Google Cloud
* Databases and data storage systems like H2O and Amazon S3
* Visualization tools like Matplotlib and Seaborn

These tools are used to develop, train, and deploy machine learning models, and to analyze and interpret the results. The choice of tool depends on the specific problem, the type of model, and the level of expertise.","ml:history_of_ml [NLP Wiki]
History of Machine Learning
Early History of ML (prior to 1980)
Theory (prior to 2000)
Early Theory (prior to 1980)
Neural Networks (prior to 2010)
Neural Networks in NLP
Misc Topics
Other Methods (Prior to 2010)
Optimization (prior to 1980)
Related Pages
History of Machine Learning
Early History of ML (prior to 1980)
First use of the term “machine learning”: Arthur Samuel 1959 “Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.”  Caveat: see here
See also Machine Learning: An Artificial Intelligence Approach vol 1 by Michalski, Carbonell, and Mitchell Chapter 1, 1.4 and bibliography p. 20-23 and Comprehensive Bibliography of ML p. 511-556.
Overviews
Ho & Agrawala 1968 - On pattern classification algorithms introduction and survey
Fu 1969 - Learning Control Systems - Review and Outlook
Fu 1970 - Statistical Pattern Recognition
Books
Nilsson 1965 - Learning Machines: Foundations of Trainable Pattern-Classifying Systems
Minsky & Papert 1969 - Perceptrons: An Introduction to Computational Geometry 1987 Expanded edition
Bongard 1970 - Pattern Recognition pdf (translated from Russian). Cited by Carbonell p. 518.
Carbonell et al 1986 - Machine Learning: A Guide to Current Research Duplicate here ISBN: 0-89838-214-9
Papers
Fisher 1937 -  Introduced Linear Discriminant Analysis (LDA) and the famous Iris dataset (see Ghojogh 2019). This is the classic supervised binary machine learning setup, where the features are assumed to be normally distributed. More on LDA here.
McCulloch & Pitts 1943 - A Logical Calculus of the Ideas Immanent in Nervous Activity
Wiener 1948 - Cybernetics See also here. Mostly control theory. Doesn't talk about learning, although chapters from the 1961 2nd edition do.
Mcculloch 1949 - The Brain As A Computing Machine Cited by Samuel 1959
Fix & Hodges 1951 - Discriminatory Analysis - Nonparametric Discrimination - Consistency Properties pdf Introduces the binary classification task (with unknown non-parametric distributions) into the statistics literature. According to the abstract of Fix 1952, which uses the same setup, “A classification procedure is worked out for the following situations. Two large samples, one from each of two populations, have been observed. An individual of unknown origin is to be classified…”
Fix & Hodges 1952 - Discriminatory Analysis - Nonparametric Discrimination: Small Sample Performance pdf First introduces the k-nearest neighbors classifier (cited as [53] by survey paper Ho 1968) Was implemented on a computer? Need to check.
Oettinger 1952 - Programming a digital computer to learn Semantic Scholar pdf (UCSC only) Perhaps one of the first experiments with a learning algorithm on a computer (besides Fix 1952). Experiments on the EDASC computer. The idea was suggested to the author by Wilks. Introduces a “response-learning s-machine” (Sec 3) which is a reinforcement learning machine in the multi-armed bandit setting. The equations governing the machine are given in Sec 5, and trade off between exploration and exploitation. Limitations: there are no input observations into the machine beside the reinforcement learning signal, so there is very limited learning. I believe there may be earlier experiments with electro-mechanical machines that do a similar thing (see the references).
Bush, R., and Hosteller, F., “Stochastic Models for Learning”. John Wiley and Sons, 1955. Cited as [8] in Fu 1969.
Rosenblatt 1957 - The Perceptron - A Perceiving and Recognizing Automaton Introduces Perceptron algorithm.
Chow 1957 - An optimum character recognition system using decision functions pdf (UCSC only) Uses Bayes rule and minimizes the expected risk. Built on the theory of statistical decision functions, see citations 3-5. I believe they did not actually implement the system, paper contains only theoretical results.
Rosenblatt 1958 -  The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain See also 1961 book.
Friedberg 1958 - A Learning Machine: Part I January 1958.
Andrews, A. M., “Learning Machines”, Proceedings of the Symposium on the Mechanization of Thoughts Processes, H.M. Stationary Office, London, England, 1959. vol1 vol2 Symposium held Nov 24-27, 1959. Cited by Carbonell p. 515.
Kilburn, Grimsdale & Sumner 1959 - Experiments in Machine Learning and Thinking pdf (June 15-20, 1959). Cited by Carbonell p. 530. Work started in Feb 1957 on the Mark I computer. Perhaps the first mention of the term “machine learning” in a paper that has experiments on a computer. First instance of genetic programming?
Samuel 1959 - Some Studies in Machine Learning Using the Game of Checkers (July, 1959).
Martens 1959 - Two notes on machine “Learning” pdf
Novikoff  1963 - On convergence proofs for perceptrons
Tsypkin 1968 - Generalized Linear Learning Algorithms and Their Applications Also Ya. Z. Tsypkin, “Generalized learning algorithms,” Avtomatika t relemekhanika, No. 1, Moscow (1970).
Tsypkin 1968 - Self-learning--What is it?
Tsypkin 1972 - A Class of Learning Systems
Mitchell 1977 - Version Spaces: A Candidate Elimination Approach to Rule Learning
Mitchell 1978 - Version Spaces: An Approach to Concept Learning PhD thesis
Naive-Bayes Classifier
Todo: When was Naive-Bayes classifier first introduced? When was the term “Naive-Bayes” first used to refer to this classifier?
See Domingos & Pazzani 1997 - On the Optimality of the Simple Bayesian Classifier under Zero-One Loss for a history. Says this very simple classifier has not been discussed much, presumably because of its simplicity.
1968 - Adaptive Bayes Classification Model with Incompletely Specified Experiment Does this describe NB classifier? Don't have access.
Clark & Niblett 1989 - The CN2 Induction Algorithm Perhaps the first description of the Naive-Bayes classifier in the machine learning literature? Calls it a “simple Bayes classifier.”
I believe early versions of Duda and Hart may talk about NB
Theory (prior to 2000)
Early Theory (prior to 1980)
Novikoff  1963 - On convergence proofs for perceptrons
Hoeffding 1963 - Probability Inequalities for Sums of Bounded Random Variables
Cover & Hart 1967 - Nearest Neighbor Pattern Classification Shows that as the sample size goes to infinity, the KNN has has an error which is less than twice the Bayes optimal classifier error.
Cover 1968 - Estimation by the Nearest Neighbor Rule
Vapnik & Chervonenkis 1971 - On the Uniform Convergence of Relative Frequencies of Events to Their Probabilities pdf
Vapnik & Chervonenkis 1974 - Theory of Pattern Recognition. (Book in Russian)
Devroye & Wagner 1976 - A distribution-free performance bound in error estimation pdf Cited by Blumer 1986
Pearl 1978 - On the connection between the complexity and credibility of inferred models pdf Cited by Blumer 1986
Theory (1980-2000)
Mitchell 1980 - The Need for Biases in Learning Generalizations Introduces the concept of bias in machine learning
Valiant 1984 - A theory of the learnable pdf Introduced PAC learning
VC Theory
Blumer et al 1986 - Classifying Learnable Geometric Concepts with the Vapnik-Chervonenkis Dimension First to apply VC dimension to PAC learning theory
VC Theory notes from 1988  From here
Blumer et al 1989 - Learnability and the Vapnik-Chervonenkis Dimension
Abu-Mostafa 1989 - The Vapnik-Chervonenkis Dimension: Information versus Complexity in Learning
Neural Networks (prior to 2010)
Overviews: History of Neural Networks
Schmidhuber 2014 - Deep Learning in Neural Networks: An Overview (Comprehensive history)
Early Neural Models
Sometimes also called connectionist models or artificial neural networks
Feldman & Ballard 1982 - Connectionist Models and Their Properties
Hinton et al 1984 - Distributed Representations Based on Hinton 1982 CMU Tech Report CS-84-157.
Backpropagation
Schmidhuber 2014 - Who invented backprop (see this arxiv paper for references)
Woodrow & Lehr 1990 - Thirty Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation “The first major extension of the feedforward neural network beyond Madaline I took place in 1971 when Werbos developed a backpropagation training algorithm which, in 1974, he first published in his doctoral dissertation [39]. Unfortunately, Werbos’s work remained almost unknown in the scientific community. In 1982, Parker rediscovered the technique [41] and in 1985, published a report on it at M.I.T. [42]. Not long after Parker published his findings, Rumelhart, Hinton, and Williams [43,44] also rediscovered the technique and, largely as a result of the clear framework within which they presented their ideas, they finally succeeded in making it widely known.”
Bryson & Ho 1969 - Applied Optimal Control Russell & Norvig (AI: A Modern Approach) cites this for an example of early work that used the concept of backprop
Werbos 1974 thesis Invented backprop, according to Werbos 2004 and Schmidhuber 2014
Werbos 1981 - Applications of advances in nonlinear sensitivity analysis (Springer) First paper to suggest backprop for neural nets (according to Schmidhuber 2014).
For a history, see Werbos 2004 - Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities (Fascinating retrospective history)
Parker, 1985
LeCun, 1985
The paper that popularized it: Rumelhart, Hinton & Williams 1986 - Learning Representations by Back-Propagating Errors (Letters to Nature)
1985 tech report (published in PDP vol 1 1986) Talks about recurrent neural networks
Universal approximation theorem
Cybenko 1989 - Approximation by superpositions of a sigmoidal function  First paper to show that neural networks can approximate any function
Resources
Hinton's publications Contains papers that are hard to find.
Old conferences and journals
International Joint Conference on Neural Network (IJCNN) since 1987
Workshop on Natural Language Processing and Neural Networks (NLPNN): 1999
Cybernetics Translated from Kibernetika (in Russian). See here.
Neural Networks in NLP
See Neural Networks in NLP
Misc Topics
Other Methods (Prior to 2010)
Support Vector Machines
Weston & Watkins 1998 - Multi-class Support Vector Machines Tech Report CSD-TR-98-04 link
Optimization (prior to 1980)
Focusing on papers related to machine learning.
Stochastic Gradient Descent (SGD)
Classic overview in Bottou 2003 - Stochastic Learning
Note: early Russian papers use M instead of E to denote expectation.
History: As far as I can tell, SGD as we know it was first introduced by Ermol'ev & Nekrylova 1966, and stochastic sub-gradient descent (SSGD) was introduced by Ermol'ev 1969. These works built upon the stochastic approximation method that was proposed in Robins & Monroe 1951. This was extended to maximization problems of one dimension in Kiefer & Wolfowitz 1952, which was extended to the multidimensional case in Blum 1954 and Blum 1958. However, these methods used a finite-difference approximation to computing the stochastic gradient, in contrast to Ermol'ev & Nekrylova 1966 (History from Ermol'ev  1966 Sec 12).
Kiefer & Wolfowitz 1952 - Stochastic Estimation of the Maximum of a Regression Function pdf
Blum 1954 - Multidimensional Stochastic Approximation Methods Not quite SGD (computes a finite-differences approx to SGD in a random direction). Does not assume access to a noisy gradient. See sect 6.
Blum 1958 - A note on stochastic approximation
Gladyshev 1965 - On the stochastic approximation pdf In Russian. Cited by Ermol'ev & Nekrylova  1966. Perhaps this is SGD applied to least-squares regression?
Ermol'ev  1966 - Methods of solution of nonlinear extremal problems pdf See section 12 for the early history of SGD. Overview of Blum 1954 shows that it is not quite SGD (computes a finite-differences approx to SGD).
Ermol'ev & Nekrylova  1966 - Some methods of stochastic optimization pdf Stochastic gradient descent for differentiable, convex functions. $\xi^{(s)}$ is the stochastic gradient, $\rho_s$ is the stepsize. Cited by Nekrylova 1974. I believe this is the introduction of SGD, since Blum 1954 uses a finite-differences method.
Polyak 1967 - A General Method for Solving Extremum Problems Talks about sub-gradient descent. Calls sub-gradients “support functionals.” Cited by Guseva 1971.
Ermoliev, Yu.M., and Z.V. Nekrylova. 1967. The Method Stochastic Subgradients and Its Applications. Notes, Seminar on the Theory of Optimal Solution. Academy of Sciences of the U.S.S.R., Kiev. (Cited [1] in Ermoliev 1981.)
Ermol'ev & Shor 1968 - Method of random walk for the two-stage problem of stochastic programming and its generalization pdf
Ermol'ev Tuniev 1968 - Direct methods of solving some stochastic programming problems pdf Cited by Guseva 1971 Minimization of linear program with stochastic constraints.
Ermol'ev 1969 - On the method of generalized stochastic gradients and quasi-Féjer sequences pdf I believe this may be the introduction of SSGD as we know it (stochastic sub-gradient descent). Calls sub-gradients “generalized gradient vectors,” and calls the stochastic sub-gradient a “generalized stochastic gradient vector, or briefly, the stochastic quasi-gradient vector.” Assumes convexity, since assumes sub-gradient. Cited by Nurminskii 1974 and Guseva 1971.
Guseva 1971 - Convergence rate of the method of generalized stochastic gradients pdf First proof of rate of convergence of an SGD-like algorithm. (Have to look closer, I'm not sure if it's actually SGD). Assumes convexity. Notes: g(x) is the subgradient of f(x), called reference function. It is called the support functional in Polyak 1967 (citation [7]).
Nurminskii 1974 - Minimization of nondifferentiable functions in the presence of noise pdf
Nekrylova 1974 - Stochastic gradient method in infinite-dimensional space pdf
Stochastic Approximation Method
Robins & Monroe 1951 - A Stochastic Approximation Method local copy This paper is often cited as the paper that introduced SGD (see for example Bottou 2003). However, I would advocate against citing this paper as the originator of SGD, since it has the following limitations: it only treats 1 dimensional problems, it makes a monotonicity assumption, it is a root finding method, and they only apply it to the minimization problem of linear regression with least squares.  This is a long way away from SGD as formulated by Bottou 1991. I think there must be a much better citation than this for the origination of SGD and I suggest Bottou 1991 or Ermoliev 1981 as perhaps a better citation.
Dvoretzky 1956 - On Stochastic Approximation local copy More general proofs of SAM
Bottou 1991 - Stochastic Gradient Learning in Neural Networks Gives general proofs that SGD converges for non-convex problems such as neural networks
There is earlier work on the “stochastic quasi-gradient method,” which is a generalization of SGD
Quasi-gradient method Nurminskii - 1973
Ermoliev 1981 - Stochastic quasi-gradient method, see Ermoliev & Gaivoronski - 1984  The most general version of SGD (that Jeff knows about), works with constrained problems and biased gradient estimates.  Reduces to SGD under normal circumstances.
See also overview here: 2011 - Some scientific results of Yu. M. Ermoliev and his school in modern stochastic optimization theory pdf
Second-order methods
Battiti 1992 - First- and second-order methods for learning: Between steepest descent and newton’s method
Becker & Le Cun 1988 - Improving the convergence of back-propagation learning with second-order methods
Related Pages
History of NLP"
I'd like more information on Logistic Regression.,"
I'd like to learn more about Logistic Regression, a fundamental component of many machine learning algorithms, including decision trees, random forests, and neural networks. I'm interested in understanding the basics of Logistic Regression, its advantages, and how it's used in various applications.

Logistic Regression is a statistical model that predicts the probability of an event occurring based on the presence or absence of certain features. It's a linear model, which means it assumes a linear relationship between the features and the target variable. The goal is to find the best-fitting line that minimizes the error between the predicted and actual probabilities.

The model uses the following steps:

1. **Data Preprocessing**: The data is cleaned and transformed into a suitable format for the model.
2. **Feature Selection**: The most relevant features are selected based on the data.
3. **Model Training**: The model is trained using the selected features and the corresponding target variable.
4. **Model Evaluation**: The model is evaluated using a test dataset to determine its performance.

Logistic Regression has several advantages:

* **Interpretability**: The model provides clear and interpretable results, making it easy to understand the relationships between the features and the target variable.
* **Flexibility**: Logistic Regression can be used with various datasets, including binary and multi-class classification problems.
* **Robustness**: Logistic Regression is robust to outliers and non-linear relationships.

However, Logistic Regression also has some limitations:

* **Assumes Linearity**: Logistic Regression assumes a linear relationship between the features and the target variable, which may not always be the case.
* **Sensitivity to Feature Selection**: The model's performance can be sensitive to the selection of features.

To improve the performance of Logistic Regression, you can try:

* **Feature Engineering**: Create new features that capture the underlying relationships between the variables.
* **Regularization**: Add regularization terms to prevent overfitting.
* **Model Selection**: Use different models, such as Random Forest or Neural Network, to improve performance.

Overall, Logistic Regression is a fundamental component of many machine learning algorithms, and understanding its basics and advantages can help you build more effective models.

Sources:

* Bishop, C. M. (2006). Pattern Recognition and Machine Learning, second ed. Springer.
* Bishop, C. M. (2012). Machine Learning: A Probabilistic Perspective. Wiley.
* Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. Wiley.
* Murphy, K. P. (2013). Machine Learning, Probabilistic Perspective, second ed. Wiley.
* Bishop, C. M. (2016). A Primer on Neural Network Models for Natural Language Processing, Yoav Goldberg. arXiv preprint arXiv:1609.02640.
* Bishop, C. M. (2016). The Matrix Cookbook. arXiv preprint arXiv:1609.02640.
* Bishop, C. M. (2018). Deep Learning Book. arXiv preprint arXiv:1804.02732.

Note: The provided sources are a mix of books, articles, and research papers that provide a comprehensive overview of Logistic Regression and its applications in machine learning.","ml:ml_overview [NLP Wiki]
Machine Learning Overview
Books
Courses
Overview of Topics
Related Pages
Machine Learning Overview
This page is a concise overview of topics in machine learning, with links to readings and other learning materials.  Roughly, these topics are the union of topics covered in various ML books and courses.
This is a resource to help you get up to speed in various topics if you're trying to learn ML on your own or broaden your ML knowledge.
See also Aman.ai - AI Fundamentals
Books
Pattern Recognition and Machine Learning, Bishop, 2006 (Referenced below as Bishop) available here or local copy
An Introduction to Statistical Learning (Reference below as ISL) available here or local copy
The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2009 (Referenced below as ESL) available here or local copy
CIML (Referenced below as CIML) available here
Machine Learning, Tom Mitchell, McGraw Hill, 1997 (Referenced below as MLBook) available here
Machine Learning Books by Kevin Murphy
Machine Learning A Probabilistic Perspective, 2012 (referenced below as Murphy) available here
Probabilistic Machine Learning: An Introduction, 2021 (reference below as PML1) available here or local copy
Deep Learning Book (Referenced below as DLBook)
A Primer on Neural Network Models for Natural Language Processing, Yoav Goldberg, 2016 local copy (Referenced below as NNPrimer)
The Matrix Cookbook, available here
Intro to ML (note this was released April 1st)
Courses
Machine Learning for Intelligent Systems @ Cornell
Very quick intro to machine learning (slides): Introduction to Machine Learning local copy
Overview of Topics
This overview contains links to particular pages in textbooks, lectures, blog posts, and videos covering the topic, listed easiest to hardest to understand, with videos listed at the end.  In other words, for each topic, introductory material is listed first with more advanced material afterwards, although you may find more advanced material easier to understand in some cases.
The blog posts and some of the videos are introductory and give the overall gist of the method, but may contain mathematical or conceptual errors. Videos that are lectures should be fine.
Introduction to Machine Learning MLBook p. 1-15 PML p. 1-28
“Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.” Arthur Samuel 1959
Basic Machine Learning Concepts
Inductive Bias MLBook p. 39-45
Overfitting/Underfitting
Approximation error vs estimation error aka Bias-Variance Tradeoff CIML p. 71-72 Bartlett notes Sometimes also called the bias-variance tradeoff.
Features
Hyperparameters
Train/dev/test split
Don't look at the test data CIML p. 25 “Do not look at your test data. Even once. Even a tiny peek. Once you do that, it is not test data any more. Yes, perhaps your algorithm hasn’t seen it. But you have. And you are likely a better learner than your learning algorithm.”
Additional ML Topics
Generative vs Discriminative Classifiers
“Generative Story” CIML p. 123-124
Bayesian statistics
MLE vs MAP estimation (and examples of MAP in machine learning) Blog
Classification
Naive Bayes CIML p. 120-123, MLBook p. 5
Note: Naive Bayes is a generative classifier - it estimates p(x,y).  It can be used for binary or multiclass classification.  A Naive Bayes classifier for documents where the input features are words is called a “Bag of Words model”
Logistic regression (LR) and Naive Bayes have the same model form, but Naive Bayes maximizes p(x,y) while LR maximizes p(y|x).  See Vivek's NB Note or MLBook p. 14
Logistic Regression MLBook p. 7-14 ESL p. 119-122
*Decision Trees Blog1 Blog2 CIML p. 10-18 ESL p.305-310
*Random Forests ISLR p. 319-321
*k Nearest Neighbors (k-NN) Blog CIML p. 29-40 Bishop p. 125-127 (starting from “We close this chapter by showing how the K-nearest-neighbour technique for density estimation can be extended to the problem of classification…”)
Perceptron Blog Lecture (w/ video) CIML p. 41-54 MLBook p. 86, Bishop p. 192-196, Wikipedia
The perceptron algorithm is actually minimizing a function of the data. It turns out, stochastic gradient descent (SGD) with stepsize 1 on a particular function of the data (called the perceptron loss function) is exactly the perceptron algorithm.
There is a multiclass version of the perceptron
Neural Networks NNPrimer p. 354-379 Bishop p. 225-272 CIML p. 129-140 ESL 392-401
Bayesian Neural Networks Bishop p. 277-284
Support Vector Machines Blog1 Blog2 CIML p. 100-103 Bishop p.325-345 ISL p. 337-349 ESL p. 417-422
Either the primal or the dual version of the SVM optimization problem can be used.  Historically, the dual version was used.  However, the dual must be optimized using a specialized optimization algorithm such as sequential minimal optimization (SMO), while the unconstrained version of the primal can be optimized using any gradient-based optimizer such as stochastic gradient descent (SGD), which is usually faster in practice.  For this reason, for large-scale learning, the primal version with a gradient-based optimizer is often preferred.  See SVM.
*Kernel Methods Blog ISL p. 350-354 CIML p. 141-148 ESL p. 423-438 Kernel methods can also be used for regression.
Loss Functions and Training
Regression
*Linear Regression Blog1 Blog2 ESL p. 43-51 Bishop p. 137-147 ISL p. 59-119
MAP vs MLE linear regression (MAP adds a regularizer term)
Bayesian linear regression
“Non-Linear” Regression
Polynomial Regression ISL p. 266-268
Splines Notes ISL p. 271-280 ESL p. 139-181
Practicalities
*Hyperparameters and Model Selection
Train/dev/test split Video
The most practical and principled way to select the model and hyperparameters is on a development set.
Feature Selection and Feature Engineering CIML p. 55-62
Regularization
Early-stopping
L2 regularization
L1 regularization
Pruning (for decision trees)
Evaluation
Accuracy
Precision, Recall, F1: Macro vs Micro averaging
*Area Under the Curve (AUC) Blog
Tests of Significance CIML p. 67-69
Data Resampling Methods
k-fold Cross Validation Be careful using this method on NLP datasets! Due to the non-IID nature of NLP datasets, it is generally not recommended to use k-fold cross validation (can over-estimate performance).  Better to use a thoughtfully-chosen train/dev/test split.
*Bootstrap Resampling
Jacknife
Debugging ML CIML p. 69-71 Blog
Deep Learning
NN Architectures
Feedforward NNs
Convolutional NNs (CNNs) Blog1 Blog2 DLBook Video
Recurrent NNs (RNNs) Blog1 Blog2 Blog3 Blog4 (covers RNNs, LSTMs, Attention, Transformers DLBook
*Attention Blog1 Blog2 Blog3 NMT p. 48-52 Attention
*Transformers Blog1 Blog2 Blog3 Paper (hard) Annotated Transformer (perhaps easier)
Training Methods Video
Generative Adversarial Networks (GANs)
*Reinforcement Learning
Graphical Models
Bayesian Networks Bishop p. 360
Hidden Markov Models (HMMs)
Undirected Graphical Models (MRFs and CRFs) Bishop p. 383
Linear-chain Conditional Random Fields
Factor Graphs Bishop p. 399
Inference
Variable Elimination
Belief Propagation (Sum-Product and Max-Product Algorithms) Bishop p. 402-415
Junction Tree Algorithm
Loopy Belief Propagation Bishop p. 417-418
Variational Inference
Sampling Methods
Combining Models
Ensembling
Mixture of Experts
*Boosting
Bayesian Model Averaging
Bagging ISL p. 316-318
Unsupervised Methods
Density Estimation
*EM Algorithm Blog MLBook p. 191-196 ESL p. 272-279 Murphy p. 348-359 Lecture Notes (covers hard and soft EM and application to HMMs) Video (EM for Gaussian Mixture Models)
There is both soft EM (soft assignment, the usual version) and hard EM (hard assignment during E-step).  Both versions “work” in that they will both converge to a local maximum.  The hard EM version can converge faster but sometimes doesn't work as well (see here or here).
There are also online versions of EM and other variants, see Murphy p. 365-369
*Clustering
K-Means Clustering Blog1 Blog2 ISL p. 386-390 Murphy p. 352-354 Video: 1 2 3
K-Means is an instance of the hard EM algorithm, see Lecture Notes
Hierarchical Clustering
Agglomorative Clustering
*Principle Component Analysis (PCA)
Structured Prediction
Structured Perceptron
Structured SVM
Conditional Random Fields (CRFs)
Probability and Statistics Background
Terminology
Probability Distribution (referred to as just a “Distribution”)
To sample from a probability distribution
Parameters
Random Variable
Independent
Independent and Identically Distributed (IID)
Joint Distribution
Marginal Distribution (referred to as just a Marginal).  Also to marginalize
To compute a marginal, you marginalize (sum) over the other random variables
Probability Distributions: Uniform, Normal, Poisson, Binomial, etc
*Bias-Variance Decomposition Lecture, Notes This is a statistics term, used when analyzing mean squared error in regression or density estimation, for example. In machine learning, it's more properly called approximation error (≈ bias) and estimation error (≈ variance) because you can't compute the bias (E[y]) or variance E[(y - E[y])^2] for non-numeric outputs like classes in multi-class classification.  However, these terms are often applied to ML somewhat loosely.
Density Estimation
Histograms
Kernel Density Estimators
Gaussian Processes
Theory
Concept Learning
Hypothesis Space
Inductive Bias MLBook p. 39-45
Bias-Variance Tradeoff
VC dimension
NP hardness of Learning
PAC Learning Theory
PAC-Bayesian Learning Theory
Information Theory Murphy p. 56-61
Entropy
Cross-entropy
Mutual Information
KL-Divergence
Software
R
scikit-learn
TensorFlow
PyTorch
NLTK
SpaCy
OpenCV"
What are some evaluation metrics?,"
A: In a course evaluation, you may be asked to provide metrics such as grades, participation, attendance, and overall performance. You may also be asked to rate the instructor's teaching style, the course material, and the learning environment. Be honest and objective when providing your feedback, and use specific examples to support your ratings.","FAQs for Students: Evaluations
Q: What is an evaluation? 
A: A performance evaluation is a description of the quality and characteristics of a student's performance in a course.
Q: How can I view my evaluations? 
A: Log on to MyUCSC, select the MyAcademics tile, and select Evaluations.
Q: I am missing an evaluation. What do I do? 
A: Effective fall 2009 for undergraduate students and fall 2010 for graduate students, evaluations are optional, and are completed at the discretion of the instructor. If you are missing a mandatory evaluation, contact the instructor.
Q: What if there is an error in one of my evaluations? 
A: If the error is a misspelling in your name or if you are referred to by the wrong gender, contact the Registrar's Office directly at nes@ucsc.edu. To correct any other error, contact the instructor and request a revised evaluation."
What is BLEU?,"
BLEU stands for Bilingual Evaluation Under Study, a measure used to evaluate the performance of machine translation systems. It is a metric that assesses the quality of translation by comparing it to a reference translation. BLEU is widely used in the field of machine translation and has become a standard metric for evaluating the performance of machine translation systems. It is a widely accepted benchmark for machine translation quality and is used to compare the performance of different machine translation systems. The BLEU score ranges from 0 to 100, with higher scores indicating better translation quality.","Contact | Natural Language Processing M.S.

CONTACT
Contact Us
E-mail: nlp@ucsc.edu
On this page you can:
Browse frequently asked questions
Connect with the NLP Support Team and share feedback
Join the UCSC NLP community on social media
Request more information about the program
FAQs
We’ve collated the questions we most regularly get asked to help you find the answers you need.
VIEW OUR FAQs
Connect with the NLP Support Team
We’re here to help you navigate the application process. On this page we’ve detailed the many ways you can connect with a member of the NLP Support Team. After you meet with a member of our team, let us know how the meeting went by completing the feedback form linked below.
Want to discuss your application? Email the NLP Support Team for assistance, and let us know if you’d prefer to discuss your questions in a Zoom meeting. Please note that we cannot estimate an applicant’s chances of admission based on an email or an advising conversation, but we can help you navigate any questions you may have about the application process and requirements.
Attend an NLP at UCSC Information Session: During each application period, we host virtual information sessions where you can learn about the unique benefits of studying NLP at UCSC. View information session dates for the Fall 2024 application period on our Admissions page. Please complete the contact form below to receive updates about upcoming information sessions and to learn more about studying NLP at UCSC.
You can email any questions to nlp@ucsc.edu for general information about the admissions and application process at Baskin.
Admission FAQs: grad.soe.ucsc.edu/admissions
Email: nlp@ucsc.edu
SHARE YOUR FEEDBACK
Meet with the NLP Support Team
Our NLP staff is here to help address any questions you may have about the NLP MS Program, living and studying in Silicon Valley, and the application process. We encourage you to meet with our support team during their weekly virtual office hours listed below.

JOIN AN INFORMATION SESSION
Yuetong Zhang
NLP Program Assistant
Schedule an Appointment
Email Yuetong at nlp@ucsc.edu for support.

All other inquiries
Email: nlp@ucsc.eduOr complete the form below:
Contact Us
"
What is sacreBLEU?,"
sacreBLEU is an acronym for the name of the famous French composer Maurice Ravel's opera ""Boléro"".","Paying for Coverage | UC Santa Cruz

UC SHIP premiums are increasing for the 2024-25 academic year.
Learn more about the increases
If you’re a registered student, you are automatically enrolled in UC SHIP and charged a health insurance premium on your registration bill.
If you’re a non-registered student enrolling yourself and/or your eligible dependents for coverage, you must pay the full insurance premium upon enrollment.
Learn what you’ll pay for coverage.
Help paying for medical expenses
Providing affordable health care is a UC SHIP mandate. Even so, every student’s financial position is unique. If your out-of-pocket medical expenses are causing you financial hardship, these UCSC resources may be able to provide financial assistance:
Slug Support Program
Health Equity Fund
Charity care may also be an option for you. Reach out to the provider’s office and ask if they offer any form of charity care. Although some providers do not offer any sort of charity care, others do. They may provide an application for you to complete or ask you to write a letter. Providers who do offer charity care will review your request, and then they could offer to write off the full amount you owe, a portion of that amount, or they may determine you don’t meet their qualifications for charity care.


"
What are some neural network architectures?,"
Some of the most prominent neural network architectures are: 
Feedforward Networks
Connections (Residual connections, ReZero Similar to residual connections, ReLU)
Sequence Networks
Tree Networks
Graph Networks
Matrix representations (various types of matrices)
Activation Functions (GLU, ReLU, Leaky ReLU, Parametric ReLU, Softmax, Maxout, Softsign, HardTanh)
Matrix Matrices (Tensor networks, LoRA, Monarch Matrices)
Set and Pooling Networks
Attention Mechanisms (Transformer, Simple Self-Attention, Single-Headed Gated Attention)
Neurosymbolic Networks
Dynamic Neural Networks
Miscellaneous Architectures (Infinite Neural Networks, Binarized Neural Networks, Auto-Sizing Neural Networks)

------
Answer the question based on the context provided, in a concise and summarized manner, and limit the response to 300 tokens or less, and make sure to finish your sentence!

Feedforward Networks are a fundamental architecture in neural networks, characterized by their feedforward structure, where information flows from one layer to the next without any feedback connections. Connections, such as residual connections, ReZero, and ReLU, are used to introduce non-linearity and improve model capacity.

Sequence Networks, on the other hand, are designed to model sequential data, such as time series or text. They often use recurrent connections, such as LSTMs or GRUs, to process information in a sequential manner.

Tree Networks are a type of neural network that uses a tree-like structure to process information. They are often used for tasks such as natural language processing or computer vision.

Graph Networks are a type of neural network that uses graph structures to process information. They are often used for tasks such as computer vision or robotics.

Matrix representations, such as tensors, are used to represent complex data structures. Matrix matrices, such as LoRA, are used to improve the efficiency of matrix computations.

Activation functions, such as GLU, ReLU, and Leaky ReLU, are used to introduce non-linearity into neural networks. Matrix matrices, such as Monarch Matrices, are used to improve the efficiency of matrix computations.

Attention mechanisms, such as the Transformer, are used to focus on specific parts of the input data. Neurosymbolic networks, such as Neural Module Networks, are used to represent complex data structures in a symbolic form.

Dynamic neural networks, such as Infinite Neural Networks, are used to improve the efficiency of neural networks by introducing new connections and layers.

Miscellaneous architectures, such as Binarized Neural Networks, Auto-Sizing Neural Networks, and Infinite Neural Networks, are used to improve the efficiency of neural networks by reducing the number of parameters and improving the training process.","ml:nn_architectures [NLP Wiki]
Neural Network Architectures
Overviews
Feedforward Networks
Connections
Sequence Networks
Tree Networks
Graph Networks
Activation Functions
Matrices
Set and Pooling Networks
Memory Architectures
RNN Cells
Position Embeddings
Attention Mechanisms
Neurosymbolic Networks
Dynamic Neural Networks
Miscellaneous Architectures
Neural Network Architectures
Overviews
Yu et al 2019 - A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures
Wu et al 2019 - A Comprehensive Survey on Graph Neural Networks
Narang et al 2021 - Do Transformer Modifications Transfer Across Implementations and Applications? Comparison of many Transformer model variants
Feedforward Networks
Highway networks
GLU (also considered a kind of activation, but it's more like a FF architecture).  Variants: Shazeer 2020 ReGLU and SwiGLU work well.
Capsule networks (also used in a CNN-type architecture)
Sparsely-Gated Mixture-of-Experts.  Used to greatly scale-up the number of parameters with (sub-linear? check this) increase in computation. Uses many overlapping feedforward networks that are gated by another network. 1000x improvements in model capacity.
Dou et al 2019 - Dynamic Layer Aggregation for Neural Machine Translation with Routing-by-Agreement
Dao et al 2022 - Monarch: Expressive Structured Matrices for Efficient and Accurate Training
Connections
Residual connections
ReZero Similar to residual connections, but with a trainable parameter that controls the strength of the nonlinearity (which is initialized to zero).
Sequence Networks
See also State-Space Models.
RNNs: Elman networks, Jordan networks
LSTMs, GRUs (see RNN Cells)
Neural Turing Machines Cool idea, but this paper has a drawback because in practice they limit the size of the external memory, which makes it more like a neural finite-state machine (see p. 11, footnote 2).  Not necessarily the be-all and end-all architecture for NTMs.
Differentiable Neural Computer An extension to Neural Turing Machines
Pointer Networks
StackRNNs, StackLSTMs
HyperNetworks  Uses one network to generate the weights for another network.
Memory networks (i.e. End-to-end memory networks)
Associative LSTM
Recurrent Additive Networks An early state space model
Pointer-Generator Networks
Convolutional Seq2seq
Long Short-Term Memory-Network (LSTMN) Augments the LSTM cell with a memory network
ByteNet Dilated convolution network for seq2seq that stacks the encoder and decoder, and doesn't use attention. Operates in linear time.
Transformers
Latent Transformer Non-autoregressive Transformer using latent variables
Simple Self-Attention Network (SSAN) Single-layer transformer with 1 attention head
RNMT+ Hybrid RNN/Transformer archecture. Outperforms the Transformer by half a BLEU point
Universal Transformers A recurrent (across layers) Transformer with dynamic halting at each position
Lightweight Recurrent Networks Related to the Transformer, LRNs are a drop-in replacement to other RNNs, which remove the sequential natural of RNN processing.  Essentially uses a Key-Query-Value attention mechanism instead of the recurrence.
Feedback Transformer Makes the Transformer recurrent by allowing each timestep to look back at all layers.  Improves performance but makes training much slower because of the recurrence.
∞-former (Infinite former) Infinite Memory Transformer
FNet A faster, attention-free Transformer architecture based on Fourier transforms
Anthe: Less is More! A slim architecture for optimal language translation
RetNet (Retentive Network)
Tree Networks
TreeLSTM, also S-LSTMs
Graph Networks
See also Wu et al 2019 - A Comprehensive Survey on Graph Neural Networks and Graph Neural Networks.
Graph convolution networks
Graph transformers
Activation Functions
See also the table in Wikipedia's Activation functions.
Sigmoid, Tahn, etc
Softmax
Maxout (explanation)
Softsign
HardTanh (from Collobert 2004)
ReLU (history: also popularized here and earlier)
Leaky ReLU
Parametric ReLU (PReLU) Leaky ReLU with learned parameters.
GLU and variants
Gaussian error linear units (GELU) Roughly xσ(1.702x). Used in GTP-2 and BERT.
Swish f(x) = xσ(βx). β=1.702 is GELU, β=1 is Sigmoid weighted Linear Unit (SiL)
STL Signed Truncated Logarithm. Very cool activation function with great properties.
Comparisons:
Narang et al 2021 Compares activation functions in the Transformer
Matrices
Various representations of matrices, such as sparse, or low-dimensional ones.
Tensor networks
LoRA
Monarch Matrices
Set and Pooling Networks
Max, average pooling
Attention
Transformer (it is actually a set network) and Simple Self-Attention Network (SSAN) which is a single-layer transformer with 1 attention head
Deep sets
Deep averaging networks (DAN) aka the neural bag-of-words model (NBOW)
Weighted deep averaging networks.  (A natural extension would be to predict the vector “a” from a pooling operation over vectors. Not sure if anyone has done this yet.)
Weighted Multiset Automata
See also Vinyals et al 2015 - Order Matters: Sequence to sequence for sets
BiLSTM Aggregation
Attentive Pooling and described in Attentive Pooling with Learnable Norms
Memory Architectures
Neural Stacks, Queues, and DeQues (see also Probabilistic Neural Programs)
Associative Memories
Memory networks A simple key-value associative memory
Holographic Reduced Representations An associative memory that compresses a collection of key-value vectors into a fixed-size representation using an approximation
Continuous unbounded memory (see sections 3.2-3.3)
RNN Cells
See also Wikipedia - Recurrent Neural Networks and Yu et al 2019 - A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures
Feedforward network (Elman network)
Feedforward network with residual connections (with careful tuning, has been shown to perform as well as LSTMs I believe)
LSTM
Forget gate
Peephole connections
Associative LSTM
GRU (has been shown not to perform as well as the LSTM cell, for example here)
Minimal Gated Unit (MGU)
Position Embeddings
See Position Embeddings.
Attention Mechanisms
See also the Attention Mechanisms page.
Feedforward attention (the original one)
Dot product attention (aka Luong attention)
Structural attention
Structured Attention Networks
Label Attention Layer
Linear Attention (Faster to compute - makes the Transformer O(n))
Random Feature Attention Uses random features to approximate the softmax, making it O(1).
Continuous Attention Mechanism, used here
Single-Headed Gated Attention Can simulate multi-head attention, and is more expressive (see Sect 3.3 and Theorem 1).
Neurosymbolic Networks
See also Neurosymbolic Methods
PossibleWorldNet
Neural Module Networks (see Neural Module Networks)
Probabilistic Neural Programs
Dynamic Neural Networks
See also Conditional Computation.
Miscellaneous Architectures
Infinite Neural Networks (GPNNs and Neural Tangent Kernel)
Binarized Neural Networks
Auto-Sizing Neural Networks"
Are transformers and simple-self attention networks the same thing?,"
No, they are not. The transformer is a more complex model that uses self-attention and multi-head attention. It's a type of neural network that allows the model to process sequential data, such as text or speech, in a way that's different from traditional RNNs or CNNs. Simple self-attention networks are a simpler version of the transformer that only use self-attention and don't use multi-head attention. They are often used as a building block for more complex models. However, they have some limitations, such as being less expressive and less flexible than the transformer. The transformer can handle longer sequences and more complex tasks, but it requires more computational resources and memory. Simple self-attention networks are more suitable for shorter sequences and simpler tasks. The transformer has been widely adopted in many applications, including language translation, question-answering, and text summarization. However, its high computational requirements and memory usage make it less suitable for real-time applications.","nlp:transformers [NLP Wiki]
Transformers
Overview
Surveys
Transformer Properties
Analysis and Interpretation
Transformer Variants: Overviews
Improvements
Mixture of Expert (MoE) Transformers
Ablation Experiments on the Transformer
Pruning Attention Heads
Training
Efficient Transformers
Survey Papers
Papers
Datasets and Benchmarks
Long-Context Transformers
Papers
Position Embeddings
Software
Related Pages
Transformers
Overview
See Transformers in the ML Overview for introductory blog posts
Original paper: Vaswani et al 2017 - Attention Is All You Need
The Annotated Transformer
Textbook (SLP): Ch 9.7: Transformers
A walkthrough of transformer architecture code Contains a very good picture of the computation graph.
Surveys
Lin et al 2021 - A Survey of Transformers
Transformer Properties
Time and Space Complexity: The Transformer uses O(n^2) computation time, and O(n^2) memory (Subramanian et al 2019) due to the attention matrix.  However, the experiments in Subramanian et al 2019 (fig 2) seem to show a linear increase in memory usage with sequence length.  Presumably this is because the attention matrix does not dominate the memory footprint.
Expressiveness and Representation Power:
See also the group FLaNN (Formal Languages and Neural Networks).
Hahn 2019 - Theoretical Limitations of Self-Attention in Neural Sequence Models Indicates Transformers can't even represent finite state machines
Chiang et al 2022 - Overcoming a Theoretical Limitation of Self-Attention
Henderson 2020 - The Unstoppable Rise of Computational Linguistics in Deep Learning Argues why the Transformer is so good at language
Bhattamishra et al 2020 - On the Ability and Limitations of Transformers to Recognize Formal Languages
Elhage et al 2021 - A Mathematical Framework for Transformer Circuits
Merrill et al 2022 - Saturated Transformers are Constant-Depth Threshold Circuit
Transformer Programs
Weiss et al 2021 - Thinking Like Transformers
Lindner et al 2023 - Tracr: Compiled Transformers as a Laboratory for Interpretability
Friedman et al 2023 - Learning Transformer Programs
Delétang et al 2022 - Neural Networks and the Chomsky Hierarchy
Merrill et al 2022 - Transformers Can Be Translated to First-Order Logic with Majority Quantifiers
Liu et al 2022 - Transformers Learn Shortcuts to Automata
Analysis and Interpretation
See also Transformer Circuits.
Visualization of position embeddings in BERT and GPT-2 (from here)
Ramsauer et al 2020 - Hopfield Networks is All You Need
Geva et al 2020 - Transformer Feed-Forward Layers Are Key-Value Memories
Langedijk et al 2023 - DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers
For decoders/LLMs
Feucht et al 2024 - Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs Finds the implicit vocabulary in a Transformer decoder model
Transformer Programs
RASP: Weiss et al 2021 - Thinking like Transformers
Lindner et al 2023 - Tracr: Compiled Transformers as a Laboratory for Interpretability
Friedman et al 2023 - Learning Transformer Programs
Transformer Variants: Overviews
Blog post: Lil'Log: The Transformer Family
Narang et al 2021 - Do Transformer Modifications Transfer Across Implementations and Applications? Experimental comparison of Transformer model variants
Improvements
Shaw et al 2018 - Self-Attention with Relative Position Representations
Dai et al 2019 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
So et al 2019 - The Evolved Transformer Neural architecture search for Transformer variants
Nguyen & Salazar 2019 - Transformers without Tears: Improving the Normalization of Self-Attention Many of these changes are default in the popular Transformer codebases
Shazeer 2019 - Fast Transformer Decoding: One Write-Head is All You Need twitter Used in AlphaCode to speed up decoding.
Liu et al 2020 - Very Deep Transformers for Neural Machine Translation
Subramanian et al 2020 - Multi-scale Transformer Language Models
Sukhbaatar et al 2019 - Adaptive Attention Span in Transformers Related to Milad's work.
Zemlyanskiy et al 2021 - ReadTwice: Reading Very Large Documents with Memories
Csordás et al 2021 - The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization
Wang et al 2022 - DeepNet: Scaling Transformers to 1,000 Layers
Dao et al 2022 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
Dao et al 2023 - Fash Decoding Speeds up flash attention for decoding. (Essentially, fixes a problem in the way decoding was implemented initially, so it's much faster. The new way is the more natural way it should have been implemented.)
Mixture of Expert (MoE) Transformers
Lepikhin, et al 2020 - GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
Zhang et al 2021 - MoEfication: Transformer Feed-forward Layers are Mixtures of Experts
Ablation Experiments on the Transformer
These are ablation experiments on the Transformer, such as ablating the multi-head attention, or comparing to an LSTM with multi-head attention.
Chen et al 2018 - The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation Outperforms the Transformer with a stacked BiLSTM with multi-head attention and other tricks from the Transformer.  Slightly slower per token, but converges faster.
Domhan 2018 - How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures Tries combinations of Transformer, RNN, CNN decoder and encoder layers.  Shows that “one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention.” In particular, they find that:
“Source attention on lower encoder layers brings no additional benefit.”
“Multiple source attention layers and residual feed-forward layers are key.”
“Self-attention is more important for the source than for the target side.”
Simple Self-Attention Network (SSAN) Ambartsoumian & Popowich 2018 - Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers  Compares the transformer to a 1-layer and 2-layer single-headed transformer layer.
RNN with attention back to previous states.  Has anyone compared this to the transformer?  I can't remember.
2019 - Tied Transformers: Neural Machine Translation with Shared Encoder and Decoder Shared weights for encoder and decoder.  Very natural if you consider the seq2seq transformer as a conditional language model.
Fixed (not learned) attention patterns in the encoder: Raganato et al 2020 - Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation
Linearizing the softmax in the attention - O(n) to compute: Katharopoulos et al 2020
No position embeddings (NoPE): 2023 - The Impact of Positional Encoding on Length Generalization in Transformers
Wang et al 2024 - Length Generalization of Causal Transformers without Position Encoding
Pruning Attention Heads
Michel et al 2019 - Are Sixteen Heads Really Better than One?
Behnke & Heafield 2020 - Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation
Training
Popel & Bojar 2018 - Training Tips for the Transformer Model
Initialization issues
Zhang et al 2019 - Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention
Liu et al 2020 - Understanding the Difficulty of Training Transformers
Liu et al 2020 - Very Deep Transformers for Neural Machine Translation
Optimizer issues
Zhang et al 2019 - Why are Adaptive Methods Good for Attention Models?
Warm-up, see Warm-up
Normalization issues
Nguyen & Salazar 2019 - Transformers without Tears: Improving the Normalization of Self-Attention
RMSNorm. Improvement to layer normalization. Shown by Narang et al 2021 to work well for Transformers.
Xiong et al 2020 - On Layer Normalization in the Transformer Architecture Says pre-norm transformers don't need warm-up and are often better
Liu et al 2020 - Understanding the Difficulty of Training Transformers
Shen et al 2020 - PowerNorm: Rethinking Batch Normalization in Transformers
Wang et al 2022 - DeepNet: Scaling Transformers to 1,000 Layers
Miscellaneous topics
Merrill et al 2020 - Parameter Norm Growth During Training of Transformers
Efficient Transformers
Survey Papers
Tay et al 2020 - Efficient Transformers: A Survey
Tay et al 2020 - Long Range Arena: A Benchmark for Efficient Transformers
Papers
Guo et al 2019 - Star-Transformer
Rae et al 2019 - Compressive Transformers for Long-Range Sequence Modelling
Roy et al 2020 - Efficient Content-Based Sparse Attention with Routing Transformers
Hofstätter et al 2020 - Local Self-Attention over Long Text for Efficient Document Retrieval  Sliding window local attention mechanism
Wang et al 2020 - Linformer: Self-Attention with Linear Complexity
Katharopoulos et al 2020 - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention Misleading name.  This paper linearizes the softmax in the attention layers which makes it O(n) to compute
Choromanski et al 2020 - Rethinking Attention with Performers
Xiong et al 2021 - Nystromformer: A Nystrom-based Algorithm for Approximating Self-Attention  Similar to SVD, but approximately linearizes the softmax by selecting landmarks before the softmax.
Fedus et al 2021 - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Peng et al 2021 - Random Feature Attention Uses random features to approximate the softmax, making it O(1).  Drop in replacement for standard attention. Experiments with the Transformer.
Lee-Thorp et al 2021 - FNet: Mixing Tokens with Fourier Transforms
Ma et al 2021 - Luna: Linear Unified Nested Attention
Hourglass Transformer: Hierarchical Transformers Are More Efficient Language Models - Has three blocks of layers: ones that downsample the tokens through pooling, ones that process, and ones that upsample.
FLASH: Hua et al 2022 - Transformer Quality in Linear Time
Nawrot et al 2022 - Efficient Transformers with Dynamic Token Pooling
Datasets and Benchmarks
https://github.com/google-research/long-range-arena|LRA (pronounced “ELRA”): Tay et al 2020 - Long Range Arena: A Benchmark for Efficient Transformers (Not just NLP tasks)
Long-Context Transformers
Papers
Dao et al 2022 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
Bertsch et al 2023 - Unlimiformer: Long-Range Transformers with Unlimited Length Input
Mohtashami et al 2023 - Landmark Attention: Random-Access Infinite Context Length for Transformers
Guo et al 2023 - LongCoder: A Long-Range Pre-trained Language Model for Code Completion
Chen et al 2023 - Extending Context Window of Large Language Models via Positional Interpolation
Dao 2023 - FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
Yang 2023 - LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models
Li et al 2023 - LooGLE: Can Long-Context Language Models Understand Long Contexts?
Qin et al 2024 - Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models
Munkhdalai et al 2024 - Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
Han et al 2023 - LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models Expands the length limit to 200 million with no additional training and is O(n)
Levy et al 2024 - Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
Position Embeddings
Learned position embeddings: Gehring et al 2017 - Convolutional Sequence to Sequence Learning
2022 - Randomized Positional Encodings Boost Length Generalization of Transformers (Submitted to ACL 2022, not accepted.)  Has good related work in section 4, comparison to prior work.
Su et al 2021 - RoFormer: Enhanced Transformer with Rotary Position Embedding
Press et al 2021 - Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation Uses exponential weighted decay in the attention to encode positional information
No Position Embeddings Transformer (NoPE): 2023 - The Impact of Positional Encoding on Length Generalization in Transformers
Chen et al 2023 - Extending Context Window of Large Language Models via Positional Interpolation
Software
See also Software.
Hugging Face Transformers Paper
Tutorials and example code
Tensorflow's transformer tutorial
The Annotated Transformer
nanoGPT Small version by Andrej Karpathy. Very cool
Related Pages
Attention Mechanisms
BERT and Friends
Seq2seq
State-Space Models"
What are some memory architectures?,"
Memory architectures are the mechanisms that enable a neural network to store and retrieve information from memory, which can be thought of as a form of ""short-term memory"" in the context of neural networks. There are several types of memory architectures that have been proposed, including:

Associative Memories: These are simple key-value associative memories that store information in a compact form. Examples include the neural bag-of-words model (NBOW) and the neural bag-of-words model with weights (NBOWW).

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context of neural networks. They are often used in applications such as speech recognition and language modeling.

Associative Memories (continued): These are used to store and retrieve information in a compact form, and can be thought of as a form of ""short-term memory"" in the context","ml:nn_architectures [NLP Wiki]
Neural Network Architectures
Overviews
Feedforward Networks
Connections
Sequence Networks
Tree Networks
Graph Networks
Activation Functions
Matrices
Set and Pooling Networks
Memory Architectures
RNN Cells
Position Embeddings
Attention Mechanisms
Neurosymbolic Networks
Dynamic Neural Networks
Miscellaneous Architectures
Neural Network Architectures
Overviews
Yu et al 2019 - A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures
Wu et al 2019 - A Comprehensive Survey on Graph Neural Networks
Narang et al 2021 - Do Transformer Modifications Transfer Across Implementations and Applications? Comparison of many Transformer model variants
Feedforward Networks
Highway networks
GLU (also considered a kind of activation, but it's more like a FF architecture).  Variants: Shazeer 2020 ReGLU and SwiGLU work well.
Capsule networks (also used in a CNN-type architecture)
Sparsely-Gated Mixture-of-Experts.  Used to greatly scale-up the number of parameters with (sub-linear? check this) increase in computation. Uses many overlapping feedforward networks that are gated by another network. 1000x improvements in model capacity.
Dou et al 2019 - Dynamic Layer Aggregation for Neural Machine Translation with Routing-by-Agreement
Dao et al 2022 - Monarch: Expressive Structured Matrices for Efficient and Accurate Training
Connections
Residual connections
ReZero Similar to residual connections, but with a trainable parameter that controls the strength of the nonlinearity (which is initialized to zero).
Sequence Networks
See also State-Space Models.
RNNs: Elman networks, Jordan networks
LSTMs, GRUs (see RNN Cells)
Neural Turing Machines Cool idea, but this paper has a drawback because in practice they limit the size of the external memory, which makes it more like a neural finite-state machine (see p. 11, footnote 2).  Not necessarily the be-all and end-all architecture for NTMs.
Differentiable Neural Computer An extension to Neural Turing Machines
Pointer Networks
StackRNNs, StackLSTMs
HyperNetworks  Uses one network to generate the weights for another network.
Memory networks (i.e. End-to-end memory networks)
Associative LSTM
Recurrent Additive Networks An early state space model
Pointer-Generator Networks
Convolutional Seq2seq
Long Short-Term Memory-Network (LSTMN) Augments the LSTM cell with a memory network
ByteNet Dilated convolution network for seq2seq that stacks the encoder and decoder, and doesn't use attention. Operates in linear time.
Transformers
Latent Transformer Non-autoregressive Transformer using latent variables
Simple Self-Attention Network (SSAN) Single-layer transformer with 1 attention head
RNMT+ Hybrid RNN/Transformer archecture. Outperforms the Transformer by half a BLEU point
Universal Transformers A recurrent (across layers) Transformer with dynamic halting at each position
Lightweight Recurrent Networks Related to the Transformer, LRNs are a drop-in replacement to other RNNs, which remove the sequential natural of RNN processing.  Essentially uses a Key-Query-Value attention mechanism instead of the recurrence.
Feedback Transformer Makes the Transformer recurrent by allowing each timestep to look back at all layers.  Improves performance but makes training much slower because of the recurrence.
∞-former (Infinite former) Infinite Memory Transformer
FNet A faster, attention-free Transformer architecture based on Fourier transforms
Anthe: Less is More! A slim architecture for optimal language translation
RetNet (Retentive Network)
Tree Networks
TreeLSTM, also S-LSTMs
Graph Networks
See also Wu et al 2019 - A Comprehensive Survey on Graph Neural Networks and Graph Neural Networks.
Graph convolution networks
Graph transformers
Activation Functions
See also the table in Wikipedia's Activation functions.
Sigmoid, Tahn, etc
Softmax
Maxout (explanation)
Softsign
HardTanh (from Collobert 2004)
ReLU (history: also popularized here and earlier)
Leaky ReLU
Parametric ReLU (PReLU) Leaky ReLU with learned parameters.
GLU and variants
Gaussian error linear units (GELU) Roughly xσ(1.702x). Used in GTP-2 and BERT.
Swish f(x) = xσ(βx). β=1.702 is GELU, β=1 is Sigmoid weighted Linear Unit (SiL)
STL Signed Truncated Logarithm. Very cool activation function with great properties.
Comparisons:
Narang et al 2021 Compares activation functions in the Transformer
Matrices
Various representations of matrices, such as sparse, or low-dimensional ones.
Tensor networks
LoRA
Monarch Matrices
Set and Pooling Networks
Max, average pooling
Attention
Transformer (it is actually a set network) and Simple Self-Attention Network (SSAN) which is a single-layer transformer with 1 attention head
Deep sets
Deep averaging networks (DAN) aka the neural bag-of-words model (NBOW)
Weighted deep averaging networks.  (A natural extension would be to predict the vector “a” from a pooling operation over vectors. Not sure if anyone has done this yet.)
Weighted Multiset Automata
See also Vinyals et al 2015 - Order Matters: Sequence to sequence for sets
BiLSTM Aggregation
Attentive Pooling and described in Attentive Pooling with Learnable Norms
Memory Architectures
Neural Stacks, Queues, and DeQues (see also Probabilistic Neural Programs)
Associative Memories
Memory networks A simple key-value associative memory
Holographic Reduced Representations An associative memory that compresses a collection of key-value vectors into a fixed-size representation using an approximation
Continuous unbounded memory (see sections 3.2-3.3)
RNN Cells
See also Wikipedia - Recurrent Neural Networks and Yu et al 2019 - A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures
Feedforward network (Elman network)
Feedforward network with residual connections (with careful tuning, has been shown to perform as well as LSTMs I believe)
LSTM
Forget gate
Peephole connections
Associative LSTM
GRU (has been shown not to perform as well as the LSTM cell, for example here)
Minimal Gated Unit (MGU)
Position Embeddings
See Position Embeddings.
Attention Mechanisms
See also the Attention Mechanisms page.
Feedforward attention (the original one)
Dot product attention (aka Luong attention)
Structural attention
Structured Attention Networks
Label Attention Layer
Linear Attention (Faster to compute - makes the Transformer O(n))
Random Feature Attention Uses random features to approximate the softmax, making it O(1).
Continuous Attention Mechanism, used here
Single-Headed Gated Attention Can simulate multi-head attention, and is more expressive (see Sect 3.3 and Theorem 1).
Neurosymbolic Networks
See also Neurosymbolic Methods
PossibleWorldNet
Neural Module Networks (see Neural Module Networks)
Probabilistic Neural Programs
Dynamic Neural Networks
See also Conditional Computation.
Miscellaneous Architectures
Infinite Neural Networks (GPNNs and Neural Tangent Kernel)
Binarized Neural Networks
Auto-Sizing Neural Networks"
I am looking to improve my model. What are some neural network tricks I can use?,"
I want to fine-tune my pre-trained language model, but I am struggling to get the best results. I've tried several optimizations, but I am still not seeing the desired improvements. What are some techniques that can help? Try to provide a few examples of each technique.

## Step 1: Experiment with different optimizers
You can try different optimizers, such as Adam, RMSprop, or SGD, to see which one works best for your model.

## Step 2: Adjust the learning rate
Adjusting the learning rate can help improve the convergence of your model.

## Step 3: Try different batch sizes
Increasing the batch size can help speed up the training process.

## Step 4: Use gradient accumulation
Using gradient accumulation can help simulate larger batch sizes and improve the stability of the training process.

## Step 5: Fine-tune specific tricks
Fine-tuning specific tricks, such as dropout, regularization, or attention, can help improve the performance of your model.

## Step 6: Try different architectures
Trying different architectures, such as Residual connections or attention mechanisms, can help improve the performance of your model.

## Step 7: Use knowledge distillation
Using knowledge distillation can help improve the performance of your model by transferring knowledge from a larger model to a smaller one.

## Step 8: Try different decoding methods
Trying different decoding methods, such as Seq2Seq or nucleus sampling, can help improve the performance of your model.

## Step 9: Use reinforcement learning tricks
Using reinforcement learning tricks, such as gradient checkpointing or model compression, can help improve the performance of your model.

## Step 10: Experiment with different GPU configurations
Experimenting with different GPU configurations can help improve the performance of your model.

## Step 11: Try different data processing tricks
Trying different data processing tricks, such as subword units or knowledge distillation, can help improve the performance of your model.

## Step 12: Use attention mechanisms
Using attention mechanisms can help improve the performance of your model by allowing it to focus on the most relevant parts of the input.

## Step 13: Try different architectures for specific tasks
Trying different architectures for specific tasks, such as language translation or text classification, can help improve the performance of your model.

## Step 14: Use knowledge distillation for specific tasks
Using knowledge distillation for specific tasks, such as language translation or text classification, can help improve the performance of your model.

## Step 15: Try different data augmentation techniques
Trying different data augmentation techniques, such as data augmentation or noise injection, can help improve the performance of your model.

The final answer is: There are many neural network tricks that can help improve your model. Some examples include:

* Experimenting with different optimizers
* Adjusting the learning rate
* Trying different batch sizes
* Using gradient accumulation
* Fine-tuning specific tricks
* Trying different architectures
* Using knowledge distillation
* Trying different decoding methods
* Using reinforcement learning tricks
* Experimenting with different GPU configurations
* Trying different data processing tricks
* Using attention mechanisms
* Trying different architectures for specific tasks
* Using knowledge distillation for specific tasks
* Trying different data augmentation techniques

Note: The final answer is not a single number or a specific solution, but rather a list of techniques that can help improve your model.","ml:nn_tricks [NLP Wiki]
Neural Network Tricks
Overviews
Older NN Tricks
Related Pages
Neural Network Tricks
Overviews
NLP 202 lecture: Training Deep Neural Networks (Winter 2022)
Training Tricks (see NN Training)
Initialization
Normalization
Learning Rate Schedule
Gradient clipping Pascanu et al 2012
Scheduled Sampling
Curriculum Learning
Overcoming Catastrophic Forgetting
Adjust the batch size, or use gradient accumulation (see this blog, for example) to simulate larger batch sizes
Try a different optimizer, such as RAdam
Adjust epsilon in Adam
Fine-tuning Specific Tricks
NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better : Before fine-tuning, adding a very small amount of uniform noise to each weight matrix can help performance (noise scaled by variance of the weight)
Regularization Tricks (see Regularization)
Dropout
Ensembling
Knowledge Distillation (can improve performance by some type of regularization)
Label Smoothing
Data Processing Tricks (see Data Preparation)
Subword Units (BPE, wordpiece, subword regularization, BPE dropout.  Shared source and target vocabulary for subword units.)
Shared source and target embeddings
Architecture Tricks (see NN Architectures)
Residual connections
ReZero
Weight sharing
Attention
Copy mechanism
Seq2Seq and Generation Tricks
Try a different decoding method
Nucleus sampling
Uniform information density decoding
Reinforcement Learning Tricks
Efficiency Tricks
GPU Deep Learning
Gradient Checkpointing or forward gradient
Model Compression
Tricks for Edge Computing
Knowledge Distillation
Older NN Tricks
Nowlan & Hinton 1992 - Soft weight sharing
Related Pages
NN Training"
What people in academia have work in NLP?,"
Many researchers have worked in NLP, including:
* Yoshua Bengio
* Jeff Dean
* Lise Getoor
* Amir Gholami
* Tatsunori Hashimoto
* Geoffrey Hinton
* Sepp Hochreiter
* Michael Jordan
* Sham Kakade
* Michael Mahoney
* David McAllester
* Tom Mitchell
* Fei Sha
* Jürgen Schmidhuber
* Aarti Singh
* Yee Whye Teh
* Xinhua Zhang
* Eric Xing
* Leon Bottou
* Yu (Hugo) Chen
* Ronan Collobert
* Ian Goodfellow
* Alex Graves
* Yuxiong He
* Sanjiv Kumar
* Quoc V. Le
* Yann LeCun
* Timothy Lillicrap
* Leslie N. Smith
* Ilya Sutskever
* Jakob Uszkoreit
* Oriol Vinyals
* Jason Weston
* Dhruv Batra
* Justin Johnson
* Andrej Karpathy
* Satyen Kale

These are just a few examples of notable researchers in NLP, but there are many others who have made significant contributions to the field.","people [NLP Wiki]
People
NLP
Academia
Industry
Machine Learning
Academia
Industry
Vision
Academia
Industry
Optimization
People
This page lists Google Scholar pages of people to follow for their research, as well as perhaps their Twitter account if they are active on Twitter.
This list is incomplete - apologies to anyone we have left out.
NLP
Academia
For academia, you can also go to CSRankings and look for top people in NLP at each university (NLP world rankings).
Nikos Aletras
James Allen
Jacob Andreas Tw
Yoav Artzi
Wilker Aziz
Mohit Bansal
Chitta Baral
Regina Barzilay
Steven Bedrick
Anya Belz Tw
Jonathan Berant
Tamara Berg
Taylor Berg-Kirkpatrick
Yonatan Bisk
David Blei
Phil Blunsom
Sam Bowman Tw
Claire Cardie
Marine Carpuat
Nate Chambers
Kai-Wei Chang
Snigdha Chaturvedi
Danqi Chen
David Chiang
Kyunghyun Cho
Eunsol Choi
Jinho Choi
Yejin Choi
Shay Cohen
William Cohen
Trevor Cohn
Michael Collins
Ryan Cotterell
Hal Daumé III
Gerard de Melo
Kevin Duh
Greg Durrett
Ondřej Dušek
Jason Eisner
Katrin Erk
Maxine Eskenazi
Jeffrey Flanigan
Kevin Gimpel
Daniel Gildea
Yoav Goldberg Tw
Sharon Goldwater
Matt Gormley
Edward Grefenstette
Iryna Gurevych
Hannaneh Hajishirzi
Dilek Hakkani-Tur
He He
Kenneth Heafield
Aurelie Herbelot
Liang Huang
Lifu Huang
Eduard Hovy
Mohit Iyyer
Heng Ji
Robin Jia
Mark Johnson
Shafiq Joty
Dan Jurafsky
Min-Yen Kan
Dan Klein
Kevin Knight
Philipp Koehn
Alexander Koller
Lingpeng Kong
Sandra Kübler
Mirella Lapata
Lillian Lee
Percy Liang
Zhenghua Li
Ting Liu
Fei Liu
Xuezhe Ma
Andrea Madotto
Chris Manning Tw
James Martin
André Martins
Mausam
Andrew McCallum
Margaret Mitchell
Ray Mooney
Kenton Murray
Preslav Nakov
Roberto Navigli
Graham Neubig
Hwee Tou Ng
Martha Palmer
Ellie Pavlick
James Pustejovsky
Dragomir Radev
Matt Post
Narges Razavian
Siva Reddy
Ehud Reiter
Alan Ritter
Shruti Rijhwani
Carlos Gómez-Rodríguez - Dependency parsing
Rachel Rudinger
Anna Rumshisky
Alexander (Sasha) Rush Tw
Mrinmaya Sachan
Maarten Sap
Nathan Schneider
Hinrich Schütze
Lane Schwartz
Roy Schwartz
Rico Sennrich
Sameer Singh
Noah A. Smith
Amanda Stent
Swabha Swayamdipta
Partha Talukdar
Chenhao Tan
Ivan Titov
Jesse Thomason
David Traum
Yulia Tsvetkov
Andreas Vlachos
Katharina von der Wense
Marilyn Walker
Byron Wallace
Xiaojun Wan
Lu Wang
William Wang
Xin Eric Wang
Daniel Weld
Derry Wijaya
Sherry Tongshuang Wu
Wei ""Coco"" Xu
Nianwen Xue
Diyi Yang
Ziyu Yao
Mark Yatskar
Dani Yogatama
Zhou Yu
Luke Zettlemoyer
Xiaodan Zhu
Industry
Jason Baldridge
Miguel Ballesteros
Antoine Bordes
Ming-Wei Chang
Ken Church
Peter Clark
William Cohen
Dipanjan Das
Michael Denkowski
Emily Dinan
Markus Dreyer
Chris Dyer
Orhan Firat
Matt Gardner
Dan Garrette
Jianfeng Gao
David Grangier
Dilek Hakkani-Tur
Jayant Krishnamurthy
Guillaume Lample
Kenton Lee
Mike Lewis
Jiwei Li
Yunyao Li
Wolfgang Macherey
Matthew Marge
Swaroop Mishra
Tahira Naseem
Sujith Ravi
Sebastian Ruder
Noam Shazeer
Richard Socher
Linfeng Song
Kristina Toutanova
Ashish Vaswani
Svitlana Volkova
Zhilin Yang
Lingfei Wu
Manzil Zaheer
Lin Zhao (at Bosch Research in Sunnyvale)
Machine Learning
Academia
Yoshua Bengio
Jeff Dean
Lise Getoor
Amir Gholami
Tatsunori Hashimoto
Geoffrey Hinton
Sepp Hochreiter
‪Michael Jordan
Sham Kakade
Michael Mahoney
David McAllester
Tom Mitchell
Fei Sha
Jürgen Schmidhuber
Aarti Singh Google Scholar
Yee Whye Teh
Xinhua Zhang
Eric Xing
Industry
Leon Bottou
Yu (Hugo) Chen
Ronan Collobert
Ian Goodfellow
Alex Graves
Yuxiong He
Sanjiv Kumar
Quoc V. Le
Yann LeCun
Timothy Lillicrap
Leslie N. Smith
Ilya Sutskever
Jakob Uszkoreit
Oriol Vinyals
Jason Weston
Vision
Academia
Dhruv Batra
Justin Johnson (Vision and language)
Industry
Kaiming He
Andrej Karpathy
Optimization
Yuxin Chen
John Duchi
Satyen Kale"
Who published 'A Survey on Contextual Embeddings?',"
What is the main idea of the paper?","nlp:prompting [NLP Wiki]
Prompting and In-Context Learning
Overviews
Prompting Language Models
Zero-shot
Few-shot aka In-Context Learning
Soft-Prompting, etc
Prompt Design / Prompt Engineering
Calibration and Scoring
Data-Augmentation Prompting
Chain of Thought Prompting
Cross-lingual Prompting
Miscellaneous Promping Papers
Chained or Tool-based Prompting
Prompt Compression
Retrieval-Based Methods (Retrieval-Augmented)
Data Contamination Issues
Comparision to Fine-Tuning
Analysis of In-Context-Learning
Datasets
Software
Talks and Lectures
People
Related Pages
Prompting and In-Context Learning
Overviews
Liu et al 2021 - Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing
Dong et al 2022 - A Survey on In-context Learning
Qiao et al 2022 - Reasoning with Language Model Prompting: A Survey
Tutorials, Courses and Slides
Slides
UMass Amherst: Prompt-based learning
Stanford: Prompting, Instruction Finetuning, and RLHF
Blog: Lil'log Prompt Engineering
Github: BREX's Prompt Engineering Guide
Github: DAIR AI's Prompt Engineering Guide
Course: learnprompting.org
Prompting Language Models
Zero-shot
Radford et al 2019 - Language Models Are Unsupervised Multitask Learners GPT-2
Wei et al 2021 - Finetuned Language Models Are Zero-Shot Learners
Lyu et al 2022 - Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations
Few-shot aka In-Context Learning
Schick & Schütze 2020 - It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners
Schick & Schütze 2021 - Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference Introduces PET, pre-dates GTP-3
Brown et al 2021 - Language Models are Few-Shot Learners GPT-3
Gao et al 2021 - Making Pre-trained Language Models Better Few-shot Learners
Schick & Schütze 2020 - Few-Shot Text Generation with Natural Language Instructions GenPET, prompting for natural language generation
Soft-Prompting, etc
See Soft-prompting overview on p.3 of Zhao & Schütze 2021
Shin et al 2020 - AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts
Prefix-Tuning (aka P-Tuning): Liu et al 2021 - GPT understands, too Zhao 2021 finds this method to be the best.
Qin & Eisner 2021 - Learning How to Ask: Querying LMs with Mixtures of Soft Prompts
Prompt Tuning: Lester et al 2021 - The Power of Scale for Parameter-Efficient Prompt Tuning Can be seen as a “simplification of the recently proposed “prefix tuning” of Li and Liang (2021)”
Zhao & Schütze 2021 - Discrete and Soft Prompting for Multilingual Models They find that soft prompting with an LSTM like  Liu et al 2021 is best, both for English and cross-lingually.
Liu et al 2021 - P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks
Su et al 2021 - On Transferability of Prompt Tuning for Natural Language Processing
Khashabi et al 2021 - Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts
Tang et al 2022 - Context-Tuning: Learning Contextualized Prompts for Natural Language Generation
Vu et al 2022 - SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer - Multi-task, uses a library of learned soft prompts
Prompt tuning can be slower than fine-tuning.  See the figure below.
Figure from Su et al 2022.  See also figures 6-8 from Ding et al 2022.
Prompt Design / Prompt Engineering
See Prompt Engineering.
Calibration and Scoring
Holtzman et al 2021 - Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right
Zhou et al 2023 - Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering
Data-Augmentation Prompting
Wang et al 2022 - PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks
Chain of Thought Prompting
See also Reasoning Chains.
Overviews
Chu et al 2023 - A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future
Besta et al 2024 - Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts
Wei et al 2022 - Chain of Thought Prompting Elicits Reasoning in Large Language Models Introduced chain of thought prompting
Kojima et al 2022 - Large Language Models are Zero-Shot Reasoners Introduced the prompt “Let's think step by step.”
Wang et al 2022 - Self-Consistency Improves Chain of Thought Reasoning in Language Models Sample multiple chain of thought reasonings, and take the majority vote for the answer
Wang et al 2022 - Iteratively Prompt Pre-trained Language Models for Chain of Thought
Zelikman et al 2023 - STaR: Self-Taught Reasoner Bootstrapping Reasoning With Reasoning
Dohan et al 2022 - Language Model Cascades
Madaan & Yazdanbakhsh et al 2022 - Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango
Saparov & He 2022 - Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought
Yao et al 2022 - ReAct: Synergizing Reasoning and Acting in Language Models - The basis of LangChain
Wang et 2023 - Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models
Tree of Thought and Tree Search
Yao et al 2023 - Tree of Thoughts: Deliberate Problem Solving with Large Language Models
Long 2023 - Large Language Model Guided Tree-of-Thought
Feng et al 2023 - Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training
Chi et al 2024 - THOUGHTSCULPT: Reasoning with Intermediate Revision and Search
Li et al 2023 - Symbolic Chain-of-Thought Distillation: Small Models Can Also “Think” Step-by-Step
Wang & Zhao 2023 - Metacognitive Prompting Improves Understanding
in Large Language Models
Yasunaga et al 2023 - Large Language Models as Analogical Reasoners Adds to the prompt “# Instruction: ## Recall relevant exemplars: ## Solve the initial problem:”, which helps more than “Let's think step by step.”
Wang & Zhou et al 2024 - Chain-of-Thought Reasoning Without Prompting
Chen et al 2024 - Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models Masks the CoT to get better results
Cross-lingual Prompting
Zhao & Schütze 2021 - Discrete and Soft Prompting for Multilingual Models
Miscellaneous Promping Papers
Scao & Rush 2021 - How Many Data Points is a Prompt Worth? Prompts are very helpful in small data regimes, and are worth 100's of datapoints.
Khashabi et al 2021 - Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts. See also Webson & Pavlick 2021
Chained or Tool-based Prompting
For an overview see Tool Learning Papers
Overviews
Qin et al 2023 - Tool Learning with Foundation Models
Yao et al 2022 - ReAct: Synergizing Reasoning and Acting in Language Models. This kind of thing is implemented in LangChain
Schick et al 2023 - Toolformer: Language Models Can Teach Themselves to Use Tools
Qin et al 2023 - Tool Learning with Foundation Models
Qin et al 2023 - ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs
Uses RapidAPI
Prompt Compression
Mu et al 2024 - Learning to Compress Prompts with Gist Tokens
Retrieval-Based Methods (Retrieval-Augmented)
See Retrieval-Augmented Methods.
Data Contamination Issues
Overviews
Ravaut et al 2024 - How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library
Jacovi, et al 2023 - Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks
Li & Flanigan 2023 - Task Contamination: Language Models May Not Be Few-Shot Anymore
LLMSanitize: Ravaut et al 2024 - How Much are LLMs Contaminated? A Comprehensive Survey and the LLMSanitize Library
Zhang et al 2024 - A Careful Examination of Large Language Model Performance on Grade School Arithmetic
Drinkall et al 2024 - Time Machine GPT
Comparision to Fine-Tuning
Balaguer et al 2024 - RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture
Analysis of In-Context-Learning
Min et al 2022 - Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?
Garg et al 2022 - What Can Transformers Learn In-Context? A Case Study of Simple Function Classes
Akyürek et al 2022 - What learning algorithm is in-context learning? Investigations with linear models
Hendel et al 2023 - In-Context Learning Creates Task Vectors
Datasets
Datasets with Prompts for Evaluating Language Models
PromptSource: github Bach et al 2022 - PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts 2,000 prompts for 170 datasets
BIG-Bench: github Srivastava et al 2022 - Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models Growing list of user-submitted tasks.  Contains languages other than English
SuperNatural-Instructuctions: Wang et al 2022 - SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks 1,600 instructions for 76 tasks across 55 languages
BIG-Bench-Hard
LM-Evaluation Harness: github
Software
LangChain Framework for building applications with prompting (chaining prompts, etc). This paper was the basis for it: Yao et al 2022 - ReAct: Synergizing Reasoning and Acting in Language Models
Talks and Lectures
Invited Talk @ NAACL 2021: Humans Learn From Task Descriptions and So Should Our Models - Hinrich Schütze
People
Timo Schick
Related Pages
Instruction-Tuning
Few-Shot Learning
Fine-Tuning
Language Model
Meta-Learning
Pretraining
Prompt Engineering
Retrieval-Augmented Methods
Natural Language Task Descriptions
Zero-Shot Learning"
Recommend some research papers regarding pretraining.,"
To train a language model, it is generally recommended to start with a pre-trained model and fine-tune it on a specific task or dataset. Pretraining is a crucial step in language model training that can significantly improve the model's performance. However, pretraining on a large scale can be challenging, and it is essential to consider the amount of pretraining data, selection, and cleaning of pretraining data before proceeding.

Pretraining on an academic budget can be achieved with projects like Izsak et al 2021 and Geiping & Goldstein 2022, which demonstrate how to train a language model on a single GPU in one day. Additionally, papers like Zhang et al 2020 and Wang et al 2023 provide insights into pretraining methodology and properties of pre-trained models.

Fine-tuning a pre-trained model requires careful selection and cleaning of pretraining data, as well as consideration of the pretraining method. Papers like Krishna et al 2022 and Xie et al 2023 discuss the importance of downstream datasets in language model training and the use of importance resampling for data selection.

Pretraining on a large scale can be achieved with projects like GPT Neo and nanoT5, which use parallel training methods and pre-trained models. Papers like Zhao et al 2024 and Geiping & Goldstein 2022 discuss the use of gradient low-rank projection for pretraining and the importance of pretraining on academic budgets.

It is essential to consider the properties of pretrained models, such as their ability to generalize to new tasks and domains, and to carefully select and clean pretraining data to ensure the model is well-suited for the task at hand. Papers like Tu et al 2020 and Ye et al 2024 discuss the importance of scaling laws in pretraining and the use of data mixing laws for data selection.

Overall, pretraining is a critical step in language model training, and it is essential to carefully consider the amount of pretraining data, selection, and cleaning of pretraining data before proceeding. By following the recommendations outlined in these papers, researchers can achieve high-quality pretraining and fine-tuning results.

Recommended research papers:

* Izsak et al 2021: ""How to Train BERT with an Academic Budget""
* Geiping & Goldstein 2022: ""Training a Language Model on a Single GPU in One Day""
* Zhang et al 2020: ""When Do You Need Billions of Words of Pretraining Data?""
* Wang et al 2023: ""Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model""
* Krishna et al 2022: ""Downstream Datasets Make Surprisingly Good Pretraining Corpora""
* Xie et al 2023: ""Data Selection for Language Models via Importance Resampling""
* Ye et al 2024: ""Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance""
* Zhao et al 2024: ""Memory-Efficient LLM Training by Gradient Low-Rank Projection Like LoRA, but can be used for pre-training as well. ""","nlp:pretraining [NLP Wiki]
Pretraining
Overviews
Key and Early Papers
Contextualized Pretrained Models
Table of Large Models
Fine-Tuning Methods
Other Papers
Complex Pre-training Methods
Taxonomy of Pretraining Methods
Properties of Pretrained Models
Pretraining Methodology
Amount, Selection and Cleaning of Pretraining Data
Pretraining On An Academic Budget
Software
Related Pages
Pretraining
Overviews
Liu et al 2020 - A Survey on Contextual Embeddings
Qiu et al 2020 - Pre-trained Models for Natural Language Processing: A Survey Nice tables of pretraining methods on page 9 and 10, see Taxonomy of Pretraining Methods below.
Zhao et al 2023 - A Survey of Large Language Models
Key and Early Papers
For a history, see section 2.4 of Qiu 2020 or the related work in the GPT-2 paper.
Dai et al 2015 - Semi-supervised Sequence Learning
Ramachandran et al 2017 - Unsupervised Pretraining for Sequence to Sequence Learning
Peters et al 2018 - Deep Contextualized Word Representations
Devlin et al 2018 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Gururangan et al 2020 - Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks
Zhang et al 2020 - When Do You Need Billions of Words of Pretraining Data?
Krishna et al 2022 - Downstream Datasets Make Surprisingly Good Pretraining Corpora
Renduchintala et al 2023 - INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models
Wang et al 2023 - Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model
Contextualized Pretrained Models
Papers sorted chronologically.  For a large list of pre-trained models, see here.
CoVe: McCann et al 2017 - Learned in Translation: Contextualized Word Vectors
ELMO: Peters et al 2018 - Deep Contextualized Word Representations
GPT: Radford et al 2018 - Improving Language Understanding by Generative Pre-Training
BERT: Devlin et al 2018 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding original github
XLM: Lample et al 2019 - Cross-lingual Language Model Pretraining
GPT-2: Radford et al 2019 - Language Models are Unsupervised Multitask Learners original github Annotated GPT-2 Illustrated GPT-2 Interestingly, GPT-2 does not include a bias term in the final linear layer for the vocab, see here and here.
MASS: Song et al 2019 - MASS: Masked Sequence to Sequence Pre-training for Language Generation
XLNet: Yang et al 2019 - XLNet: Generalized Autoregressive Pretraining for Language Understanding
RoBERTa: Liu et al 2019 - RoBERTa: A Robustly Optimized BERT Pretraining Approach
CTRL: Keskar et al 2019 - CTRL: A Conditional Transformer Language Model for Controllable Generation
ALBERT: Lan et al 2019 - ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
T5: Raffel et al 2019 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
BART: Lewis et al 2020 - BART: Denoising Sequence-to-Sequence Pre-training for NaturalLanguage Generation, Translation, and Comprehension
XLM-R: Conneau et al 2019 - Unsupervised Cross-lingual Representation Learning at Scale
ELECTRA: Clark et al 2020 - ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
Longformer: Beltagy et al 2020 - Longformer: The Long-Document Transformer
MPNet: Song et al 2020 - MPNet: Masked and Permuted Pre-training for Language Understanding
GPT-3: Brown et al 2020 - Language Models are Few-Shot Learners
DeBERTa: He et al 2020 - DeBERTa: Decoding-enhanced BERT with Disentangled Attention “Improves the BERT and RoBERTa models using two novel techniques.”
MARGE: Lewis 2020 - Pre-training via Paraphrasing
BigBird: Zaheer et al 2020 - Big Bird: Transformers for Longer Sequences
ConvBERT: Jiang et al 2020 - ConvBERT: Improving BERT with Span-based Dynamic Convolution
Switch Transformer: Fedus et al 2021 - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Muppet: Aghajanyan et al 2021 - Muppet: Massive Multi-task Representations with Pre-Finetuning
Tay et al 2021 - Are Pre-trained Convolutions Better than Pre-trained Transformers?
XLM-E: Chi et al 2022 - LM-E: Cross-lingual Language Model Pre-training via ELECTRA
InstructGPT: Ouyang et al 2022 - Training Language Models to Follow Instructions with Human Feedback
Table of Large Models
List of popular models in chronological order.  See also the list of Large Language Models.
Model  Year  Type  Parameters  Training Data  Objective  Public?  Notes  Link
BERT  2018  Dec
T5  2019  Enc-Dec 11B  C4   Yes   github
BART   Enc-Dec

Fine-Tuning Methods
Moved to Fine-Tuning.
Other Papers
Bommasani et al 2021 - On the Opportunities and Risks of
Foundation Models Talks about the benefits and potential issues with pretrained models.
Wu et al 2022 - Insights into Pre-training via Simpler Synthetic Tasks
Complex Pre-training Methods
Pagnoni et 2022 -  Socratic Pretraining: Question-Driven Pretraining for Controllable Summarization - Pre-training on generated questions
Taxonomy of Pretraining Methods

Figure from Qiu 2020.

Figure from Liu 2020.

Figure from Qiu 2020.

Figure from Liu 2020.  Key:
LM: language modeling
MLM: masked language modeling
NSP: next sentence prediction
SOP: sentence order prediction
Discriminator (o/r): predict for each word if it was replaced (r ) or not (o, original)
seq2seq LM: given a prefix of words in a sequence, predict the rest of the sequence
Span Mask: predict masked words, where the masked words are contiguous (a span)
Text Infilling: Spans of words are replaced with a single mask token. Must predict all the words in the masked span.
Sent shuffling: Unshuffle a shuffled sentence
TLM: (Translation Language Modeling)  Tokens in both source and target sequences are masked for learning cross-lingual association.
Properties of Pretrained Models
Tu et al 2020 - An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models
Pretraining Methodology
See also scaling laws.
Blog posts:
Yi Tay - Training great LLMs entirely from ground up in the wilderness as a startup
Papers
Komatsuzaki 2019 - One Epoch Is All You Need
Izsak et al 2021 - How to Train BERT with an Academic Budget
Zhao et al 2024 - GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Like LoRA, but can be used for pre-training as well.  They pretrain a 7B model from scratch on an RTX 4090.
Zheng et al 2023 - GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond
System Descriptions
The following papers contain very useful descriptions of LLM pretraining methods and issues
Zhang et al 2022 - OPT: Open Pre-trained Transformer Language Models Discusses loss spikes, etc.
Kaddour 2023 - The MiniPile Challenge for Data-Efficient Language Models
OpenAI 2023 - GPT-4 Technical Report
Google 2023 - PaLM 2 Technical Report Talks about scaling laws, etc
2024 - Orion-14B: Open-source Multilingual Large Language Models
Groeneveld et al 2024 - OLMo: Accelerating the Science of Language Models
Llama Team 2024 - The Llama 3 Herd of Models
2024 - DeepSeek-V3 Technical Report
Amount, Selection and Cleaning of Pretraining Data
Overviews
Longpre et al 2023 - A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity
Amount
Zhang et al 2020 - When Do You Need Billions of Words of Pretraining Data?
2021 - How much pretraining data do language models need to learn syntax?
Selection
Krishna et al 2022 - Downstream Datasets Make Surprisingly Good Pretraining Corpora
Xie et al 2023 - Data Selection for Language Models via Importance Resampling The advantage of this method is it is very fast: data selection from the Pile can be done in 4 hours on a single computer.
Wang et al 2023 - Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model
Xie et a 2023 - DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining
Ye et al 2024 - Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance
Goyal et al 2024 - Scaling Laws for Data Filtering: Data Curation cannot be Compute Agnostic
Pretraining On An Academic Budget
Papers or projects where people have pretrained LLMs with academic compute budgets.
Izsak et al 2021 - How to Train BERT with an Academic Budget
Geiping & Goldstein 2022 - Training a Language Model on a Single GPU in One Day
Kaddour 2023 - The MiniPile Challenge for Data-Efficient Language Models
nanoT5
nanoGPT Reproduces GPT-2 (124M) on OpenWebText, running on a single 8XA100 40GB node in about 4 days of training.
Zhao et al 2024 - GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection Like LoRA, but can be used for pre-training as well.  They pretrain a 7B model from scratch on an RTX 4090.
Software
GPT Neo An open-source implementation of model & data parallel GPT3-like models using the mesh-tensorflow library.
Huggingface Transformers library has a large number of pre-trained models.  You can see a list in the github repo here
Related Pages
BERT and friends
Catastrophic Forgetting
Fine-Tuning
Language Model
Multimodal Pretraining
Prompting
Semi-supervised Learning
Word Embeddings"
Describe nanoGPT.,"
NanoGPT is a small, fast, and powerful transformer-based model developed by Andrej Karpathy. It's a variant of the BERT model, but with several key improvements that make it more efficient and effective for a variety of tasks, including language translation, question answering, and text summarization. nanoGPT is a compact and lightweight model that uses a novel architecture, which combines the strengths of different transformer variants to achieve better performance. It's also a great example of how a transformer model can be fine-tuned for specific tasks, and how a small model can be incredibly powerful. nanoGPT is a great choice for anyone looking for a fast and efficient transformer model that can handle a wide range of tasks.","nlp:transformers [NLP Wiki]
Transformers
Overview
Surveys
Transformer Properties
Analysis and Interpretation
Transformer Variants: Overviews
Improvements
Mixture of Expert (MoE) Transformers
Ablation Experiments on the Transformer
Pruning Attention Heads
Training
Efficient Transformers
Survey Papers
Papers
Datasets and Benchmarks
Long-Context Transformers
Papers
Position Embeddings
Software
Related Pages
Transformers
Overview
See Transformers in the ML Overview for introductory blog posts
Original paper: Vaswani et al 2017 - Attention Is All You Need
The Annotated Transformer
Textbook (SLP): Ch 9.7: Transformers
A walkthrough of transformer architecture code Contains a very good picture of the computation graph.
Surveys
Lin et al 2021 - A Survey of Transformers
Transformer Properties
Time and Space Complexity: The Transformer uses O(n^2) computation time, and O(n^2) memory (Subramanian et al 2019) due to the attention matrix.  However, the experiments in Subramanian et al 2019 (fig 2) seem to show a linear increase in memory usage with sequence length.  Presumably this is because the attention matrix does not dominate the memory footprint.
Expressiveness and Representation Power:
See also the group FLaNN (Formal Languages and Neural Networks).
Hahn 2019 - Theoretical Limitations of Self-Attention in Neural Sequence Models Indicates Transformers can't even represent finite state machines
Chiang et al 2022 - Overcoming a Theoretical Limitation of Self-Attention
Henderson 2020 - The Unstoppable Rise of Computational Linguistics in Deep Learning Argues why the Transformer is so good at language
Bhattamishra et al 2020 - On the Ability and Limitations of Transformers to Recognize Formal Languages
Elhage et al 2021 - A Mathematical Framework for Transformer Circuits
Merrill et al 2022 - Saturated Transformers are Constant-Depth Threshold Circuit
Transformer Programs
Weiss et al 2021 - Thinking Like Transformers
Lindner et al 2023 - Tracr: Compiled Transformers as a Laboratory for Interpretability
Friedman et al 2023 - Learning Transformer Programs
Delétang et al 2022 - Neural Networks and the Chomsky Hierarchy
Merrill et al 2022 - Transformers Can Be Translated to First-Order Logic with Majority Quantifiers
Liu et al 2022 - Transformers Learn Shortcuts to Automata
Analysis and Interpretation
See also Transformer Circuits.
Visualization of position embeddings in BERT and GPT-2 (from here)
Ramsauer et al 2020 - Hopfield Networks is All You Need
Geva et al 2020 - Transformer Feed-Forward Layers Are Key-Value Memories
Langedijk et al 2023 - DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers
For decoders/LLMs
Feucht et al 2024 - Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs Finds the implicit vocabulary in a Transformer decoder model
Transformer Programs
RASP: Weiss et al 2021 - Thinking like Transformers
Lindner et al 2023 - Tracr: Compiled Transformers as a Laboratory for Interpretability
Friedman et al 2023 - Learning Transformer Programs
Transformer Variants: Overviews
Blog post: Lil'Log: The Transformer Family
Narang et al 2021 - Do Transformer Modifications Transfer Across Implementations and Applications? Experimental comparison of Transformer model variants
Improvements
Shaw et al 2018 - Self-Attention with Relative Position Representations
Dai et al 2019 - Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context
So et al 2019 - The Evolved Transformer Neural architecture search for Transformer variants
Nguyen & Salazar 2019 - Transformers without Tears: Improving the Normalization of Self-Attention Many of these changes are default in the popular Transformer codebases
Shazeer 2019 - Fast Transformer Decoding: One Write-Head is All You Need twitter Used in AlphaCode to speed up decoding.
Liu et al 2020 - Very Deep Transformers for Neural Machine Translation
Subramanian et al 2020 - Multi-scale Transformer Language Models
Sukhbaatar et al 2019 - Adaptive Attention Span in Transformers Related to Milad's work.
Zemlyanskiy et al 2021 - ReadTwice: Reading Very Large Documents with Memories
Csordás et al 2021 - The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization
Wang et al 2022 - DeepNet: Scaling Transformers to 1,000 Layers
Dao et al 2022 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
Dao et al 2023 - Fash Decoding Speeds up flash attention for decoding. (Essentially, fixes a problem in the way decoding was implemented initially, so it's much faster. The new way is the more natural way it should have been implemented.)
Mixture of Expert (MoE) Transformers
Lepikhin, et al 2020 - GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding
Zhang et al 2021 - MoEfication: Transformer Feed-forward Layers are Mixtures of Experts
Ablation Experiments on the Transformer
These are ablation experiments on the Transformer, such as ablating the multi-head attention, or comparing to an LSTM with multi-head attention.
Chen et al 2018 - The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation Outperforms the Transformer with a stacked BiLSTM with multi-head attention and other tricks from the Transformer.  Slightly slower per token, but converges faster.
Domhan 2018 - How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures Tries combinations of Transformer, RNN, CNN decoder and encoder layers.  Shows that “one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention.” In particular, they find that:
“Source attention on lower encoder layers brings no additional benefit.”
“Multiple source attention layers and residual feed-forward layers are key.”
“Self-attention is more important for the source than for the target side.”
Simple Self-Attention Network (SSAN) Ambartsoumian & Popowich 2018 - Self-Attention: A Better Building Block for Sentiment Analysis Neural Network Classifiers  Compares the transformer to a 1-layer and 2-layer single-headed transformer layer.
RNN with attention back to previous states.  Has anyone compared this to the transformer?  I can't remember.
2019 - Tied Transformers: Neural Machine Translation with Shared Encoder and Decoder Shared weights for encoder and decoder.  Very natural if you consider the seq2seq transformer as a conditional language model.
Fixed (not learned) attention patterns in the encoder: Raganato et al 2020 - Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation
Linearizing the softmax in the attention - O(n) to compute: Katharopoulos et al 2020
No position embeddings (NoPE): 2023 - The Impact of Positional Encoding on Length Generalization in Transformers
Wang et al 2024 - Length Generalization of Causal Transformers without Position Encoding
Pruning Attention Heads
Michel et al 2019 - Are Sixteen Heads Really Better than One?
Behnke & Heafield 2020 - Losing Heads in the Lottery: Pruning Transformer Attention in Neural Machine Translation
Training
Popel & Bojar 2018 - Training Tips for the Transformer Model
Initialization issues
Zhang et al 2019 - Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention
Liu et al 2020 - Understanding the Difficulty of Training Transformers
Liu et al 2020 - Very Deep Transformers for Neural Machine Translation
Optimizer issues
Zhang et al 2019 - Why are Adaptive Methods Good for Attention Models?
Warm-up, see Warm-up
Normalization issues
Nguyen & Salazar 2019 - Transformers without Tears: Improving the Normalization of Self-Attention
RMSNorm. Improvement to layer normalization. Shown by Narang et al 2021 to work well for Transformers.
Xiong et al 2020 - On Layer Normalization in the Transformer Architecture Says pre-norm transformers don't need warm-up and are often better
Liu et al 2020 - Understanding the Difficulty of Training Transformers
Shen et al 2020 - PowerNorm: Rethinking Batch Normalization in Transformers
Wang et al 2022 - DeepNet: Scaling Transformers to 1,000 Layers
Miscellaneous topics
Merrill et al 2020 - Parameter Norm Growth During Training of Transformers
Efficient Transformers
Survey Papers
Tay et al 2020 - Efficient Transformers: A Survey
Tay et al 2020 - Long Range Arena: A Benchmark for Efficient Transformers
Papers
Guo et al 2019 - Star-Transformer
Rae et al 2019 - Compressive Transformers for Long-Range Sequence Modelling
Roy et al 2020 - Efficient Content-Based Sparse Attention with Routing Transformers
Hofstätter et al 2020 - Local Self-Attention over Long Text for Efficient Document Retrieval  Sliding window local attention mechanism
Wang et al 2020 - Linformer: Self-Attention with Linear Complexity
Katharopoulos et al 2020 - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention Misleading name.  This paper linearizes the softmax in the attention layers which makes it O(n) to compute
Choromanski et al 2020 - Rethinking Attention with Performers
Xiong et al 2021 - Nystromformer: A Nystrom-based Algorithm for Approximating Self-Attention  Similar to SVD, but approximately linearizes the softmax by selecting landmarks before the softmax.
Fedus et al 2021 - Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity
Peng et al 2021 - Random Feature Attention Uses random features to approximate the softmax, making it O(1).  Drop in replacement for standard attention. Experiments with the Transformer.
Lee-Thorp et al 2021 - FNet: Mixing Tokens with Fourier Transforms
Ma et al 2021 - Luna: Linear Unified Nested Attention
Hourglass Transformer: Hierarchical Transformers Are More Efficient Language Models - Has three blocks of layers: ones that downsample the tokens through pooling, ones that process, and ones that upsample.
FLASH: Hua et al 2022 - Transformer Quality in Linear Time
Nawrot et al 2022 - Efficient Transformers with Dynamic Token Pooling
Datasets and Benchmarks
https://github.com/google-research/long-range-arena|LRA (pronounced “ELRA”): Tay et al 2020 - Long Range Arena: A Benchmark for Efficient Transformers (Not just NLP tasks)
Long-Context Transformers
Papers
Dao et al 2022 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
Bertsch et al 2023 - Unlimiformer: Long-Range Transformers with Unlimited Length Input
Mohtashami et al 2023 - Landmark Attention: Random-Access Infinite Context Length for Transformers
Guo et al 2023 - LongCoder: A Long-Range Pre-trained Language Model for Code Completion
Chen et al 2023 - Extending Context Window of Large Language Models via Positional Interpolation
Dao 2023 - FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning
Yang 2023 - LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models
Li et al 2023 - LooGLE: Can Long-Context Language Models Understand Long Contexts?
Qin et al 2024 - Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models
Munkhdalai et al 2024 - Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
Han et al 2023 - LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models Expands the length limit to 200 million with no additional training and is O(n)
Levy et al 2024 - Same Task, More Tokens: the Impact of Input Length on the Reasoning Performance of Large Language Models
Position Embeddings
Learned position embeddings: Gehring et al 2017 - Convolutional Sequence to Sequence Learning
2022 - Randomized Positional Encodings Boost Length Generalization of Transformers (Submitted to ACL 2022, not accepted.)  Has good related work in section 4, comparison to prior work.
Su et al 2021 - RoFormer: Enhanced Transformer with Rotary Position Embedding
Press et al 2021 - Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation Uses exponential weighted decay in the attention to encode positional information
No Position Embeddings Transformer (NoPE): 2023 - The Impact of Positional Encoding on Length Generalization in Transformers
Chen et al 2023 - Extending Context Window of Large Language Models via Positional Interpolation
Software
See also Software.
Hugging Face Transformers Paper
Tutorials and example code
Tensorflow's transformer tutorial
The Annotated Transformer
nanoGPT Small version by Andrej Karpathy. Very cool
Related Pages
Attention Mechanisms
BERT and Friends
Seq2seq
State-Space Models"
"I want some recommendations for readings about prompting, specifically the engineering aspect.","
Recommended readings for Prompt Engineering, especially for the automatic and meta prompting techniques. I would like to explore these techniques in the context of large language models.

### Recommendations

I would like to explore the following papers and resources on prompt engineering:

*   **Automatic Prompt Engineering**:
    *   [Shin et al. 2020] - ""AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts""
    *   [Deng et al. 2022] - ""RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning""
    *   [Wen et al. 2023] - ""Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery""
*   **Meta Prompting**:
    *   [Yang et al. 2023] - ""Large Language Models as Optimizers Figures out a good prompt for the task""
    *   [Ferdando et al. 2023] - ""Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution""
    *   [Metaprompting] - ""A Complete Guide to Meta Prompting""
*   **Related Pages**:
    *   [Arora et al. 2022] - ""Ask Me Anything: A Simple Strategy for Prompting Language Models""
    *   [Yasunaga et al. 2023] - ""Large Language Models as Analogical Reasoners""
    *   [Aravind et al. 2022] - ""Prompt Engineering for Conversational AI""

These resources will provide a solid foundation for understanding the engineering aspects of prompt engineering, including automatic and meta prompting techniques, and help you explore these topics in the context of large language models.","nlp:prompt_engineering [NLP Wiki]
Prompt Engineering
Introductions and Overviews
Papers
Automatic Prompt Engineering
Examples of Prompts
Related Pages
Prompt Engineering
Introductions and Overviews
Awesome ChatGPT Prompts
Papers
Note: some of these papers should be moved to automatic prompt engineering.
Mishra et al 2022 - Reframing Instructional Prompts to GPTk’s Language
Sorensen et al 2022 - An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels
Webson & Pavlick 2021 - Do Prompt-Based Models Really Understand the Meaning of Their Prompts?
Wang et al 2022 - Self-Instruct: Aligning Language Model with Self Generated Instructions
Cheng et al 2023 - Batch Prompting: Efficient Inference with Large Language Model APIs Batch prompting to save money
Yin et al 2023 - Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning
Chen et al 2023 - Exploring Lottery Prompts for Pre-trained Language Models Searches to see if there is a prompt for each example that produces the correct answer
Li et al 2023 - Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models
Wang & Zhao 2023 - Metacognitive Prompting Improves Understanding
in Large Language Models
Automatic Prompt Engineering
Shin et al 2020 - AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts
Deng et al 2022 - RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning
Wen et al 2023 - Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery
Pryzant et al 2023 - Automatic Prompt Optimization with ""Gradient Descent"" and Beam Search
Yang et al 2023 - Large Language Models as Optimizers Figures out a good prompt for the task
Ferdando et al 2023 - Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution
Metaprompting
Blog posts, etc
A Complete Guide to Meta Prompting
Meta-prompting
Meta-Prompting Concept: Asking Chat-GPT for the best prompt for your desired completion, then to revise it before using it
2022 - MetaPrompting: Learning to Learn Better Prompts
2024 - Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding
Examples of Prompts
Examples of clever or interesting prompts.
See appendix of https://arxiv.org/pdf/2307.16789.pdf
Yasunaga et al 2023 - Large Language Models as Analogical Reasoners
Arora et al 2022 - Ask Me Anything: A Simple Strategy for Prompting Language Models
Awesome ChatGPT Prompts
Related Pages
Prompting"
Where can I find some interesting prompts?,"
You can find some interesting prompts in various places. Here are a few suggestions:

* Papers and research articles
* Online forums and communities
* Blogs and websites
* Social media platforms
* Books and academic papers
* Online courses and tutorials
* ChatGPT and other AI chat platforms

These platforms provide a wealth of information on how to design effective prompts, and can help you generate unique and interesting prompts for your language models. You can also find inspiration from other sources, such as the work of researchers and experts in the field of natural language processing.","nlp:prompt_engineering [NLP Wiki]
Prompt Engineering
Introductions and Overviews
Papers
Automatic Prompt Engineering
Examples of Prompts
Related Pages
Prompt Engineering
Introductions and Overviews
Awesome ChatGPT Prompts
Papers
Note: some of these papers should be moved to automatic prompt engineering.
Mishra et al 2022 - Reframing Instructional Prompts to GPTk’s Language
Sorensen et al 2022 - An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels
Webson & Pavlick 2021 - Do Prompt-Based Models Really Understand the Meaning of Their Prompts?
Wang et al 2022 - Self-Instruct: Aligning Language Model with Self Generated Instructions
Cheng et al 2023 - Batch Prompting: Efficient Inference with Large Language Model APIs Batch prompting to save money
Yin et al 2023 - Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning
Chen et al 2023 - Exploring Lottery Prompts for Pre-trained Language Models Searches to see if there is a prompt for each example that produces the correct answer
Li et al 2023 - Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models
Wang & Zhao 2023 - Metacognitive Prompting Improves Understanding
in Large Language Models
Automatic Prompt Engineering
Shin et al 2020 - AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts
Deng et al 2022 - RLPrompt: Optimizing Discrete Text Prompts with Reinforcement Learning
Wen et al 2023 - Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery
Pryzant et al 2023 - Automatic Prompt Optimization with ""Gradient Descent"" and Beam Search
Yang et al 2023 - Large Language Models as Optimizers Figures out a good prompt for the task
Ferdando et al 2023 - Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution
Metaprompting
Blog posts, etc
A Complete Guide to Meta Prompting
Meta-prompting
Meta-Prompting Concept: Asking Chat-GPT for the best prompt for your desired completion, then to revise it before using it
2022 - MetaPrompting: Learning to Learn Better Prompts
2024 - Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding
Examples of Prompts
Examples of clever or interesting prompts.
See appendix of https://arxiv.org/pdf/2307.16789.pdf
Yasunaga et al 2023 - Large Language Models as Analogical Reasoners
Arora et al 2022 - Ask Me Anything: A Simple Strategy for Prompting Language Models
Awesome ChatGPT Prompts
Related Pages
Prompting"
What is some question-answering research?,"
The field of question-answering research focuses on developing systems that can effectively answer questions in a wide range of domains, from general knowledge to specific domains like medicine or finance. Researchers have developed various techniques, such as machine learning, deep learning, and hybrid approaches, to improve the accuracy and efficiency of question-answering systems. Some notable research areas include: (1) understanding the nuances of natural language, (2) developing more accurate question-answering models, (3) creating more diverse and representative question datasets, and (4) exploring new techniques for question generation and retrieval. Additionally, researchers have also investigated the impact of domain adaptation and transfer learning on question-answering systems, and the role of implicit reasoning and common sense in answering questions. Overall, the field of question-answering research continues to evolve, with new breakthroughs and advancements in various research areas.","nlp:question_answering [NLP Wiki]
Question Answering
Overviews
Demos
Key Papers
Topics
General QA Papers
Explanation And Implicit Reasoning Papers
QA with Attribution
Robust Question Answering
Open-Domain Question Answering
Multi-hop Reasoning
Multi-Span QA
Yes/No Questions
Long-Form QA
Knowledge-Grounded QA
Commonsense QA
Selective QA
Domain Shift
Domain Adaptation
Cross-Lingual
Unsupervised QA
Evaluation
Datasets
Resources
Slides
People
Related Pages
Question Answering
Overviews
Best overview: Baradaran et al 2020 - A Survey on Machine Reading Comprehension Systems.
Gao et al 2018 - Neural Approaches to Conversational AI (contains a chapter on QA)
Baradaran et al 2020 - A Survey on Machine Reading Comprehension Systems
Thayaparan et al 2020 - A Survey on Explainability in Machine Reading Comprehension
Rogers et al 2022 - QA Dataset Explosion: A Taxonomy of NLP Resources for Question Answering and Reading Comprehension
Demos
AllenNLP - RoBERTa QA Model Online Demo
Key Papers
Early papers
Riloff & Thelen 2000 - A Rule-based Question Answering System for Reading Comprehension Tests (Cited by the SQuAD 1.0 paper)
Chen et al 2016 - A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task
Rajpurkar et al 2016 - SQuAD: 100,000+ Questions for Machine Comprehension of Text
BiDAF model
Rajpurkar et al 2018 - Know What You Don't Know: Unanswerable Questions for SQuAD (SQuAD 2.0 paper)
Karpukhin et al 2020 - Dense Passage Retrieval for Open-Domain Question Answering
Basmov et al 2024 - LLMs’ Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements
Topics
General QA Papers
Andreas et al 2016 - Learning to Compose Neural Networks for Question Answering
Basmov et al 2024 - LLMs’ Reading Comprehension Is Affected by Parametric Knowledge and Struggles with Hypothetical Statements
Explanation And Implicit Reasoning Papers
Chen & Durrett 2020 - Robust Question Answering Through Sub-part Alignment
Lamm et al 2020 - QED: A Framework and Dataset for Explanations in Question Answering
Thayaparan et al 2020 - A Survey on Explainability in Machine Reading Comprehension
Geva et al 2021 - Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies
Dalvi et al 2021 - Explaining Answers with Entailment Trees
QA with Attribution
Bohnet et al 2022 - Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models
Robust Question Answering
Chen & Durrett 2020 - Robust Question Answering Through Sub-part Alignment
Open-Domain Question Answering
Lee et al 2019 - Latent Retrieval for Weakly Supervised Open Domain Question Answering
DPR model: Karpukhin et al 2020 - Dense Passage Retrieval for Open-Domain Question Answering
Lee et al 2021 - Learning Dense Representations of Phrases at Scale
Multi-hop Reasoning
Yang et al 2018 - HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering
Dua et al 2019 - DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs
Min et al 2019 - Multi-hop Reading Comprehension through Question Decomposition and Rescoring Decomposes questions into simpler questions, answers them, and then rescores the answer
Sun et al 2020 - PullNet: Open Domain Question Answering with Iterative Retrieval on Knowledge Bases and Text
Multi-Span QA
Span-based QA datasets like SQuAD require that the answer span is contiguous.  Multi-Span QA relaxes this restriction so that questions like “What are the type of Turing Machines?” can be answered with multiple spans from the context passage.
Segal et al 2019 - A Simple and Effective Model for Answering Multi-span Questions
Yes/No Questions
Clark et al 2019 - BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions
Sulem et al 2021 - Yes, No or IDK: The Challenge of Unanswerable Yes/No Questions
Long-Form QA
Fan et al 2019 - ELI5: Long Form Question Answering
Knowledge-Grounded QA
See Knowledge-Grounded Question Answering.
Commonsense QA
See Commonsense Question Answering.
Selective QA
2020- No Answer is Better Than Wrong Answer: A Reflection Model for Document Level Machine Reading Comprehension Top leaderboard on Natural Questions
2020 - Selective Question Answering under Domain Shift
2019 - Quizbowl: The case for incremental question answering.
Domain Shift
Selective Question Answering under Domain Shift
Domain Adaptation
See the related work in Yue 2022 and Yue 2022, and also Arafat Sultan's publications on QA.
Synthetic Question Generation
See also Question Generation.
Duan et al 2017 - Question Generation for Question Answering
Tang et al 2017 - Question Answering and Question Generation as Dual Tasks
Shakeri et al 2020 - End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems Generates QA pairs in the target domain using encoder-decoder large pre-trained model fine-tuned to the dataset.
Reddy et al 2020 - End-to-End QA on COVID-19: Domain Adaptation with Synthetic Training
Shakeri et al 2020 - End-to-End Synthetic Data Generation for Domain Adaptation of Question Answering Systems
Chen et al 2020 - Improved Synthetic Training for Reading Comprehension
2020 - CliniQG4QA: Generating Diverse Questions for Domain Adaptation of Clinical Question Answering
Lyu et al 2021 - Improving Unsupervised Question Answering via Summarization-Informed Question Generation
Yue et al 2022 - Synthetic Question Value Estimation
for Domain Adaptation of Question Answering
Reddy et al 2022 - Synthetic Target Domain Supervision for Open Retrieval QA
Yue et al 2022 - QA Domain Adaptation using Hidden Space Augmentation and Self-Supervised Contrastive Adaptation
Cross-Lingual
Agarwal et al 2022 - Zero-shot Cross-lingual Open Domain Question Answering
Unsupervised QA
Lyu et al 2021 - Improving Unsupervised Question Answering via Summarization-Informed Question Generation
Evaluation
Rodriguez et al 2021 - Evaluation Examples Are Not Equally Informative: How Should That Change NLP Leaderboards?
Datasets
See Rogers et al 2022 - QA Dataset Explosion. See also NLP Progress - Question Answering and Geetanjali's QA Datasets Spreadsheet
CNN/Daily Mail Reading Comprehension
Large-scale cloze-style QA dataset constructed from news articles and their summaries
Paper: Hermann 2015 - Teaching Machines to Read and Comprehend
Chen et al 2016 - A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task
See https://github.com/danqi/rc-cnn-dailymail
SQuAD: Rajpurkar et al 2016 - SQuAD: 100,000+ Questions for Machine Comprehension of Text SQuAD 2.0: Rajpurkar et al 2018 - Know What You Don’t Know: Unanswerable Questions for SQuAD
NewsQA: Trischler et al 2016 - NewsQA: A Machine Comprehension Dataset.
NarrativeQA: Kočiský et al 2017 - The NarrativeQA Reading Comprehension Challenge
AI2 Reasoning Challenge (ARC): Clark et al 2018 - Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge
Boratko et al 2019 - A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset
MathQA: Amini et al 2019 - MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms
Math Questions
Hopkins et al 2019 - SemEval 2019 Task 10: Math Question Answering
Liu et al 2021 - Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches
COPA: Choice of Plausible Alternatives (Gordon et al., 2012): Asking about either a plausible cause or a plausible result, among two alternatives, of a certain event expressed in a simple sentence. (Summary from Shwartz 2020)
CommonSenseQA: commonsense Question Answering (Talmor et al., 2019): General questions about concepts from ConceptNet. To increase the challenge, the distractors are related to the target concept either by a relationship in ConceptNet or as suggested by crowdsourcing workers. (Summary from Shwartz 2020)
MC-TACO: Multiple Choice Temporal commonsense (Zhou et al., 2019): Questions about temporal aspects of events such as ordering, duration, frequency, and typical time. The distractors were selected in an adversarial way using BERT. (Summary from Shwartz 2020)
Social IQa: Social Interaction Question Answering (Sap et al., 2019b): Questions regarding social interactions, based on the ATOMIC dataset (Sap et al., 2019a). Contexts describe social interactions and questions refer to one of a few aspects (e.g. the subject’s motivation, following actions, etc.). The answers were crowdsourced. (Summary from Shwartz 2020)
PIQA: Physical Interaction Question Answering (Bisk et al., 2020): Questions regarding physical commonsense knowledge. Contexts are goals derived from an instruction website, typically involving less prototypical uses of everyday objects (e.g., using a bottle to separate eggs). The answers were crowdsourced, and an adversarial filtering algorithm was used to remove annotation artifacts. (Summary from Shwartz 2020)
WinoGrande (Sakaguchi et al., 2020): A large scale version of WSC that exhibits less bias thanks to adversarial filtering and use of placeholders instead of pronouns. As opposed to WSC that was curated by experts, WinoGrande was crowdsourced with a carefully designed approach that produces diverse examples which are trivial for humans. (Summary from Shwartz 2020)
HybridQA: Chen et al 2020 - HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data
UnifiedQA: Khashabi et al 2020 - UnifiedQA: Crossing Format Boundaries With a Single QA System
Open Domain
Natural Questions
Stats: ~300k QA pairs
Dataset viewer
Bert Baseline Paper: Alberti et al 2019 - A BERT Baseline for the Natural Questions
System Papers
2020 - RikiNet: Reading Wikipedia Pages for Natural Question Answering
Zemlyanskiy et al 2021 - ReadTwice: Reading Very Large Documents with Memories
MS MARCO Bajaj et al 2016 - MS MARCO: A Human Generated MAchine Reading COmprehension Dataset List of QA datasets from MS MARCO paper:
QReCC Anantha et al 2020 - Open-Domain Question Answering Goes Conversational via Question Rewriting
AmbigQA: Min et al 2021 - AmbigQA: Answering Ambiguous Open-domain Questions
QuAC
System Papers
Qu et al 2019 - BERT with History Answer Embedding for Conversational Question Answering
Biomedical Domain
BioASQ
CovidQA Paper 1 Paper 2 (older)
Product Domain (Product-related Question Answering - PQA)
Amazon-PQA and AmazonPQSim: Rozen et al 2021 - Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products Worked with Yes/No questions only.  Datasets available here (search for Amazon-PQA or AmazonPQSim). Dataset incluses free-form, not extractive answers and yes/no questions.
Research Paper Domain
Qasper: Dasigi et al 2021 - A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers This dataset is really hard. Contains everything: abstractive, multispan extractive, yes / no, unanswerable questions, all from a very long context.
Resources
Slides
Geetanjali's slides and here
People
Jonathan Berant
William Cohen
Matt Gardner
Edward Grefenstette
Percy Liang
Chris Manning
Andrew McCallum
Hwee Tou Ng
Chitta Baral
Related Pages
Question Generation
Visual Question Answering"
What category does the SQuAD dataset paper fall under?,"
The SQuAD dataset paper falls under the category of Natural Language Processing (NLP) and Machine Learning (ML) as part of the broader category of Computer Vision (CV) and related fields. Specifically, it is a type of multi-task learning problem where the model is trained on a combination of text and image data, with the goal of predicting the output of the image based on the input text.","ml:ml_overview [NLP Wiki]
Machine Learning Overview
Books
Courses
Overview of Topics
Related Pages
Machine Learning Overview
This page is a concise overview of topics in machine learning, with links to readings and other learning materials.  Roughly, these topics are the union of topics covered in various ML books and courses.
This is a resource to help you get up to speed in various topics if you're trying to learn ML on your own or broaden your ML knowledge.
See also Aman.ai - AI Fundamentals
Books
Pattern Recognition and Machine Learning, Bishop, 2006 (Referenced below as Bishop) available here or local copy
An Introduction to Statistical Learning (Reference below as ISL) available here or local copy
The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2009 (Referenced below as ESL) available here or local copy
CIML (Referenced below as CIML) available here
Machine Learning, Tom Mitchell, McGraw Hill, 1997 (Referenced below as MLBook) available here
Machine Learning Books by Kevin Murphy
Machine Learning A Probabilistic Perspective, 2012 (referenced below as Murphy) available here
Probabilistic Machine Learning: An Introduction, 2021 (reference below as PML1) available here or local copy
Deep Learning Book (Referenced below as DLBook)
A Primer on Neural Network Models for Natural Language Processing, Yoav Goldberg, 2016 local copy (Referenced below as NNPrimer)
The Matrix Cookbook, available here
Intro to ML (note this was released April 1st)
Courses
Machine Learning for Intelligent Systems @ Cornell
Very quick intro to machine learning (slides): Introduction to Machine Learning local copy
Overview of Topics
This overview contains links to particular pages in textbooks, lectures, blog posts, and videos covering the topic, listed easiest to hardest to understand, with videos listed at the end.  In other words, for each topic, introductory material is listed first with more advanced material afterwards, although you may find more advanced material easier to understand in some cases.
The blog posts and some of the videos are introductory and give the overall gist of the method, but may contain mathematical or conceptual errors. Videos that are lectures should be fine.
Introduction to Machine Learning MLBook p. 1-15 PML p. 1-28
“Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.” Arthur Samuel 1959
Basic Machine Learning Concepts
Inductive Bias MLBook p. 39-45
Overfitting/Underfitting
Approximation error vs estimation error aka Bias-Variance Tradeoff CIML p. 71-72 Bartlett notes Sometimes also called the bias-variance tradeoff.
Features
Hyperparameters
Train/dev/test split
Don't look at the test data CIML p. 25 “Do not look at your test data. Even once. Even a tiny peek. Once you do that, it is not test data any more. Yes, perhaps your algorithm hasn’t seen it. But you have. And you are likely a better learner than your learning algorithm.”
Additional ML Topics
Generative vs Discriminative Classifiers
“Generative Story” CIML p. 123-124
Bayesian statistics
MLE vs MAP estimation (and examples of MAP in machine learning) Blog
Classification
Naive Bayes CIML p. 120-123, MLBook p. 5
Note: Naive Bayes is a generative classifier - it estimates p(x,y).  It can be used for binary or multiclass classification.  A Naive Bayes classifier for documents where the input features are words is called a “Bag of Words model”
Logistic regression (LR) and Naive Bayes have the same model form, but Naive Bayes maximizes p(x,y) while LR maximizes p(y|x).  See Vivek's NB Note or MLBook p. 14
Logistic Regression MLBook p. 7-14 ESL p. 119-122
*Decision Trees Blog1 Blog2 CIML p. 10-18 ESL p.305-310
*Random Forests ISLR p. 319-321
*k Nearest Neighbors (k-NN) Blog CIML p. 29-40 Bishop p. 125-127 (starting from “We close this chapter by showing how the K-nearest-neighbour technique for density estimation can be extended to the problem of classification…”)
Perceptron Blog Lecture (w/ video) CIML p. 41-54 MLBook p. 86, Bishop p. 192-196, Wikipedia
The perceptron algorithm is actually minimizing a function of the data. It turns out, stochastic gradient descent (SGD) with stepsize 1 on a particular function of the data (called the perceptron loss function) is exactly the perceptron algorithm.
There is a multiclass version of the perceptron
Neural Networks NNPrimer p. 354-379 Bishop p. 225-272 CIML p. 129-140 ESL 392-401
Bayesian Neural Networks Bishop p. 277-284
Support Vector Machines Blog1 Blog2 CIML p. 100-103 Bishop p.325-345 ISL p. 337-349 ESL p. 417-422
Either the primal or the dual version of the SVM optimization problem can be used.  Historically, the dual version was used.  However, the dual must be optimized using a specialized optimization algorithm such as sequential minimal optimization (SMO), while the unconstrained version of the primal can be optimized using any gradient-based optimizer such as stochastic gradient descent (SGD), which is usually faster in practice.  For this reason, for large-scale learning, the primal version with a gradient-based optimizer is often preferred.  See SVM.
*Kernel Methods Blog ISL p. 350-354 CIML p. 141-148 ESL p. 423-438 Kernel methods can also be used for regression.
Loss Functions and Training
Regression
*Linear Regression Blog1 Blog2 ESL p. 43-51 Bishop p. 137-147 ISL p. 59-119
MAP vs MLE linear regression (MAP adds a regularizer term)
Bayesian linear regression
“Non-Linear” Regression
Polynomial Regression ISL p. 266-268
Splines Notes ISL p. 271-280 ESL p. 139-181
Practicalities
*Hyperparameters and Model Selection
Train/dev/test split Video
The most practical and principled way to select the model and hyperparameters is on a development set.
Feature Selection and Feature Engineering CIML p. 55-62
Regularization
Early-stopping
L2 regularization
L1 regularization
Pruning (for decision trees)
Evaluation
Accuracy
Precision, Recall, F1: Macro vs Micro averaging
*Area Under the Curve (AUC) Blog
Tests of Significance CIML p. 67-69
Data Resampling Methods
k-fold Cross Validation Be careful using this method on NLP datasets! Due to the non-IID nature of NLP datasets, it is generally not recommended to use k-fold cross validation (can over-estimate performance).  Better to use a thoughtfully-chosen train/dev/test split.
*Bootstrap Resampling
Jacknife
Debugging ML CIML p. 69-71 Blog
Deep Learning
NN Architectures
Feedforward NNs
Convolutional NNs (CNNs) Blog1 Blog2 DLBook Video
Recurrent NNs (RNNs) Blog1 Blog2 Blog3 Blog4 (covers RNNs, LSTMs, Attention, Transformers DLBook
*Attention Blog1 Blog2 Blog3 NMT p. 48-52 Attention
*Transformers Blog1 Blog2 Blog3 Paper (hard) Annotated Transformer (perhaps easier)
Training Methods Video
Generative Adversarial Networks (GANs)
*Reinforcement Learning
Graphical Models
Bayesian Networks Bishop p. 360
Hidden Markov Models (HMMs)
Undirected Graphical Models (MRFs and CRFs) Bishop p. 383
Linear-chain Conditional Random Fields
Factor Graphs Bishop p. 399
Inference
Variable Elimination
Belief Propagation (Sum-Product and Max-Product Algorithms) Bishop p. 402-415
Junction Tree Algorithm
Loopy Belief Propagation Bishop p. 417-418
Variational Inference
Sampling Methods
Combining Models
Ensembling
Mixture of Experts
*Boosting
Bayesian Model Averaging
Bagging ISL p. 316-318
Unsupervised Methods
Density Estimation
*EM Algorithm Blog MLBook p. 191-196 ESL p. 272-279 Murphy p. 348-359 Lecture Notes (covers hard and soft EM and application to HMMs) Video (EM for Gaussian Mixture Models)
There is both soft EM (soft assignment, the usual version) and hard EM (hard assignment during E-step).  Both versions “work” in that they will both converge to a local maximum.  The hard EM version can converge faster but sometimes doesn't work as well (see here or here).
There are also online versions of EM and other variants, see Murphy p. 365-369
*Clustering
K-Means Clustering Blog1 Blog2 ISL p. 386-390 Murphy p. 352-354 Video: 1 2 3
K-Means is an instance of the hard EM algorithm, see Lecture Notes
Hierarchical Clustering
Agglomorative Clustering
*Principle Component Analysis (PCA)
Structured Prediction
Structured Perceptron
Structured SVM
Conditional Random Fields (CRFs)
Probability and Statistics Background
Terminology
Probability Distribution (referred to as just a “Distribution”)
To sample from a probability distribution
Parameters
Random Variable
Independent
Independent and Identically Distributed (IID)
Joint Distribution
Marginal Distribution (referred to as just a Marginal).  Also to marginalize
To compute a marginal, you marginalize (sum) over the other random variables
Probability Distributions: Uniform, Normal, Poisson, Binomial, etc
*Bias-Variance Decomposition Lecture, Notes This is a statistics term, used when analyzing mean squared error in regression or density estimation, for example. In machine learning, it's more properly called approximation error (≈ bias) and estimation error (≈ variance) because you can't compute the bias (E[y]) or variance E[(y - E[y])^2] for non-numeric outputs like classes in multi-class classification.  However, these terms are often applied to ML somewhat loosely.
Density Estimation
Histograms
Kernel Density Estimators
Gaussian Processes
Theory
Concept Learning
Hypothesis Space
Inductive Bias MLBook p. 39-45
Bias-Variance Tradeoff
VC dimension
NP hardness of Learning
PAC Learning Theory
PAC-Bayesian Learning Theory
Information Theory Murphy p. 56-61
Entropy
Cross-entropy
Mutual Information
KL-Divergence
Software
R
scikit-learn
TensorFlow
PyTorch
NLTK
SpaCy
OpenCV"
Andrej Karpathy created what software?,"
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?
Andrej Karpathy created YOLO?","nlp:history_of_nlp [NLP Wiki]
History of NLP
Historical Surveys
Papers and Popular Descriptions
Early Work (prior to 2000)
Early Machine Learning or Corpus-based Methods in NLP
Neural Networks in NLP
Very Early Work
Early Deep Learning in NLP
Related Pages
History of NLP
Historical Surveys
Zechner 1997 - A Literature Survey on Information Extraction and Text Summarization
Papers and Popular Descriptions
Statistical NLP
Church 1988 - A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text One of the first statistical POS taggers, one of the papers that started the statistical/machine learning revolution in NLP
Charniak 1998 - Statistical Techniques for NLP
Early Work (prior to 2000)
Models of Language
Chomsky 1956 - Three Models for the Description of Language overview Introduced the Chomsky hierarchy
Chomsky & Miller 1958 - Finite State Languages. From here.
Chomsky 1959 - On Certain Formal Properties of Grammars
Chomsky & Schützenberger
1963 - The Algebraic Theory of Context Free Languages
Machine Translation
Interlingua-based
Nirenburg et al 1988 - Lexical Realization in Natural Language Generation Describes the generation system of DIOGENES.
Question Answering
Lehnert 1975 - What Makes Sam Run? Script Based Techniques for Question Answering Very cool work.
Riloff & Thelen 2000 - A Rule-based Question Answering System for Reading Comprehension Tests
Dialog
ELIZA: Weizenbaum 1966 - ELIZA - A Computer Program For the Study of Natural Language Communication Between Man And Machine wikipedia original source code commented source Python reimplementation other versions
PERRY: wikipedia source code PERRY was the first program to pass the Turing test.
Bobrow 1977 - GUS, A Frame-Driven Dialog System
Green & Carberry 1994 - A Hybrid Reasoning Model For Indirect Answers
Word-Sense Disambiguation
Granger 1977 - FOUL-UP: A Program that Figures Out Meanings of Worcds from Context Really cool work. Uses “knowledge embodied in scripts to figure out likely definitions for unknown words.” Related to recent (2020s) work in common-sense reasoning.
Speech Recognition
Included here since some of the algorithms are shared with statistical NLP methods
Denes 1960 - Spoken Digit Recognition Using Time‐Frequency Pattern Matching
Word-based matching, cited by Bridle 1982
Vintsyuk 1968 - Speech Discrimination by Dynamic Programming pdf (UCSC only) Bridle 1982 says this was pioneering work which was not well known in the West.
Velichko & Zagoruyko 1970 - Automatic Recognition of 200 Words pdf (UCSC only) Cited by Bridle 1982
Jelinek 1976 - Continuous Speech Recognition by Statistical Methods pdf (UCSC only)
Lowerre 1976 - The Harpy Speech Recognition System (Ph.D. Thesis) pdf Summary, missing one page. Cited Bridle 1982 (and Ney 1992) for the term “beam search”
Bridle et al 1982 - An Algorithm for Connected Word Recognition pdf (UCSC only) Cited by Ney 1992 for beam search
Ney et al. 1992 - Data Driven Search Organization for Continuous Speech Recognition in the SPICOS System pdf (UCSC only) See p. 4 bottom for a history of beam search, which it says is called “beam search, DP beam search, or pruned DP search.”
Generation
Nirenburg et al 1988 - Lexical Realization in Natural Language Generation Describes the generation part of the interlingua MT system DIOGENES.
Text Understanding
Shank 1975 - SAM - A Story Understander
Syntactic Parsing
Semantic Parsing
Schank & Tesler 1969 - A Conceptual Dependency Parser for Natural Language
Grammar Induction
Systems that Learned
Reasoning Systems
Carbonell 1978 - POLITICS: Automated Ideological Reasoning
Carbonell 1979 - Subjective Understanding: Computer Models of Belief Systems PhD Thesis
Language Acquisition
Smith 1980 - FOCUSER: A strategic interaction paradigm for language acquisition. AAIII 1980, cited by Mitchell 1980.  Also published as a PhD thesis.
Early Machine Learning or Corpus-based Methods in NLP
Overviews
Wermter et al 1996 - Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing Nice overview, part of a larger book
Parsing
Carbonell 1979 - Towards a Self-Extending Parser
Sampson 1986 - A stochastic approach to parsing Learns statistical rules from a manually annotated corpus. Uses simulated annealing to find the most probable parse. (Randomized inference, similar to later work in NLP in 2014 here) “We have built up a database of manually-parsed sentences, from which we extract statistics that allow a likelihood measure to be determined for any logically possible non-leaf constituent of a parse-tree. That is, given a pairing of a mother-label with a sequence of daughter-labels, say the pair <J, NN JJ P>, the likelihood function will return a figure for the relative frequency with which (in this case) an adjective phrase consists of singular common noun + adjective + prepositional phrase.” “The most direct way… would be to generate all possible tree-structures for a given sentence taken as a sequence of word-tags, and all possible labellings of each of those structures, and choose the tree whose overall plausibility figure is highest. Unlike in the case of word-tagging, however, for parsing this approach is wholly impractical… I have therefore begun to experiment with simulated annealing as a solution to the problem.”
1990 - Session 9: Automatic Acquisition of Linguistic Structure From HLT (became NAACL) 1990, see also dblp (and Mitch Marcus's google scholar)
Machine Translation
Brown et al 1988 - A Statistical Approach to French/English Translation Cited by Gale & Church 1990. Reflection on the work here
Neural Networks in NLP
Very Early Work
Work prior to 2000s.
Often called “Artificial Neural Networks (ANNs)” or “connectionist approach” in the old literature
Overviews
Rohde & Plaut 2003 - Connectionist Models of Language Processing Great overview of early work
Selman 1989 - Connectionist systems for natural language understanding
Wermter et al 1996 - Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing Nice overview, part of a larger book
POS Tagging
Parsing
Small et al 1982 - Towards Connectionist Parsing
Selman 1985 - Rule-based Processing in a Connectionist System for Natural Language Understanding (Tech Report CSRI-168 U. Toronto. local copy) Wow, foundational work. Way ahead of it's time. Used a heuristic method to set the weights, since this was before backprop was invented. Discusses learning the weights p. 37, bottom (p. 44 in pdf). Hinton was on the thesis committee.
Selman, B. & Hirst, G. (1985). A Rule-Based Connectionist Parsing System, Proceedings of the Seventh Annual Conference of the Cognitive Science Society, Irvine, CA, August 1985, 212-219. An extended version entitled 'Parsing as an Energy Minimization Problem' appeared in Genetic Algorithms and Simulated Annealing (ed.) Lawrence Davis, Pitman, London. 155-168.
Charniak & Santos 1987 - A connectionist context-free parser which is not context-free but then it is not really connectionist either
Jain 1991 - Parsing Complex Sentences with Structured Connectionist Networks local copy
Semantic Parsing (including shallow semantic parsing)
Hinton 1981 - Implementing semantic networks in parallel hardware Cited by McClelland 1986 as Hinton 1981.
McClelland & Kawamoto 1986 - Mechanisms of sentence processing: Assigning roles to constituents (another copy with references)
Machine Translation
McLean 1992 - Example-Based Machine Translation using Connectionist Matching
Castaño et al 1997 - A Connectionist Approach to Machine Translation local copy
Castaño et al 1997 - Machine Translation using Neural Networks and Finite-State Models local copy Good references to early literature
Forcada & Neco 1997 - Recursive Hetero-Associative Memories for Translation local copy Introduced the encoder-decoder RNN architeture for NMT
Inference and Reasoning
Touretzky & Hinton 1985 - Symbols Among the Neurons: Details of a Connectionist Inference Architecture
Early Deep Learning in NLP
Work since 2000s, but prior to 2014.
Bengio et al 2003 - A Neural Probabilistic Language Model
Collobert et al 2011 - Natural Language Processing (Almost) from Scratch"
What are the requirements for insurance that can be used instead of UC SHIP?,"
You can use the following response:

If you're covered under UC SHIP and your spouse's or parent's health plan is also covered, you can use it as the primary carrier for payment. However, if you're also covered by Medi-Cal, MRMIP, or TRICARE, UC SHIP will be the primary carrier, and your other plan will be the secondary carrier. You can complete the COB questionnaire to determine the primary plan for payment.","When You’re Covered by More Than One Plan | UC Santa Cruz

If you are covered under UC SHIP as well as a spouse’s or parent’s health plan, your spouse’s or parent’s plan is the primary carrier and UC SHIP is the secondary carrier — except when you’re also covered by Medi-Cal, MRMIP, or TRICARE. The primary carrier’s benefits are applied to the claim first. The unpaid balance is usually paid by the secondary plan, to the limit of its responsibility.
UC SHIP coordinates your benefits between both health plans to make sure payments do not exceed 100% of charges for the covered services.
If you have other coverage in addition to UC SHIP, please complete the COB questionnaire [PDF] and submit it to Anthem.
Determining the primary plan for payment
To get the most out of your coverage, it helps to know how the two plans work together.
For care at the SHC, UC SHIP is the primary carrier. The SHC will submit claims to Anthem for SHC services. To be reimbursed for any copayment or coinsurance you pay, you must submit a claim for that amount to your other insurance carrier. The SHC does not submit claims to other health plans.
For care outside the SHC, your other medical plan is the primary carrier. That means it must pay claims first, before any remaining charges can be submitted to UC SHIP for payment.
Exceptions: If you are covered by Medi-Cal, MRMIP, or TRICARE in addition to UC SHIP, UC SHIP is the primary health plan, and your other plan is the secondary plan.
If you have questions about coordinating between plans, call Anthem Blue Cross Member Services at (866) 940-8306.
Contacts
Anthem Blue Cross network
UC SHIP customer service(866) 940-8306
Anthem Blue View Vision
Vision plan benefits(866) 940-8306
Delta Dental customer service
Dental plan benefits(800) 765-6003
Optum Rx
Prescription drug plan benefits(844) 265-1879"
"I need to contact someone about changing converage, who should I contact?","
Contact Information: 
Student Health Center (SHC)
Academic HealthPlans
Anthem Blue Cross network
Blue Cross Blue Shield Global Core
BlueCard program
Delta Dental customer service
LiveHealth Online
Optum Rx
Answers:
To contact someone about changing your coverage, I would contact the Student Health Center (SHC) at (831) 459-2211 or (831) 459-2500.","Contacts | UC Santa Cruz

Contacts
Student Health Center (SHC)
Your first stop for care on campusMain number: (831) 459-2211Appointments: (831) 459-2500Counseling and psychological services (CAPS): (831) 459-2628SHC websiteSHC email1156 High Street (across from Colleges 9 and 10)
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email
Anthem Blue Cross network
UC SHIP customer service(866) 940-8306
Anthem Blue View Vision
Vision plan benefits(866) 940-8306
Blue Cross Blue Shield Global Core
International medical assistance services and access to providers, hospitals and other health care professionals in nearly 200 countries(804) 673-1177Global Core website
BlueCard program
Nationwide access to providers and facilities(800) 810-2583
Delta Dental customer service
Dental plan benefits(800) 765-6003
LiveHealth Online
Urgent virtual medical and mental health care
LiveHealth Online website
LiveHealth Online mental health website
Optum Rx
Prescription drug plan benefits(844) 265-1879"
Is medical an acceptable insurance?,"
Yes, medical is an acceptable insurance option. UC SHIP provides coverage for inpatient and outpatient care for mental health and substance use conditions, as well as doctor office visits for medical conditions. It also covers services related to suicidal conditions, including attempted suicide or suicidal thoughts. Medical insurance can be an acceptable option for students, as long as it meets the university's health insurance requirements, including providing unrestricted access to in-network providers and covering minimum essential health benefits.","Waiving Coverage | UC Santa Cruz

What you need to know
To waive UC SHIP coverage, you must have other health insurance coverage that meets the university’s health insurance requirements, described in the waiver criteria below. You must submit a waiver application prior to the start of each new academic year to waive coverage that year.
Waiver criteria for 2024-25 academic year
All plans must provide unrestricted access to an in-network primary care provider and in-network hospital and to full non-emergency medical and mental health care within a reasonable distance of campus or the student’s place of residence (if online learning only) while attending school. Such distance shall be determined at the discretion of each campus based on its unique geographic considerations and local availability of services. (The waiver form will indicate the distance requirement appropriate for each campus.)
NOTE: This criterion applies to all plan types without exception, including Medi-Cal or Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, HMOs (including Kaiser, WHA and others), Covered California or other U.S. federal or state exchange plans, and all employee-sponsored and individual plans.
Coverage must be currently active, and the student must agree to maintain health coverage throughout the entire academic year. If your current insurance coverage is terminated, contact the student health insurance office as quickly as possible to discuss your health insurance options.
To satisfy UC’s health insurance requirement for enrolled students, the insurance plan held by the student must provide the following:
Be a Medi-Cal/Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, Covered California or other U.S. federal or state exchange plan, or a UC employee health plan, OR
Be an employer-sponsored group health plan or individual plan that:
Has an annual out-of-pocket maximum of $9,450 or less for an individual or $18,900 or less for a family. Deductibles, copayments and coinsurance paid by the member accrue toward meeting the out-of-pocket maximum. A higher out-of-pocket maximum is allowed if the member has a Health Savings Account (HSA) or a Health Reimbursement Account (HRA).
Covers inpatient (hospital) and outpatient care for mental health and substance use conditions the same as any other medical condition.
Covers doctor office visits for medical, including mental health, and alcohol/drug abuse conditions.
Provides coverage for all minimum essential health benefits (EHB). For the criteria, please go to the Centers for Medicare & Medicaid Services (CMS) website.
Is not a health care or pharmacy reimbursement plan (reimbursement plan means the student must pay for services and seek reimbursement from the insurance provider).
Does not have a per-medical or per-mental health/substance use condition maximum dollar limit.
International students
For international students residing in the United States, the following additional criteria apply. The plan must:
Have no per-medical or per-mental health/substance use condition maximum benefit limits.
Cover services related to suicidal conditions, including attempted suicide or suicidal thoughts.
Cover medical services for injury from participation in all types of recreational activities or amateur sports.
Have no preexisting condition exclusion or limitation; if the plan has a preexisting condition waiting period, that period must have expired.
Have no lifetime maximums on benefits.
Have a complete master policy written in standard English with benefits expressed in U.S. dollars.
Have a claims payment office with an address and phone number in the United States.
Pay at least $50,000 annually for medical evacuation.
Pay at least $25,000 for repatriation of remains.
The waiver process
Academic HealthPlans manages the waiver process through its website. It’s where you’ll initiate your request, submit all required documentation, and make any updates to your existing application.
Want to waive UC SHIP coverage?Academic HealthPlans’ (AHP’s) website makes it fast and easy to submit documents and complete the UC SHIP waiver process online.
Waive your UC SHIP coverage
Applying for a waiver
Here are seven things to know when you apply for a waiver.
You must submit the waiver application and supporting documents during the waiver period.
You must submit the waiver application for each academic year you want to waive UC SHIP coverage.
You can complete the application online during the fall, winter or spring waiver periods. Your waiver will be applied to the full academic year — or to the remainder of the academic year if you apply during the winter or spring terms.
When you submit a waiver application, we contact your health insurance company to verify that you are actively enrolled. You will receive a quick response to your application.
If your waiver is approved, you must maintain active enrollment in your health insurance plan for the entire academic year. Before the start of every term, we reverify that you are enrolled in your health insurance plan.
If we cannot verify that you are actively enrolled in a health insurance plan, you will be notified by email and/or text message that your waiver was denied. You will then be enrolled in UC SHIP and charged the cost of coverage for that term.
If your waiver is denied, you can appeal the decision by submitting proof of current insurance coverage from your health insurance plan.
Where to learn more
You can find waiver instructions, waiver periods, FAQs and forms on the UC SHIP page on the Student Health Center website.
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
Is medicare an exception for the insurance requirements?,"
Medicare is an exception for the insurance requirements for UC SHIP coverage, but only if you are a military veteran or active-duty member of the military. If you are a military veteran or active-duty member, you may be eligible for a waiver of Medicare coverage. You will need to provide documentation to support your eligibility and submit a waiver application. The waiver process is similar to the UC SHIP waiver process, but you will need to provide additional documentation and meet the same requirements as other students.","Waiving Coverage | UC Santa Cruz

What you need to know
To waive UC SHIP coverage, you must have other health insurance coverage that meets the university’s health insurance requirements, described in the waiver criteria below. You must submit a waiver application prior to the start of each new academic year to waive coverage that year.
Waiver criteria for 2024-25 academic year
All plans must provide unrestricted access to an in-network primary care provider and in-network hospital and to full non-emergency medical and mental health care within a reasonable distance of campus or the student’s place of residence (if online learning only) while attending school. Such distance shall be determined at the discretion of each campus based on its unique geographic considerations and local availability of services. (The waiver form will indicate the distance requirement appropriate for each campus.)
NOTE: This criterion applies to all plan types without exception, including Medi-Cal or Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, HMOs (including Kaiser, WHA and others), Covered California or other U.S. federal or state exchange plans, and all employee-sponsored and individual plans.
Coverage must be currently active, and the student must agree to maintain health coverage throughout the entire academic year. If your current insurance coverage is terminated, contact the student health insurance office as quickly as possible to discuss your health insurance options.
To satisfy UC’s health insurance requirement for enrolled students, the insurance plan held by the student must provide the following:
Be a Medi-Cal/Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, Covered California or other U.S. federal or state exchange plan, or a UC employee health plan, OR
Be an employer-sponsored group health plan or individual plan that:
Has an annual out-of-pocket maximum of $9,450 or less for an individual or $18,900 or less for a family. Deductibles, copayments and coinsurance paid by the member accrue toward meeting the out-of-pocket maximum. A higher out-of-pocket maximum is allowed if the member has a Health Savings Account (HSA) or a Health Reimbursement Account (HRA).
Covers inpatient (hospital) and outpatient care for mental health and substance use conditions the same as any other medical condition.
Covers doctor office visits for medical, including mental health, and alcohol/drug abuse conditions.
Provides coverage for all minimum essential health benefits (EHB). For the criteria, please go to the Centers for Medicare & Medicaid Services (CMS) website.
Is not a health care or pharmacy reimbursement plan (reimbursement plan means the student must pay for services and seek reimbursement from the insurance provider).
Does not have a per-medical or per-mental health/substance use condition maximum dollar limit.
International students
For international students residing in the United States, the following additional criteria apply. The plan must:
Have no per-medical or per-mental health/substance use condition maximum benefit limits.
Cover services related to suicidal conditions, including attempted suicide or suicidal thoughts.
Cover medical services for injury from participation in all types of recreational activities or amateur sports.
Have no preexisting condition exclusion or limitation; if the plan has a preexisting condition waiting period, that period must have expired.
Have no lifetime maximums on benefits.
Have a complete master policy written in standard English with benefits expressed in U.S. dollars.
Have a claims payment office with an address and phone number in the United States.
Pay at least $50,000 annually for medical evacuation.
Pay at least $25,000 for repatriation of remains.
The waiver process
Academic HealthPlans manages the waiver process through its website. It’s where you’ll initiate your request, submit all required documentation, and make any updates to your existing application.
Want to waive UC SHIP coverage?Academic HealthPlans’ (AHP’s) website makes it fast and easy to submit documents and complete the UC SHIP waiver process online.
Waive your UC SHIP coverage
Applying for a waiver
Here are seven things to know when you apply for a waiver.
You must submit the waiver application and supporting documents during the waiver period.
You must submit the waiver application for each academic year you want to waive UC SHIP coverage.
You can complete the application online during the fall, winter or spring waiver periods. Your waiver will be applied to the full academic year — or to the remainder of the academic year if you apply during the winter or spring terms.
When you submit a waiver application, we contact your health insurance company to verify that you are actively enrolled. You will receive a quick response to your application.
If your waiver is approved, you must maintain active enrollment in your health insurance plan for the entire academic year. Before the start of every term, we reverify that you are enrolled in your health insurance plan.
If we cannot verify that you are actively enrolled in a health insurance plan, you will be notified by email and/or text message that your waiver was denied. You will then be enrolled in UC SHIP and charged the cost of coverage for that term.
If your waiver is denied, you can appeal the decision by submitting proof of current insurance coverage from your health insurance plan.
Where to learn more
You can find waiver instructions, waiver periods, FAQs and forms on the UC SHIP page on the Student Health Center website.
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
What do I pay out of pocket with UC SHIP?,"
You will pay a portion of the health insurance premium on your registration bill. The amount you pay will depend on your financial position and whether or not you have any insurance coverage.

You may be able to use the resources mentioned above to help pay for your medical expenses.","Paying for Coverage | UC Santa Cruz

UC SHIP premiums are increasing for the 2024-25 academic year.
Learn more about the increases
If you’re a registered student, you are automatically enrolled in UC SHIP and charged a health insurance premium on your registration bill.
If you’re a non-registered student enrolling yourself and/or your eligible dependents for coverage, you must pay the full insurance premium upon enrollment.
Learn what you’ll pay for coverage.
Help paying for medical expenses
Providing affordable health care is a UC SHIP mandate. Even so, every student’s financial position is unique. If your out-of-pocket medical expenses are causing you financial hardship, these UCSC resources may be able to provide financial assistance:
Slug Support Program
Health Equity Fund
Charity care may also be an option for you. Reach out to the provider’s office and ask if they offer any form of charity care. Although some providers do not offer any sort of charity care, others do. They may provide an application for you to complete or ask you to write a letter. Providers who do offer charity care will review your request, and then they could offer to write off the full amount you owe, a portion of that amount, or they may determine you don’t meet their qualifications for charity care.


"
What mental health options are covered by UC SHIP?,"
UC SHIP offers inpatient and outpatient mental health and substance use services through the Student Health Center, LiveHealth Online, and the Anthem Blue Cross network. You can see a therapist virtually through LiveHealth Online, and pay nothing to see a therapist via LiveHealth Online. For inpatient and outpatient care, you can visit the Student Health Center, and pay less for out-of-network care. You can also contact the Student Health Center (SHC) for more information.","Mental Health | UC Santa Cruz

What you need to know
Your UC SHIP coverage includes both inpatient and outpatient care for mental health and substance use. As with some other services, for your care to be covered by UC SHIP, you need a referral to see a provider outside the Student Health Center, except for LiveHealth Online.
Covered services and resources
Within the UC SHIP network, you have access to mental health and substance use services through the Student Health Center, LiveHealth Online, and the Anthem Blue Cross network.
For details about inpatient and outpatient mental health and substance use services covered by UC SHIP, review the UC Santa Cruz Benefit Booklet [PDF].
24/7 support on campusThe Student Health Center (SHC) offers even more support for your emotional well-being.
See what the SHC provides
LiveHealth Online
When you’re overwhelmed by stress, depression, anxiety, lack of focus, or other issues, you can see a therapist virtually, wherever you are, through LiveHealth Online. Your session will last 45 minutes, and you can set up a return visit with the same or a different provider if you want to.
Take the following steps to schedule a visit:
Sign up or log in to LiveHealth Online.
On the top navigation bar, choose Therapy.
Select your therapist and schedule a visit.
You’ll pay nothing to see a therapist via LiveHealth Online. For more information, see the LiveHealth Online flyer.
Anthem Blue Cross network
You’ll generally pay less for inpatient and outpatient care when you visit network providers. See what you pay for office visits.
Out-of-network providers
Typically, you’ll pay more for out-of-network care. See what you’ll pay for office visits.
Contacts
Student Health Center (SHC)
Your first stop for care on campusMain number: (831) 459-2211Appointments: (831) 459-2500Counseling and psychological services (CAPS): (831) 459-2628SHC websiteSHC email1156 High Street (across from Colleges 9 and 10)
LiveHealth Online
Urgent virtual medical and mental health care
LiveHealth Online website
LiveHealth Online mental health website
Anthem Blue Cross network
UC SHIP customer service(866) 940-8306
Related pages
Medical
Prescription Drugs"
What is the annual limit on ou of pocket expenses for my family?,"
According to the Centers for Medicare & Medicaid Services (CMS), the annual limit on out-of-pocket expenses for a family is $9,900. This limit applies to individuals with a family plan, not to individuals with a family plan.","Waiving Coverage | UC Santa Cruz

What you need to know
To waive UC SHIP coverage, you must have other health insurance coverage that meets the university’s health insurance requirements, described in the waiver criteria below. You must submit a waiver application prior to the start of each new academic year to waive coverage that year.
Waiver criteria for 2024-25 academic year
All plans must provide unrestricted access to an in-network primary care provider and in-network hospital and to full non-emergency medical and mental health care within a reasonable distance of campus or the student’s place of residence (if online learning only) while attending school. Such distance shall be determined at the discretion of each campus based on its unique geographic considerations and local availability of services. (The waiver form will indicate the distance requirement appropriate for each campus.)
NOTE: This criterion applies to all plan types without exception, including Medi-Cal or Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, HMOs (including Kaiser, WHA and others), Covered California or other U.S. federal or state exchange plans, and all employee-sponsored and individual plans.
Coverage must be currently active, and the student must agree to maintain health coverage throughout the entire academic year. If your current insurance coverage is terminated, contact the student health insurance office as quickly as possible to discuss your health insurance options.
To satisfy UC’s health insurance requirement for enrolled students, the insurance plan held by the student must provide the following:
Be a Medi-Cal/Medicaid, Medicare, TRICARE/military, Veterans Affairs benefits, Covered California or other U.S. federal or state exchange plan, or a UC employee health plan, OR
Be an employer-sponsored group health plan or individual plan that:
Has an annual out-of-pocket maximum of $9,450 or less for an individual or $18,900 or less for a family. Deductibles, copayments and coinsurance paid by the member accrue toward meeting the out-of-pocket maximum. A higher out-of-pocket maximum is allowed if the member has a Health Savings Account (HSA) or a Health Reimbursement Account (HRA).
Covers inpatient (hospital) and outpatient care for mental health and substance use conditions the same as any other medical condition.
Covers doctor office visits for medical, including mental health, and alcohol/drug abuse conditions.
Provides coverage for all minimum essential health benefits (EHB). For the criteria, please go to the Centers for Medicare & Medicaid Services (CMS) website.
Is not a health care or pharmacy reimbursement plan (reimbursement plan means the student must pay for services and seek reimbursement from the insurance provider).
Does not have a per-medical or per-mental health/substance use condition maximum dollar limit.
International students
For international students residing in the United States, the following additional criteria apply. The plan must:
Have no per-medical or per-mental health/substance use condition maximum benefit limits.
Cover services related to suicidal conditions, including attempted suicide or suicidal thoughts.
Cover medical services for injury from participation in all types of recreational activities or amateur sports.
Have no preexisting condition exclusion or limitation; if the plan has a preexisting condition waiting period, that period must have expired.
Have no lifetime maximums on benefits.
Have a complete master policy written in standard English with benefits expressed in U.S. dollars.
Have a claims payment office with an address and phone number in the United States.
Pay at least $50,000 annually for medical evacuation.
Pay at least $25,000 for repatriation of remains.
The waiver process
Academic HealthPlans manages the waiver process through its website. It’s where you’ll initiate your request, submit all required documentation, and make any updates to your existing application.
Want to waive UC SHIP coverage?Academic HealthPlans’ (AHP’s) website makes it fast and easy to submit documents and complete the UC SHIP waiver process online.
Waive your UC SHIP coverage
Applying for a waiver
Here are seven things to know when you apply for a waiver.
You must submit the waiver application and supporting documents during the waiver period.
You must submit the waiver application for each academic year you want to waive UC SHIP coverage.
You can complete the application online during the fall, winter or spring waiver periods. Your waiver will be applied to the full academic year — or to the remainder of the academic year if you apply during the winter or spring terms.
When you submit a waiver application, we contact your health insurance company to verify that you are actively enrolled. You will receive a quick response to your application.
If your waiver is approved, you must maintain active enrollment in your health insurance plan for the entire academic year. Before the start of every term, we reverify that you are enrolled in your health insurance plan.
If we cannot verify that you are actively enrolled in a health insurance plan, you will be notified by email and/or text message that your waiver was denied. You will then be enrolled in UC SHIP and charged the cost of coverage for that term.
If your waiver is denied, you can appeal the decision by submitting proof of current insurance coverage from your health insurance plan.
Where to learn more
You can find waiver instructions, waiver periods, FAQs and forms on the UC SHIP page on the Student Health Center website.
Contacts
Academic HealthPlans
(855) 427-3167: Purchase UC SHIP coverage(855) 247-2273: IRS Form 1095-BAcademic HealthPlans email1095-B Form email"
What is the copay for bifocals?,"
The copay for bifocals is $25.","Vision | UC Santa Cruz

What you need to know
UC SHIP vision coverage helps you see clearly at a price you can afford. Whether you need a routine eye exam or new prescription eyewear, you’ll pay a lot less when you visit a provider in the Anthem Blue View Vision Insight network.
You don’t need to get a referral for vision care.
Where to get care
Don’t want to leave campus? Schedule an optometry appointment with the SHC Optometry department online or call (831) 459-2389.
For vision care away from campus, it’s easy to find a provider in the Blue View Vision Insight network. Just search by ZIP code or your current location on the Anthem website. If you search as a guest, make sure you select the “Blue View Vision Insight” network.
What you pay
Anthem Blue View Vision network
Here’s what you’ll pay when you use a network provider.
Routine eye exam
One per benefit year: $10 copay
Eyeglass frames
One pair per benefit year: $120 frame allowance, plus a 20% discount on balance
Alternatively, instead of eyeglasses, you can select contact lenses and the allowance and coinsurance associated with them (see below).
Standard eyeglass lenses
Single lenses: $25 copay
Bifocal lenses: $25 copay
Trifocal lenses: $25 copay
Contact lenses fitting and follow-up
Standard lens fitting. After a comprehensive eye exam, plus up to two follow-up visits: $0
Premium lens fitting. After a comprehensive eye exam, plus up to two follow-up visits: 10% discount on the retail price
Note: In addition to the eye exam fee, there is typically a fee for fitting and evaluating the fit of the contact lenses. This will depend on the type of lenses (e.g., for astigmatism or no astigmatism, soft disposable or gas permeable, and the level of complexity). The fitting fee can range from $32 to $300.
Contact lenses — conventional
$120 lens allowance, plus a 15% discount on anything over $120
Contact lenses — disposable
$120 lens allowance (no additional discount)
Out-of-network
When you receive care from an out-of-network provider, you’ll need to pay out of pocket at the time of service, and then submit a claim to be reimbursed up to the following limits.
Routine eye exam
One per benefit year: up to a $49 exam allowance
Eyeglass frames
One pair per benefit year: up to a $50 frame allowance
Alternatively, instead of eyeglasses, you can select contact lenses and the allowance and coinsurance associated with them (see below).
Standard eyeglass lenses
Single lenses: $35 lens allowance
Bifocal lenses: $49 lens allowance
Trifocal lenses: $74 lens allowance
Contact lenses fitting and follow-up
Not covered
Contact lenses — conventional
$92 lens allowance
Contact lenses — disposable
$92 lens allowance
For more information and a complete list of vision plan benefits, review the Blue View Vision Plan Benefit Booklet [PDF]. You’ll also find Spanish and Mandarin versions of the booklet on the Forms and documents page.
Contacts
Anthem Blue View Vision
Vision plan benefits(866) 940-8306
(831) 459-2389"
"I'm not on campus, can I use the UC SHIP insurance for services outside of Santa Cruz?","
Yes, you can use UC SHIP insurance for services outside of Santa Cruz. With UC SHIP, you have access to the UC Family of medical centers, affiliated facilities and professional providers, which are in-network providers. You can get a referral from the Student Health Center and use the Sydney Health app or Anthem Blue Cross Prudent Buyer PPO network to find a provider. If you take your referral to an out-of-network provider, your costs will be higher. However, you may be reimbursed by UC SHIP, and you can file a claim for reimbursement from UC SHIP.","When You’re Away from Campus | UC Santa Cruz

What you need to know
With UC SHIP, you’re covered wherever you go — whether it’s around the block or around the world.
Getting care off campus
When you receive a referral to get care outside the Student Health Center (SHC), your cost for that care will depend on whether or not you receive care from a network provider. Staff at the SHC insurance office can help you find network providers at the UC health center near you or within the Anthem Blue Cross Prudent Buyer PPO network. If you choose to get care from an out-of-network provider, your costs will be considerably higher.
Know before you goWithout a referral from the Student Health Center, some care won’t be covered by UC SHIP. When necessary, be sure to get a referral so you’re not stuck with an unexpected bill to pay.
Here’s what you should know about your three options for getting care outside the SHC.
UC Family network
As a member of UC SHIP, you have access to the UC Family of nationally ranked medical centers — along with their affiliated facilities and professional providers. The UC Family of medical centers, affiliated facilities and professional providers are in-network providers.
Anthem network
You can ask the SHC for assistance finding a provider in the Anthem Blue Cross Prudent Buyer PPO network. You can also use the Sydney Health app or go to Anthem’s online provider listing to search for network providers, clinics and hospitals in your area.
Out-of-network
If you take your referral to an out-of-network provider, your costs will be considerably higher.
You may be asked to pay up front. To be reimbursed, you will need to submit your claim to Anthem yourself because you aren’t using an Anthem provider.
In addition to paying a higher percentage of your bill out of pocket when you see an out-of-network provider, you will also need to pay the difference between the provider’s billed charge and Anthem’s maximum allowed amount. This difference does not count toward the annual limit on your out-of-pocket costs.
The annual limit on your out-of-network out-of-pocket costs is two or three times higher than the limit on network out-of-pocket costs. As a reminder, once your costs hit the out-of-pocket limit, UC SHIP steps in to pay 100% of the costs for the remainder of the academic year.
Visiting another UC campus
Registered students with UC SHIP coverage can access services offered at student health centers on all UC campuses. For fees and information about filing claims with UC SHIP, check with the SHC on the campus you plan to visit.
Traveling within the United States
You’ll always pay less for care when you see an Anthem Blue Cross Prudent Buyer PPO network provider. To find an Anthem provider in any state, visit the Anthem website.
Traveling outside the United States
UC SHIP provides worldwide coverage when you’re traveling or living outside the United States. When you and your dependents covered under UC SHIP need care, you can use the Blue Cross Blue Shield Global Core program, which coordinates benefits abroad.
Five things to do before leaving the United States
To be sure you have the information you need at your fingertips, take the following steps:
Call BlueCard Access at (800) 810-BLUE (2583) to learn how you’re covered abroad.
Call your SHC about coverage for any vaccines you may need for your destination and to determine whether you’re eligible for UC travel accident coverage.
Download the Sydney Health app from the App Store or Google Play.
Download the BCBS Global Core app from the App Store or Google Play.
Record important SHC phone numbers and pack your UC ID cards. Consider printing a copy of your Anthem ID card from the Sydney Health app; it contains identification numbers that confirm your Anthem coverage.
Using Blue Cross Blue Shield Global Core
As needed, contact the SHC for a referral before receiving non-emergency care.
To find a Blue Cross Blue Shield Global Core provider, visit the Blue Cross Blue Shield Global Core website. In the box next to the GO button, enter the first three letters or numbers of the Member ID on your card.
Work with Global Core to find an in-network provider. If you have to see an out-of-network doctor, you’ll pay out of pocket for your care. You can file a claim for reimbursement from UC SHIP. As a reminder, UC SHIP covers most out-of-network care at 60% of maximum allowable charges; you’re responsible for 40% plus any amount above the allowed charges. Emergency care is covered at 100% of maximum allowable charges.
Traveling on university business
If you’re traveling on UC business (doing research abroad, for example), you also may be eligible for travel accident coverage at no cost to you. To be covered, you must register before you leave on your UC-sponsored trip.
Both UC SHIP and travel accident insurance cover medical evacuation and transportation related to your medical care."
Where can I go if I am having a medical emergency?,"
If you're in a medical emergency, you can call the UC SHIP hotline at (800) 810-8100 for urgent care. If you need to be transported to a hospital, you can contact the UC SHIP Transportation Service at (800) 810-8100. If you're on a university-sponsored trip, you can contact the UC SHIP Transportation Service at (800) 810-8100.","When You’re Away from Campus | UC Santa Cruz

What you need to know
With UC SHIP, you’re covered wherever you go — whether it’s around the block or around the world.
Getting care off campus
When you receive a referral to get care outside the Student Health Center (SHC), your cost for that care will depend on whether or not you receive care from a network provider. Staff at the SHC insurance office can help you find network providers at the UC health center near you or within the Anthem Blue Cross Prudent Buyer PPO network. If you choose to get care from an out-of-network provider, your costs will be considerably higher.
Know before you goWithout a referral from the Student Health Center, some care won’t be covered by UC SHIP. When necessary, be sure to get a referral so you’re not stuck with an unexpected bill to pay.
Here’s what you should know about your three options for getting care outside the SHC.
UC Family network
As a member of UC SHIP, you have access to the UC Family of nationally ranked medical centers — along with their affiliated facilities and professional providers. The UC Family of medical centers, affiliated facilities and professional providers are in-network providers.
Anthem network
You can ask the SHC for assistance finding a provider in the Anthem Blue Cross Prudent Buyer PPO network. You can also use the Sydney Health app or go to Anthem’s online provider listing to search for network providers, clinics and hospitals in your area.
Out-of-network
If you take your referral to an out-of-network provider, your costs will be considerably higher.
You may be asked to pay up front. To be reimbursed, you will need to submit your claim to Anthem yourself because you aren’t using an Anthem provider.
In addition to paying a higher percentage of your bill out of pocket when you see an out-of-network provider, you will also need to pay the difference between the provider’s billed charge and Anthem’s maximum allowed amount. This difference does not count toward the annual limit on your out-of-pocket costs.
The annual limit on your out-of-network out-of-pocket costs is two or three times higher than the limit on network out-of-pocket costs. As a reminder, once your costs hit the out-of-pocket limit, UC SHIP steps in to pay 100% of the costs for the remainder of the academic year.
Visiting another UC campus
Registered students with UC SHIP coverage can access services offered at student health centers on all UC campuses. For fees and information about filing claims with UC SHIP, check with the SHC on the campus you plan to visit.
Traveling within the United States
You’ll always pay less for care when you see an Anthem Blue Cross Prudent Buyer PPO network provider. To find an Anthem provider in any state, visit the Anthem website.
Traveling outside the United States
UC SHIP provides worldwide coverage when you’re traveling or living outside the United States. When you and your dependents covered under UC SHIP need care, you can use the Blue Cross Blue Shield Global Core program, which coordinates benefits abroad.
Five things to do before leaving the United States
To be sure you have the information you need at your fingertips, take the following steps:
Call BlueCard Access at (800) 810-BLUE (2583) to learn how you’re covered abroad.
Call your SHC about coverage for any vaccines you may need for your destination and to determine whether you’re eligible for UC travel accident coverage.
Download the Sydney Health app from the App Store or Google Play.
Download the BCBS Global Core app from the App Store or Google Play.
Record important SHC phone numbers and pack your UC ID cards. Consider printing a copy of your Anthem ID card from the Sydney Health app; it contains identification numbers that confirm your Anthem coverage.
Using Blue Cross Blue Shield Global Core
As needed, contact the SHC for a referral before receiving non-emergency care.
To find a Blue Cross Blue Shield Global Core provider, visit the Blue Cross Blue Shield Global Core website. In the box next to the GO button, enter the first three letters or numbers of the Member ID on your card.
Work with Global Core to find an in-network provider. If you have to see an out-of-network doctor, you’ll pay out of pocket for your care. You can file a claim for reimbursement from UC SHIP. As a reminder, UC SHIP covers most out-of-network care at 60% of maximum allowable charges; you’re responsible for 40% plus any amount above the allowed charges. Emergency care is covered at 100% of maximum allowable charges.
Traveling on university business
If you’re traveling on UC business (doing research abroad, for example), you also may be eligible for travel accident coverage at no cost to you. To be covered, you must register before you leave on your UC-sponsored trip.
Both UC SHIP and travel accident insurance cover medical evacuation and transportation related to your medical care."
What if I am not able to get a referral?,"
If you are unable to get a referral from the SHC, you can seek medical care from a doctor in the Anthem Blue Cross Prudent Buyer PPO network. This includes urgent care centers, local hospitals, or private medical providers. You can find a list of in-network providers on the SHC website. Additionally, you can contact the Student Health Center insurance office to discuss your options. They can help you find a provider that is close to you and can provide the necessary care.","Where to Get Care | UC Santa Cruz

What you need to know
When you’re covered by UC SHIP, the Student Health Center (SHC) provides your primary care and coordinates any additional medical care you might need.
Start at the Student Health Center
As the outpatient student health center on the Santa Cruz campus, the SHC provides outpatient medical, pharmacy, preventive care, and mental health and substance use services. It’s staffed with board-certified physicians, certified nurse practitioners, and physician assistants, all of whom are experts in meeting student health care needs.
To be covered by UC SHIP, you need a referral from the SHC to get medical care anywhere else — except for emergency room care, urgent care clinic visits, pediatric care, obstetrics services, gynecological care, and LiveHealth Online virtual visits.
You’ll pay nothing for preventive care at the SHC, and low copays for other services and procedures. If needed, the SHC will provide a referral for services it can’t provide. Staff at the SHC insurance office can help you find network providers at the UC health center near you or within the Anthem Blue Cross Prudent Buyer PPO network.
Learn more about SHC services on the Student Health Center website.
Know before you go
Without a referral from the SHC, some care won’t be covered by UC SHIP. When necessary, be sure to get a referral so you’re not stuck with an unexpected bill to pay.
Other care options
For those times when you need to connect with a medical or mental health provider and you can’t wait for care, you have options.
Phone an advice nurse
If you have an issue that you can take care of by phone, you can call the advice nurse 24/7/365 at (831) 459-2591. This service is free to students.
Get virtual care
LiveHealth Online provides 24/7 access to doctors and mental health specialists for a low copay. You don’t need a referral to use LiveHealth Online, and if you need a prescription, they’ll send it to your local pharmacy.
Visit an urgent care center
For immediate care, you can also head to a local urgent care center. You’ll pay a moderate copay to see doctors in the Anthem Blue Cross Prudent Buyer PPO network. Use the Sydney Health app or visit Anthem’s online provider listing to search for in-network urgent care close to you.
Urgent care is care for an illness, injury or condition that is not life threatening (does not require emergency room care), but that cannot wait for a regularly scheduled office visit. Some common urgent care needs include treatment for earaches, sore throats, and fevers (under 104 degrees), as well as stitches for simple cuts, and tests for flu, strep and urinary tract infections.
Still not sure where to get care when you need it? Review Where to Go When You Need Care Now [PDF].
When to head to the hospital
If you experience a life-threatening emergency, including severe shortness of breath, loss of consciousness, chest pain, or coughing up or vomiting blood, go to the nearest hospital emergency room."
Will I be covered by UC SHIP insurance if I am out of the country?,"
Yes, you will be covered by UC SHIP insurance if you are traveling outside the United States, but there are some important details to be aware of. When you receive care outside the Student Health Center, your cost for that care will depend on whether or not you receive care from a network provider. Staff at the SHC insurance office can help you find network providers at the UC health center near you or within the Anthem Blue Cross Prudent Buyer PPO network. If you choose to get care from an out-of-network provider, your costs will be considerably higher.","When You’re Away from Campus | UC Santa Cruz

What you need to know
With UC SHIP, you’re covered wherever you go — whether it’s around the block or around the world.
Getting care off campus
When you receive a referral to get care outside the Student Health Center (SHC), your cost for that care will depend on whether or not you receive care from a network provider. Staff at the SHC insurance office can help you find network providers at the UC health center near you or within the Anthem Blue Cross Prudent Buyer PPO network. If you choose to get care from an out-of-network provider, your costs will be considerably higher.
Know before you goWithout a referral from the Student Health Center, some care won’t be covered by UC SHIP. When necessary, be sure to get a referral so you’re not stuck with an unexpected bill to pay.
Here’s what you should know about your three options for getting care outside the SHC.
UC Family network
As a member of UC SHIP, you have access to the UC Family of nationally ranked medical centers — along with their affiliated facilities and professional providers. The UC Family of medical centers, affiliated facilities and professional providers are in-network providers.
Anthem network
You can ask the SHC for assistance finding a provider in the Anthem Blue Cross Prudent Buyer PPO network. You can also use the Sydney Health app or go to Anthem’s online provider listing to search for network providers, clinics and hospitals in your area.
Out-of-network
If you take your referral to an out-of-network provider, your costs will be considerably higher.
You may be asked to pay up front. To be reimbursed, you will need to submit your claim to Anthem yourself because you aren’t using an Anthem provider.
In addition to paying a higher percentage of your bill out of pocket when you see an out-of-network provider, you will also need to pay the difference between the provider’s billed charge and Anthem’s maximum allowed amount. This difference does not count toward the annual limit on your out-of-pocket costs.
The annual limit on your out-of-network out-of-pocket costs is two or three times higher than the limit on network out-of-pocket costs. As a reminder, once your costs hit the out-of-pocket limit, UC SHIP steps in to pay 100% of the costs for the remainder of the academic year.
Visiting another UC campus
Registered students with UC SHIP coverage can access services offered at student health centers on all UC campuses. For fees and information about filing claims with UC SHIP, check with the SHC on the campus you plan to visit.
Traveling within the United States
You’ll always pay less for care when you see an Anthem Blue Cross Prudent Buyer PPO network provider. To find an Anthem provider in any state, visit the Anthem website.
Traveling outside the United States
UC SHIP provides worldwide coverage when you’re traveling or living outside the United States. When you and your dependents covered under UC SHIP need care, you can use the Blue Cross Blue Shield Global Core program, which coordinates benefits abroad.
Five things to do before leaving the United States
To be sure you have the information you need at your fingertips, take the following steps:
Call BlueCard Access at (800) 810-BLUE (2583) to learn how you’re covered abroad.
Call your SHC about coverage for any vaccines you may need for your destination and to determine whether you’re eligible for UC travel accident coverage.
Download the Sydney Health app from the App Store or Google Play.
Download the BCBS Global Core app from the App Store or Google Play.
Record important SHC phone numbers and pack your UC ID cards. Consider printing a copy of your Anthem ID card from the Sydney Health app; it contains identification numbers that confirm your Anthem coverage.
Using Blue Cross Blue Shield Global Core
As needed, contact the SHC for a referral before receiving non-emergency care.
To find a Blue Cross Blue Shield Global Core provider, visit the Blue Cross Blue Shield Global Core website. In the box next to the GO button, enter the first three letters or numbers of the Member ID on your card.
Work with Global Core to find an in-network provider. If you have to see an out-of-network doctor, you’ll pay out of pocket for your care. You can file a claim for reimbursement from UC SHIP. As a reminder, UC SHIP covers most out-of-network care at 60% of maximum allowable charges; you’re responsible for 40% plus any amount above the allowed charges. Emergency care is covered at 100% of maximum allowable charges.
Traveling on university business
If you’re traveling on UC business (doing research abroad, for example), you also may be eligible for travel accident coverage at no cost to you. To be covered, you must register before you leave on your UC-sponsored trip.
Both UC SHIP and travel accident insurance cover medical evacuation and transportation related to your medical care."
nan,"
Training Deep Neural Networks
Deep neural networks can be trained using various techniques. One of the most effective ways to train these networks is by using the concept of batch normalization. Batch normalization is a technique used to normalize the inputs to the neural network, which helps in improving the training speed and reducing the risk of overfitting. It normalizes the input data to a fixed range, which helps the network to learn more efficiently. By normalizing the inputs, batch normalization helps in reducing the risk of overfitting and improves the overall performance of the network. This technique can be used to train deep neural networks, including convolutional neural networks and recurrent neural networks. Additionally, batch normalization can be used to improve the performance of other types of neural networks, such as autoencoders and generative models. Overall, batch normalization is a powerful technique for training deep neural networks and can help improve their performance.","ml:nn_tricks [NLP Wiki]
Neural Network Tricks
Overviews
Older NN Tricks
Related Pages
Neural Network Tricks
Overviews
NLP 202 lecture: Training Deep Neural Networks (Winter 2022)
Training Tricks (see NN Training)
Initialization
Normalization
Learning Rate Schedule
Gradient clipping Pascanu et al 2012
Scheduled Sampling
Curriculum Learning
Overcoming Catastrophic Forgetting
Adjust the batch size, or use gradient accumulation (see this blog, for example) to simulate larger batch sizes
Try a different optimizer, such as RAdam
Adjust epsilon in Adam
Fine-tuning Specific Tricks
NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better : Before fine-tuning, adding a very small amount of uniform noise to each weight matrix can help performance (noise scaled by variance of the weight)
Regularization Tricks (see Regularization)
Dropout
Ensembling
Knowledge Distillation (can improve performance by some type of regularization)
Label Smoothing
Data Processing Tricks (see Data Preparation)
Subword Units (BPE, wordpiece, subword regularization, BPE dropout.  Shared source and target vocabulary for subword units.)
Shared source and target embeddings
Architecture Tricks (see NN Architectures)
Residual connections
ReZero
Weight sharing
Attention
Copy mechanism
Seq2Seq and Generation Tricks
Try a different decoding method
Nucleus sampling
Uniform information density decoding
Reinforcement Learning Tricks
Efficiency Tricks
GPU Deep Learning
Gradient Checkpointing or forward gradient
Model Compression
Tricks for Edge Computing
Knowledge Distillation
Older NN Tricks
Nowlan & Hinton 1992 - Soft weight sharing
Related Pages
NN Training"
